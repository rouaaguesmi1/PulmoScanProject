{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f61f9f-bd3f-4709-abab-6b72af899f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading Data and Selecting up to 50 Scans Per Class ---\n",
      "DSB Scans path: C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Stages\n",
      "DSB Labels CSV: C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\stage1_labels.csv\n",
      "Loaded 1595 total DSB patient labels.\n",
      "Original label distribution:\n",
      " cancer\n",
      "0    1176\n",
      "1     419\n",
      "Name: count, dtype: int64\n",
      "Found 98 potential patient scan folders.\n",
      "Found 98 patient IDs with both labels and scan folders.\n",
      "Available Cancerous scans with labels: 27\n",
      "Available Non-Cancerous scans with labels: 71\n",
      "Selected 27 Cancerous scans.\n",
      "Selected 50 Non-Cancerous scans.\n",
      "Total scans selected for preprocessing: 77\n",
      "\n",
      "Starting preprocessing for 77 selected scans...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 236\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting preprocessing for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(scans_to_process)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m selected scans...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    234\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscans_to_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPreprocessing \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscans_to_process\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m Selected Scans\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# <<< --- Pass the CORRECT output path --- >>>\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     success \u001b[38;5;241m=\u001b[39m preprocess_scan_dsb(patient_id, DSB_PATH, PREPROCESSED_DSB_PATH, force_preprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# %% Imports\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Use standard tqdm if not in a notebook environment\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "import random\n",
    "import scipy.ndimage\n",
    "from skimage.measure import label as skimage_label, regionprops\n",
    "from skimage.morphology import disk, binary_closing # Removed convex_hull_image, roberts as not used\n",
    "from skimage.segmentation import clear_border\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, auc, f1_score,\n",
    "                             precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay)\n",
    "\n",
    "# %% Configuration\n",
    "# --- MODIFY THESE PATHS ---\n",
    "DSB_PATH = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Stages\" # Base directory of DSB 2017 Stage 1 scans (containing patient folders)\n",
    "DSB_LABELS_CSV = r'C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\stage1_labels.csv' # Path to DSB patient cancer labels CSV\n",
    "PREPROCESSED_DSB_PATH = './preprocessed_dsb_non_cancer/' # <<-- MODIFIED: Separate dir for non-cancer scans\n",
    "# --- MODIFIED: New output path for the limited dataset ---\n",
    "PREPROCESSED_DSB_PATH = './preprocessed_dsb_50_each/'\n",
    "# ---\n",
    "\n",
    "# Preprocessing & Model Params\n",
    "TARGET_SPACING = [1.5, 1.5, 1.5]\n",
    "FINAL_SCAN_SIZE = (96, 128, 128)\n",
    "CLIP_BOUND_HU = [-1000.0, 400.0]\n",
    "PIXEL_MEAN = 0.25\n",
    "\n",
    "# Training Params\n",
    "NUM_CLASSES = 1\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 150\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# <<< --- ADDED: Scan limit per class --- >>>\n",
    "SCAN_LIMIT_PER_CLASS = 50\n",
    "# <<< --- END ADDED --- >>>\n",
    "\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PREPROCESSED_DSB_PATH, exist_ok=True) # Use new path\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True # Ensure reproducibility if using CuDNN\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# %% --- Data Loading and Selection ---\n",
    "\n",
    "print(f\"--- Loading Data and Selecting up to {SCAN_LIMIT_PER_CLASS} Scans Per Class ---\")\n",
    "\n",
    "# --- Verify Paths ---\n",
    "if not os.path.isdir(DSB_PATH): raise SystemExit(f\"ERROR: DSB Scans path not found: {DSB_PATH}\")\n",
    "if not os.path.isfile(DSB_LABELS_CSV): raise SystemExit(f\"ERROR: DSB Labels CSV not found: {DSB_LABELS_CSV}\")\n",
    "print(f\"DSB Scans path: {DSB_PATH}\")\n",
    "print(f\"DSB Labels CSV: {DSB_LABELS_CSV}\")\n",
    "\n",
    "# --- Load Labels ---\n",
    "try:\n",
    "    dsb_labels_df = pd.read_csv(DSB_LABELS_CSV)\n",
    "    dsb_labels_df = dsb_labels_df.rename(columns={'id': 'patient_id'})\n",
    "    print(f\"Loaded {len(dsb_labels_df)} total DSB patient labels.\")\n",
    "    if 'cancer' not in dsb_labels_df.columns: raise ValueError(\"Labels CSV needs 'cancer' column.\")\n",
    "    print(\"Original label distribution:\\n\", dsb_labels_df['cancer'].value_counts())\n",
    "    # Create a lookup dictionary for labels\n",
    "    patient_labels_all = dsb_labels_df.set_index('patient_id')['cancer'].to_dict()\n",
    "except Exception as e: raise SystemExit(f\"ERROR: Failed to load labels CSV: {e}\")\n",
    "\n",
    "# --- Check Scan Folders ---\n",
    "scan_folders = [f for f in os.listdir(DSB_PATH) if os.path.isdir(os.path.join(DSB_PATH, f))]\n",
    "print(f\"Found {len(scan_folders)} potential patient scan folders.\")\n",
    "found_scan_ids = set(scan_folders)\n",
    "\n",
    "# --- Find Common IDs (patients with both label and scan folder) ---\n",
    "labeled_patient_ids_all = set(dsb_labels_df['patient_id'])\n",
    "common_ids_all = labeled_patient_ids_all.intersection(found_scan_ids)\n",
    "print(f\"Found {len(common_ids_all)} patient IDs with both labels and scan folders.\")\n",
    "if not common_ids_all: raise SystemExit(\"No matching patient IDs found. Cannot continue.\")\n",
    "\n",
    "# --- Separate Common IDs by Class ---\n",
    "common_ids_cancer = []\n",
    "common_ids_non_cancer = []\n",
    "for pid in common_ids_all:\n",
    "    label = patient_labels_all.get(pid)\n",
    "    if label == 1:\n",
    "        common_ids_cancer.append(pid)\n",
    "    elif label == 0:\n",
    "        common_ids_non_cancer.append(pid)\n",
    "\n",
    "print(f\"Available Cancerous scans with labels: {len(common_ids_cancer)}\")\n",
    "print(f\"Available Non-Cancerous scans with labels: {len(common_ids_non_cancer)}\")\n",
    "\n",
    "# --- Shuffle and Select Limited Number Per Class ---\n",
    "random.shuffle(common_ids_cancer) # Shuffle in-place\n",
    "random.shuffle(common_ids_non_cancer)\n",
    "\n",
    "selected_cancer_ids = common_ids_cancer[:SCAN_LIMIT_PER_CLASS]\n",
    "selected_non_cancer_ids = common_ids_non_cancer[:SCAN_LIMIT_PER_CLASS]\n",
    "\n",
    "print(f\"Selected {len(selected_cancer_ids)} Cancerous scans.\")\n",
    "print(f\"Selected {len(selected_non_cancer_ids)} Non-Cancerous scans.\")\n",
    "\n",
    "# --- Combine selected IDs for processing ---\n",
    "scans_to_process = selected_cancer_ids + selected_non_cancer_ids\n",
    "random.shuffle(scans_to_process) # Shuffle the combined list\n",
    "\n",
    "print(f\"Total scans selected for preprocessing: {len(scans_to_process)}\")\n",
    "\n",
    "# Use the subset of labels corresponding to the selected scans\n",
    "patient_labels = {pid: patient_labels_all[pid] for pid in scans_to_process}\n",
    "\n",
    "\n",
    "# %% --- Preprocessing Functions (Unchanged) ---\n",
    "# Functions: load_scan_series, resample, get_segmented_lungs,\n",
    "#            normalize_hu, zero_center, resize_scan_to_target\n",
    "# These remain the same as in the original script.\n",
    "\n",
    "def load_scan_series(dicom_folder_path):\n",
    "    try:\n",
    "        series_ids = sitk.ImageSeriesReader.GetGDCMSeriesIDs(dicom_folder_path)\n",
    "        if not series_ids: return None, None, None\n",
    "        series_file_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(dicom_folder_path, series_ids[0])\n",
    "        series_reader = sitk.ImageSeriesReader(); series_reader.SetFileNames(series_file_names)\n",
    "        itkimage = series_reader.Execute()\n",
    "        image_array = sitk.GetArrayFromImage(itkimage); origin = np.array(list(reversed(itkimage.GetOrigin()))); spacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "        return image_array, origin, spacing\n",
    "    except Exception as e: print(f\"Error reading DICOM {os.path.basename(dicom_folder_path)}: {e}\"); return None, None, None\n",
    "\n",
    "def resample(image, original_spacing, new_spacing=TARGET_SPACING):\n",
    "    try:\n",
    "        resize_factor = np.array(original_spacing) / np.array(new_spacing)\n",
    "        new_real_shape = image.shape * resize_factor; new_shape = np.round(new_real_shape)\n",
    "        real_resize_factor = new_shape / image.shape; actual_new_spacing = original_spacing / real_resize_factor\n",
    "        resampled_image = scipy.ndimage.zoom(image, real_resize_factor, mode='nearest', order=1)\n",
    "        return resampled_image, actual_new_spacing\n",
    "    except Exception as e: print(f\"Error resamping: {e}\"); return None, None\n",
    "\n",
    "def get_segmented_lungs(im_slice, hu_threshold=-320):\n",
    "    if im_slice.ndim != 2: return im_slice\n",
    "    binary = im_slice < hu_threshold; cleared = clear_border(binary)\n",
    "    label_image = skimage_label(cleared); areas = [r.area for r in regionprops(label_image)]; areas.sort()\n",
    "    area_threshold = areas[-2] if len(areas) >= 2 else (areas[-1] if len(areas) == 1 else 0)\n",
    "    if area_threshold > 0:\n",
    "        for region in regionprops(label_image):\n",
    "            if region.area < area_threshold:\n",
    "                for coordinates in region.coords: label_image[coordinates[0], coordinates[1]] = 0\n",
    "    binary = label_image > 0; selem = disk(2); binary = binary_closing(binary, selem)\n",
    "    selem_dilate = disk(5); final_mask = ndi.binary_dilation(binary, structure=selem_dilate)\n",
    "    background_val = CLIP_BOUND_HU[0] - 1; segmented_slice = im_slice.copy()\n",
    "    segmented_slice[final_mask == 0] = background_val\n",
    "    return segmented_slice\n",
    "\n",
    "def normalize_hu(image, clip_bounds=CLIP_BOUND_HU):\n",
    "    min_bound, max_bound = clip_bounds\n",
    "    image = np.clip(image, min_bound, max_bound)\n",
    "    image = (image - min_bound) / (max_bound - min_bound)\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def zero_center(image, pixel_mean=PIXEL_MEAN):\n",
    "    image = image - pixel_mean\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def resize_scan_to_target(image, target_shape=FINAL_SCAN_SIZE):\n",
    "     if image.shape == target_shape: return image\n",
    "     resize_factor = np.array(target_shape) / np.array(image.shape)\n",
    "     try:\n",
    "         resized_image = scipy.ndimage.zoom(image, resize_factor, order=1, mode='nearest')\n",
    "         if resized_image.shape != target_shape: # Simple crop/pad correction\n",
    "              current_shape = resized_image.shape\n",
    "              diff = np.array(target_shape) - np.array(current_shape)\n",
    "              pad = np.maximum(diff, 0); crop = np.maximum(-diff, 0)\n",
    "              pad_width = tuple((p // 2, p - p // 2) for p in pad)\n",
    "              resized_image = np.pad(resized_image, pad_width, mode='edge')\n",
    "              crop_slice = tuple(slice(c // 2, s - (c - c // 2)) for c, s in zip(crop, resized_image.shape))\n",
    "              resized_image = resized_image[crop_slice]\n",
    "         if resized_image.shape != target_shape: print(f\"ERROR: Resize failed. Shape {resized_image.shape}\"); return None\n",
    "         return resized_image.astype(np.float32)\n",
    "     except Exception as e: print(f\"Error resizing to target: {e}\"); return None\n",
    "\n",
    "# --- Full Preprocessing Pipeline (Unchanged Logic, uses correct paths) ---\n",
    "def preprocess_scan_dsb(patient_id, input_base_path, output_base_path, force_preprocess=False):\n",
    "    scan_folder_path = os.path.join(input_base_path, patient_id)\n",
    "    output_filename = os.path.join(output_base_path, f\"{patient_id}.npz\") # Uses the passed output_base_path\n",
    "    if os.path.exists(output_filename) and not force_preprocess: return True\n",
    "\n",
    "    image, origin, spacing = load_scan_series(scan_folder_path)\n",
    "    if image is None: return False\n",
    "    resampled_image, new_spacing = resample(image, spacing, TARGET_SPACING)\n",
    "    if resampled_image is None: del image; return False;\n",
    "    del image;\n",
    "    segmented_lungs = np.zeros_like(resampled_image, dtype=np.float32)\n",
    "    for i in range(resampled_image.shape[0]): segmented_lungs[i] = get_segmented_lungs(resampled_image[i])\n",
    "    del resampled_image;\n",
    "    normalized_image = normalize_hu(segmented_lungs, clip_bounds=CLIP_BOUND_HU); del segmented_lungs;\n",
    "    centered_image = zero_center(normalized_image, pixel_mean=PIXEL_MEAN); del normalized_image;\n",
    "    final_image = resize_scan_to_target(centered_image, target_shape=FINAL_SCAN_SIZE); del centered_image;\n",
    "    if final_image is None: return False\n",
    "    try:\n",
    "        np.savez_compressed(output_filename, image=final_image.astype(np.float32))\n",
    "        return True\n",
    "    except Exception as e: print(f\"Error saving {patient_id}: {e}\"); return False\n",
    "\n",
    "\n",
    "# %% --- Preprocessing Execution (Using Selected IDs) ---\n",
    "\n",
    "successful_processed_ids = []\n",
    "failed_processed_ids = []\n",
    "\n",
    "print(f\"\\nStarting preprocessing for {len(scans_to_process)} selected scans...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for patient_id in tqdm(scans_to_process, desc=f\"Preprocessing {len(scans_to_process)} Selected Scans\"):\n",
    "    # <<< --- Pass the CORRECT output path --- >>>\n",
    "    success = preprocess_scan_dsb(patient_id, DSB_PATH, PREPROCESSED_DSB_PATH, force_preprocess=False)\n",
    "    if success:\n",
    "        successful_processed_ids.append(patient_id)\n",
    "    else:\n",
    "        failed_processed_ids.append(patient_id)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nPreprocessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Successfully processed/found: {len(successful_processed_ids)} scans.\")\n",
    "if failed_processed_ids: print(f\"Failed to process: {len(failed_processed_ids)} scans. IDs: {failed_processed_ids}\")\n",
    "\n",
    "# --- Final list for Dataset (only successfully processed from the selection) ---\n",
    "final_patient_list = successful_processed_ids\n",
    "if not final_patient_list: raise SystemExit(\"No scans processed successfully. Cannot continue.\")\n",
    "\n",
    "# Update patient_labels to only include successfully processed patients\n",
    "patient_labels = {pid: patient_labels[pid] for pid in final_patient_list}\n",
    "print(f\"Final patient count for training/validation: {len(final_patient_list)}\")\n",
    "final_cancer_count = sum(1 for pid in final_patient_list if patient_labels[pid] == 1)\n",
    "final_non_cancer_count = len(final_patient_list) - final_cancer_count\n",
    "print(f\"  Cancerous: {final_cancer_count}, Non-Cancerous: {final_non_cancer_count}\")\n",
    "\n",
    "\n",
    "# %% --- Dataset and DataLoader ---\n",
    "\n",
    "class PatientLevelDataset(Dataset):\n",
    "    # --- Dataset class remains UNCHANGED ---\n",
    "    def __init__(self, patient_ids, labels_dict, preprocessed_path):\n",
    "        self.patient_ids = patient_ids; self.labels_dict = labels_dict; self.preprocessed_path = preprocessed_path\n",
    "    def __len__(self): return len(self.patient_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]; label = self.labels_dict[patient_id]\n",
    "        scan_path = os.path.join(self.preprocessed_path, f\"{patient_id}.npz\")\n",
    "        try:\n",
    "            with np.load(scan_path) as npz_data: image = npz_data['image']\n",
    "            image_tensor = torch.from_numpy(image).float().unsqueeze(0)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            return image_tensor, label_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading {patient_id}: {e}\"); dummy = torch.zeros((1, *FINAL_SCAN_SIZE), dtype=torch.float32)\n",
    "            return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "\n",
    "# --- Split Data (Train/Validation) ---\n",
    "# <<< --- Stratification is important again --- >>>\n",
    "train_ids, val_ids = train_test_split(\n",
    "    final_patient_list,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=[patient_labels[pid] for pid in final_patient_list] # Stratify based on the labels of the processed subset\n",
    ")\n",
    "print(f\"\\nTraining patients: {len(train_ids)}\")\n",
    "print(f\"Validation patients: {len(val_ids)}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "# <<< --- Use the correct PREPROCESSED_DSB_PATH --- >>>\n",
    "train_dataset = PatientLevelDataset(train_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "val_dataset = PatientLevelDataset(val_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "print(f\"Using {NUM_WORKERS} workers for DataLoader.\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# --- Check DataLoader Output ---\n",
    "try:\n",
    "    print(\"\\nChecking DataLoader output (Limited Data)...\")\n",
    "    if len(train_loader) > 0:\n",
    "        sample_batch, sample_labels = next(iter(train_loader))\n",
    "        print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "        print(f\"Sample labels shape: {sample_labels.shape}\")\n",
    "        print(f\"Sample labels: {sample_labels}\") # Should contain 0s and 1s\n",
    "        if torch.any(sample_labels == -1): print(\"WARNING: Error labels (-1) detected.\")\n",
    "    else: print(\"Train loader empty.\")\n",
    "except Exception as e: print(f\"Error checking DataLoader: {e}\")\n",
    "\n",
    "\n",
    "# %% --- Model Definition (Unchanged) ---\n",
    "class PatientLevel3DCNN(nn.Module):\n",
    "    def __init__(self, input_shape=FINAL_SCAN_SIZE, input_channels=1, num_classes=1):\n",
    "        super(PatientLevel3DCNN, self).__init__()\n",
    "\n",
    "        # --- Reduced Convolutional Layers ---\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: [B, 1, D, H, W] e.g. [B, 1, 96, 128, 128]\n",
    "            nn.Conv3d(input_channels, 16, kernel_size=3, stride=1, padding=1), # Keep size\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2), # D/2, H/2, W/2 -> [B, 16, 48, 64, 64]\n",
    "\n",
    "            nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2), # D/4, H/4, W/4 -> [B, 32, 24, 32, 32]\n",
    "\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2), # D/8, H/8, W/8 -> [B, 64, 12, 16, 16]\n",
    "\n",
    "            # <<< --- Removed the Conv3d(64, 128, ...) block --- >>>\n",
    "            # nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm3d(128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool3d(kernel_size=2, stride=2), # D/16, H/16, W/16 -> [B, 128, 6, 8, 8]\n",
    "            # <<< --- End Removal --- >>>\n",
    "\n",
    "\n",
    "            # Adaptive pooling ensures fixed size before FC layers\n",
    "            # Input to this layer is now [B, 64, 12, 16, 16]\n",
    "            nn.AdaptiveMaxPool3d((2, 2, 2)) # Output size: [B, 64, 2, 2, 2]\n",
    "        )\n",
    "\n",
    "        # <<< --- Adjusted Flattened Size --- >>>\n",
    "        # Calculate flattened size based on the output of the last pooling layer (64 channels * 2 * 2 * 2)\n",
    "        flattened_size = 64 * 2 * 2 * 2  # 64 * 8 = 512\n",
    "        # <<< --- End Adjustment --- >>>\n",
    "\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # <<< --- Adjusted Linear Layer Input Size --- >>>\n",
    "            nn.Linear(flattened_size, 256), # Use the new flattened_size\n",
    "            # <<< --- End Adjustment --- >>>\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # Keep dropout for regularization\n",
    "            nn.Linear(256, num_classes) # Output 1 logit\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"Shape after conv_layers:\", x.shape) # Optional: for debugging\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "patient_model = PatientLevel3DCNN(input_shape=FINAL_SCAN_SIZE, num_classes=1).to(DEVICE)\n",
    "print(patient_model)\n",
    "try:\n",
    "    dummy_input = torch.randn(BATCH_SIZE, 1, *FINAL_SCAN_SIZE).to(DEVICE)\n",
    "    output = patient_model(dummy_input); print(f\"\\nModel output shape: {output.shape}\")\n",
    "except Exception as e: print(f\"\\nError model test: {e}\")\n",
    "\n",
    "\n",
    "# %% --- Loss and Optimizer ---\n",
    "\n",
    "# <<< --- Adjust pos_weight: Since we selected a balanced subset, weight should be close to 1.0 --- >>>\n",
    "# Calculate weight based on the *actually processed* training samples if desired,\n",
    "# but for a forced 50/50 split, weight=1 is reasonable.\n",
    "# Let's calculate it based on the final training set composition for accuracy.\n",
    "train_labels_list = [patient_labels[pid] for pid in train_ids]\n",
    "count_0 = train_labels_list.count(0)\n",
    "count_1 = train_labels_list.count(1)\n",
    "if count_1 > 0 and count_0 > 0:\n",
    "    pos_weight_val = count_0 / count_1\n",
    "    print(f\"Calculated positive weight for balanced training subset: {pos_weight_val:.4f}\")\n",
    "    pos_weight_tensor = torch.tensor([pos_weight_val], device=DEVICE)\n",
    "else:\n",
    "    print(\"Warning: Training set has only one class after split. Using default pos_weight=1.\")\n",
    "    pos_weight_tensor = torch.tensor([1.0], device=DEVICE) # Default if split results in one class\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "# <<< --- END pos_weight Adjustment --- >>>\n",
    "\n",
    "optimizer = optim.Adam(patient_model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# %% --- Training and Validation Functions (Unchanged Logic) ---\n",
    "def train_one_epoch_patient(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train(); running_loss = 0.0; total_samples = 0; correct_predictions = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for inputs, labels in progress_bar:\n",
    "        valid_indices = labels != -1; inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].unsqueeze(1).to(device)\n",
    "        if inputs.nelement() == 0: continue\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "            outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "        if torch.isnan(loss): print(\"NaN loss!\"); continue\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        running_loss += loss.item() * inputs.size(0); total_samples += inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5; correct_predictions += (preds == labels.bool()).sum().item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    if total_samples == 0: return 0.0, 0.0\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def validate_patient(model, dataloader, criterion, device):\n",
    "    model.eval(); running_loss = 0.0; total_samples = 0; all_preds_proba = []; all_labels = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in progress_bar:\n",
    "            valid_indices = labels != -1; inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].unsqueeze(1).to(device)\n",
    "            if inputs.nelement() == 0: continue\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "                outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0); total_samples += inputs.size(0)\n",
    "            all_preds_proba.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    if total_samples == 0: return 0.0, np.array([]), np.array([])\n",
    "    return running_loss / total_samples, np.array(all_labels).flatten(), np.array(all_preds_proba).flatten()\n",
    "\n",
    "\n",
    "# %% --- Training Loop ---\n",
    "print(f\"\\nStarting Training on Limited ({len(final_patient_list)}) Scan Dataset...\")\n",
    "best_val_loss = float('inf'); train_losses, val_losses, train_accs = [], [], []\n",
    "\n",
    "# <<< --- Save model in the new directory --- >>>\n",
    "MODEL_SAVE_PATH = os.path.join(PREPROCESSED_DSB_PATH, \"patient_level_model_50_each_best.pth\")\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch_patient(patient_model, train_loader, criterion, optimizer, DEVICE, scaler)\n",
    "    val_loss, val_labels_epoch, val_preds_proba_epoch = validate_patient(patient_model, val_loader, criterion, DEVICE)\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss); train_accs.append(train_acc)\n",
    "    end_epoch_time = time.time(); epoch_duration = end_epoch_time - start_epoch_time\n",
    "\n",
    "    val_acc_epoch = 0.0\n",
    "    if len(val_labels_epoch) > 0: val_acc_epoch = accuracy_score(val_labels_epoch, (val_preds_proba_epoch > 0.5).astype(int))\n",
    "    print(f\"Epoch {epoch+1} Summary: Duration: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc_epoch:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss and len(val_labels_epoch) > 0:\n",
    "        best_val_loss = val_loss\n",
    "        try: torch.save(patient_model.state_dict(), MODEL_SAVE_PATH); print(f\"  Best model saved to {MODEL_SAVE_PATH}\")\n",
    "        except Exception as e: print(f\"Error saving model: {e}\")\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nLimited Scan Dataset Training Finished.\")\n",
    "\n",
    "\n",
    "# %% --- Plot Training History ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1); plt.plot(range(1, EPOCHS + 1), train_losses, label='Train'); plt.plot(range(1, EPOCHS + 1), val_losses, label='Val')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss Curve (50 Each)'); plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2); plt.plot(range(1, EPOCHS + 1), train_accs, label='Train Acc')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy Curve (50 Each)'); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# %% --- Model Evaluation ---\n",
    "print(\"\\nEvaluating Model on Limited Validation Set...\")\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    try:\n",
    "        if 'patient_model' not in locals(): patient_model = PatientLevel3DCNN(input_shape=FINAL_SCAN_SIZE, num_classes=1).to(DEVICE)\n",
    "        patient_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "        print(f\"Loaded best model from {MODEL_SAVE_PATH}\")\n",
    "    except Exception as e: print(f\"Could not load best model: {e}. Using last epoch model.\")\n",
    "else: print(\"Best model file not found. Using last epoch model.\")\n",
    "\n",
    "val_loss_final, final_val_labels, final_val_preds_proba = validate_patient(patient_model, val_loader, criterion, DEVICE)\n",
    "\n",
    "if len(final_val_labels) == 0: print(\"No valid validation predictions. Cannot evaluate.\")\n",
    "else:\n",
    "    print(f\"\\nFinal Validation Loss: {val_loss_final:.4f}\")\n",
    "    final_val_preds_binary = (final_val_preds_proba > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(final_val_labels, final_val_preds_binary)\n",
    "    precision = precision_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    recall = recall_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    f1 = f1_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    try: auc_roc = roc_auc_score(final_val_labels, final_val_preds_proba)\n",
    "    except ValueError as e: print(f\"AUC-ROC Error: {e}\"); auc_roc = float('nan')\n",
    "\n",
    "    print(\"\\n--- Final Validation Metrics (Limited Data) ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\"); print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\"); print(f\"F1-Score:  {f1:.4f}\"); print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (Limited Data):\")\n",
    "    target_names = ['Non-Cancer (0)', 'Cancer (1)']\n",
    "    print(classification_report(final_val_labels, final_val_preds_binary, target_names=target_names, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Limited Data):\")\n",
    "    cm = confusion_matrix(final_val_labels, final_val_preds_binary); disp = ConfusionMatrixDisplay(cm, display_labels=target_names)\n",
    "    disp.plot(cmap=plt.cm.Blues); plt.show()\n",
    "\n",
    "    if not np.isnan(auc_roc):\n",
    "        fpr, tpr, _ = roc_curve(final_val_labels, final_val_preds_proba)\n",
    "        plt.figure(figsize=(8, 6)); plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_roc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--'); plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve (Limited Data)')\n",
    "        plt.legend(loc=\"lower right\"); plt.grid(True); plt.show()\n",
    "import torch.nn.functional as F\n",
    "# <<< --- MODIFIED: New output path for the limited dataset --- >>>\n",
    "PREPROCESSED_DSB_PATH = './preprocessed_dsb_50_each_64cube/' # Changed suffix to reflect size\n",
    "# ---\n",
    "\n",
    "# Preprocessing & Model Params\n",
    "TARGET_SPACING = [1.5, 1.5, 1.5]\n",
    "# <<< --- !!! CRITICAL: Changed FINAL_SCAN_SIZE to match Keras model input !!! --- >>>\n",
    "FINAL_SCAN_SIZE = (64, 64, 64)\n",
    "# <<< --- END CRITICAL CHANGE --- >>>\n",
    "print(f\"*** Using FINAL_SCAN_SIZE: {FINAL_SCAN_SIZE} to match Simp3D model ***\")\n",
    "\n",
    "CLIP_BOUND_HU = [-1000.0, 400.0]\n",
    "PIXEL_MEAN = 0.25\n",
    "\n",
    "# Training Params\n",
    "NUM_CLASSES = 1 # For BCEWithLogitsLoss\n",
    "BATCH_SIZE = 4  # Adjust based on GPU memory with the new model\n",
    "LEARNING_RATE = 4e-5 # Using the LR from the Keras model definition\n",
    "EPOCHS = 150\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Scan limit per class\n",
    "SCAN_LIMIT_PER_CLASS = 50\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PREPROCESSED_DSB_PATH, exist_ok=True) # Use new path\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# %% --- Data Loading and Selection ---\n",
    "# (This section remains the same as your provided code)\n",
    "# It will select 50 cancer and 50 non-cancer scans if available.\n",
    "# ... (Keep the existing data loading/selection code here) ...\n",
    "print(f\"--- Loading Data and Selecting up to {SCAN_LIMIT_PER_CLASS} Scans Per Class ---\")\n",
    "\n",
    "# --- Verify Paths ---\n",
    "if not os.path.isdir(DSB_PATH): raise SystemExit(f\"ERROR: DSB Scans path not found: {DSB_PATH}\")\n",
    "if not os.path.isfile(DSB_LABELS_CSV): raise SystemExit(f\"ERROR: DSB Labels CSV not found: {DSB_LABELS_CSV}\")\n",
    "print(f\"DSB Scans path: {DSB_PATH}\")\n",
    "print(f\"DSB Labels CSV: {DSB_LABELS_CSV}\")\n",
    "\n",
    "# --- Load Labels ---\n",
    "try:\n",
    "    dsb_labels_df = pd.read_csv(DSB_LABELS_CSV)\n",
    "    dsb_labels_df = dsb_labels_df.rename(columns={'id': 'patient_id'})\n",
    "    print(f\"Loaded {len(dsb_labels_df)} total DSB patient labels.\")\n",
    "    if 'cancer' not in dsb_labels_df.columns: raise ValueError(\"Labels CSV needs 'cancer' column.\")\n",
    "    print(\"Original label distribution:\\n\", dsb_labels_df['cancer'].value_counts())\n",
    "    patient_labels_all = dsb_labels_df.set_index('patient_id')['cancer'].to_dict()\n",
    "except Exception as e: raise SystemExit(f\"ERROR: Failed to load labels CSV: {e}\")\n",
    "\n",
    "# --- Check Scan Folders ---\n",
    "scan_folders = [f for f in os.listdir(DSB_PATH) if os.path.isdir(os.path.join(DSB_PATH, f))]\n",
    "print(f\"Found {len(scan_folders)} potential patient scan folders.\")\n",
    "found_scan_ids = set(scan_folders)\n",
    "\n",
    "# --- Find Common IDs (patients with both label and scan folder) ---\n",
    "labeled_patient_ids_all = set(dsb_labels_df['patient_id'])\n",
    "common_ids_all = labeled_patient_ids_all.intersection(found_scan_ids)\n",
    "print(f\"Found {len(common_ids_all)} patient IDs with both labels and scan folders.\")\n",
    "if not common_ids_all: raise SystemExit(\"No matching patient IDs found. Cannot continue.\")\n",
    "\n",
    "# --- Separate Common IDs by Class ---\n",
    "common_ids_cancer = []\n",
    "common_ids_non_cancer = []\n",
    "for pid in common_ids_all:\n",
    "    label = patient_labels_all.get(pid)\n",
    "    if label == 1: common_ids_cancer.append(pid)\n",
    "    elif label == 0: common_ids_non_cancer.append(pid)\n",
    "\n",
    "print(f\"Available Cancerous scans with labels: {len(common_ids_cancer)}\")\n",
    "print(f\"Available Non-Cancerous scans with labels: {len(common_ids_non_cancer)}\")\n",
    "\n",
    "# --- Shuffle and Select Limited Number Per Class ---\n",
    "random.shuffle(common_ids_cancer); random.shuffle(common_ids_non_cancer)\n",
    "selected_cancer_ids = common_ids_cancer[:SCAN_LIMIT_PER_CLASS]\n",
    "selected_non_cancer_ids = common_ids_non_cancer[:SCAN_LIMIT_PER_CLASS]\n",
    "print(f\"Selected {len(selected_cancer_ids)} Cancerous scans.\")\n",
    "print(f\"Selected {len(selected_non_cancer_ids)} Non-Cancerous scans.\")\n",
    "\n",
    "# --- Combine selected IDs for processing ---\n",
    "scans_to_process = selected_cancer_ids + selected_non_cancer_ids\n",
    "random.shuffle(scans_to_process)\n",
    "print(f\"Total scans selected for preprocessing: {len(scans_to_process)}\")\n",
    "patient_labels = {pid: patient_labels_all[pid] for pid in scans_to_process}\n",
    "\n",
    "\n",
    "# %% --- Preprocessing Functions ---\n",
    "# (Functions load_scan_series, resample, get_segmented_lungs,\n",
    "#  normalize_hu, zero_center remain unchanged)\n",
    "def load_scan_series(dicom_folder_path):\n",
    "    try:\n",
    "        series_ids = sitk.ImageSeriesReader.GetGDCMSeriesIDs(dicom_folder_path)\n",
    "        if not series_ids: return None, None, None\n",
    "        series_file_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(dicom_folder_path, series_ids[0])\n",
    "        series_reader = sitk.ImageSeriesReader(); series_reader.SetFileNames(series_file_names)\n",
    "        itkimage = series_reader.Execute()\n",
    "        image_array = sitk.GetArrayFromImage(itkimage); origin = np.array(list(reversed(itkimage.GetOrigin()))); spacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "        return image_array, origin, spacing\n",
    "    except Exception as e: print(f\"Error reading DICOM {os.path.basename(dicom_folder_path)}: {e}\"); return None, None, None\n",
    "\n",
    "def resample(image, original_spacing, new_spacing=TARGET_SPACING):\n",
    "    try:\n",
    "        resize_factor = np.array(original_spacing) / np.array(new_spacing)\n",
    "        new_real_shape = image.shape * resize_factor; new_shape = np.round(new_real_shape)\n",
    "        real_resize_factor = new_shape / image.shape; actual_new_spacing = original_spacing / real_resize_factor\n",
    "        resampled_image = scipy.ndimage.zoom(image, real_resize_factor, mode='nearest', order=1)\n",
    "        return resampled_image, actual_new_spacing\n",
    "    except Exception as e: print(f\"Error resamping: {e}\"); return None, None\n",
    "\n",
    "def get_segmented_lungs(im_slice, hu_threshold=-320):\n",
    "    if im_slice.ndim != 2: return im_slice\n",
    "    binary = im_slice < hu_threshold; cleared = clear_border(binary)\n",
    "    label_image = skimage_label(cleared); areas = [r.area for r in regionprops(label_image)]; areas.sort()\n",
    "    area_threshold = areas[-2] if len(areas) >= 2 else (areas[-1] if len(areas) == 1 else 0)\n",
    "    if area_threshold > 0:\n",
    "        for region in regionprops(label_image):\n",
    "            if region.area < area_threshold:\n",
    "                for coordinates in region.coords: label_image[coordinates[0], coordinates[1]] = 0\n",
    "    binary = label_image > 0; selem = disk(2); binary = binary_closing(binary, selem)\n",
    "    selem_dilate = disk(5); final_mask = ndi.binary_dilation(binary, structure=selem_dilate)\n",
    "    background_val = CLIP_BOUND_HU[0] - 1; segmented_slice = im_slice.copy()\n",
    "    segmented_slice[final_mask == 0] = background_val\n",
    "    return segmented_slice\n",
    "\n",
    "def normalize_hu(image, clip_bounds=CLIP_BOUND_HU):\n",
    "    min_bound, max_bound = clip_bounds\n",
    "    image = np.clip(image, min_bound, max_bound)\n",
    "    image = (image - min_bound) / (max_bound - min_bound)\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def zero_center(image, pixel_mean=PIXEL_MEAN):\n",
    "    image = image - pixel_mean\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "\n",
    "# --- Modified resize_scan_to_target to use the new FINAL_SCAN_SIZE ---\n",
    "def resize_scan_to_target(image, target_shape=FINAL_SCAN_SIZE): # Uses global FINAL_SCAN_SIZE\n",
    "     if image.shape == target_shape: return image\n",
    "     resize_factor = np.array(target_shape) / np.array(image.shape)\n",
    "     try:\n",
    "         # Using order=1 (bilinear) for resizing, 'nearest' might lose info\n",
    "         resized_image = scipy.ndimage.zoom(image, resize_factor, order=1, mode='nearest')\n",
    "         # Simple crop/pad if zoom result is slightly off due to rounding\n",
    "         if resized_image.shape != target_shape:\n",
    "              current_shape = resized_image.shape\n",
    "              diff = np.array(target_shape) - np.array(current_shape)\n",
    "              pad = np.maximum(diff, 0); crop = np.maximum(-diff, 0)\n",
    "              pad_width = tuple((p // 2, p - p // 2) for p in pad)\n",
    "              resized_image = np.pad(resized_image, pad_width, mode='edge') # Pad with edge value\n",
    "              crop_slice = tuple(slice(c // 2, s - (c - c // 2)) for c, s in zip(crop, resized_image.shape))\n",
    "              resized_image = resized_image[crop_slice]\n",
    "         # Final check\n",
    "         if resized_image.shape != target_shape:\n",
    "             print(f\"ERROR: Resize failed. Target: {target_shape}, Got: {resized_image.shape}\")\n",
    "             return None\n",
    "         return resized_image.astype(np.float32)\n",
    "     except Exception as e:\n",
    "         print(f\"Error resizing image of shape {image.shape} to target {target_shape}: {e}\")\n",
    "         return None\n",
    "\n",
    "\n",
    "# --- Full Preprocessing Pipeline (Unchanged Logic, uses correct paths and FINAL_SCAN_SIZE) ---\n",
    "def preprocess_scan_dsb(patient_id, input_base_path, output_base_path, force_preprocess=False):\n",
    "    scan_folder_path = os.path.join(input_base_path, patient_id)\n",
    "    output_filename = os.path.join(output_base_path, f\"{patient_id}.npz\")\n",
    "    if os.path.exists(output_filename) and not force_preprocess:\n",
    "        # print(f\"Skipping {patient_id}, already preprocessed.\")\n",
    "        return True # Already exists\n",
    "\n",
    "    # print(f\"Processing {patient_id}...\")\n",
    "    image, origin, spacing = load_scan_series(scan_folder_path)\n",
    "    if image is None: return False\n",
    "\n",
    "    # print(f\"  Original shape: {image.shape}, spacing: {np.round(spacing, 2)}\")\n",
    "    resampled_image, new_spacing = resample(image, spacing, TARGET_SPACING)\n",
    "    if resampled_image is None: del image; return False\n",
    "    # print(f\"  Resampled shape: {resampled_image.shape}, spacing: {np.round(new_spacing, 2)}\")\n",
    "    del image\n",
    "\n",
    "    # Segment lungs slice by slice\n",
    "    segmented_lungs = np.zeros_like(resampled_image, dtype=np.float32)\n",
    "    for i in range(resampled_image.shape[0]):\n",
    "        segmented_lungs[i] = get_segmented_lungs(resampled_image[i])\n",
    "    del resampled_image\n",
    "\n",
    "    # Normalize HU\n",
    "    normalized_image = normalize_hu(segmented_lungs, clip_bounds=CLIP_BOUND_HU)\n",
    "    del segmented_lungs\n",
    "\n",
    "    # Zero center\n",
    "    centered_image = zero_center(normalized_image, pixel_mean=PIXEL_MEAN)\n",
    "    del normalized_image\n",
    "\n",
    "    # Resize to final target shape (e.g., 64x64x64)\n",
    "    final_image = resize_scan_to_target(centered_image, target_shape=FINAL_SCAN_SIZE) # Uses the global\n",
    "    del centered_image\n",
    "    if final_image is None: return False\n",
    "    # print(f\"  Final shape: {final_image.shape}\")\n",
    "\n",
    "    # Save compressed\n",
    "    try:\n",
    "        np.savez_compressed(output_filename, image=final_image.astype(np.float32))\n",
    "        # print(f\"  Saved to {output_filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {patient_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# %% --- Preprocessing Execution (Using Selected IDs) ---\n",
    "# Ensure you re-run this if you changed FINAL_SCAN_SIZE\n",
    "print(f\"\\n--- Starting Preprocessing for {len(scans_to_process)} scans to size {FINAL_SCAN_SIZE} ---\")\n",
    "print(f\"Output directory: {PREPROCESSED_DSB_PATH}\")\n",
    "# Set force_preprocess=True if you need to overwrite existing files with the new size\n",
    "FORCE_REPROCESS = False # Set to True to re-process all selected scans\n",
    "\n",
    "successful_processed_ids = []\n",
    "failed_processed_ids = []\n",
    "start_time = time.time()\n",
    "\n",
    "for patient_id in tqdm(scans_to_process, desc=f\"Preprocessing {len(scans_to_process)} Scans ({FINAL_SCAN_SIZE})\"):\n",
    "    success = preprocess_scan_dsb(patient_id, DSB_PATH, PREPROCESSED_DSB_PATH, force_preprocess=FORCE_REPROCESS)\n",
    "    if success:\n",
    "        successful_processed_ids.append(patient_id)\n",
    "    else:\n",
    "        failed_processed_ids.append(patient_id)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nPreprocessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"Successfully processed/found: {len(successful_processed_ids)} scans.\")\n",
    "if failed_processed_ids: print(f\"Failed to process: {len(failed_processed_ids)} scans. IDs: {failed_processed_ids}\")\n",
    "\n",
    "# --- Final list for Dataset (only successfully processed from the selection) ---\n",
    "final_patient_list = successful_processed_ids\n",
    "if not final_patient_list: raise SystemExit(\"No scans processed successfully. Cannot continue.\")\n",
    "\n",
    "# Update patient_labels to only include successfully processed patients\n",
    "patient_labels = {pid: patient_labels[pid] for pid in final_patient_list}\n",
    "print(f\"Final patient count for training/validation: {len(final_patient_list)}\")\n",
    "final_cancer_count = sum(1 for pid in final_patient_list if patient_labels[pid] == 1)\n",
    "final_non_cancer_count = len(final_patient_list) - final_cancer_count\n",
    "print(f\"  Cancerous: {final_cancer_count}, Non-Cancerous: {final_non_cancer_count}\")\n",
    "\n",
    "\n",
    "# %% --- Dataset and DataLoader ---\n",
    "\n",
    "class PatientLevelDataset(Dataset):\n",
    "    # --- Dataset class remains UNCHANGED ---\n",
    "    def __init__(self, patient_ids, labels_dict, preprocessed_path):\n",
    "        self.patient_ids = patient_ids; self.labels_dict = labels_dict; self.preprocessed_path = preprocessed_path\n",
    "        self.target_size = FINAL_SCAN_SIZE # Store target size for checks\n",
    "\n",
    "    def __len__(self): return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]; label = self.labels_dict[patient_id]\n",
    "        scan_path = os.path.join(self.preprocessed_path, f\"{patient_id}.npz\")\n",
    "        try:\n",
    "            with np.load(scan_path) as npz_data: image = npz_data['image']\n",
    "            # Verify shape\n",
    "            if image.shape != self.target_size:\n",
    "                print(f\"ERROR: Shape mismatch for {patient_id}. Expected {self.target_size}, got {image.shape}. Skipping.\")\n",
    "                dummy = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "                return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "\n",
    "            image_tensor = torch.from_numpy(image).float().unsqueeze(0) # Add channel dim\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            return image_tensor, label_tensor\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERROR: File not found {scan_path}. Skipping.\")\n",
    "             dummy = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "             return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading {patient_id}: {e}. Skipping.\")\n",
    "            dummy = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "            return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "\n",
    "# --- Split Data (Train/Validation) ---\n",
    "train_ids, val_ids = train_test_split(\n",
    "    final_patient_list,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=[patient_labels[pid] for pid in final_patient_list] # Stratify\n",
    ")\n",
    "print(f\"\\nTraining patients: {len(train_ids)}\")\n",
    "print(f\"Validation patients: {len(val_ids)}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = PatientLevelDataset(train_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "val_dataset = PatientLevelDataset(val_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "\n",
    "NUM_WORKERS = 0 # Set to 0 for easier debugging if issues arise\n",
    "print(f\"Using {NUM_WORKERS} workers for DataLoader.\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# --- Check DataLoader Output ---\n",
    "try:\n",
    "    print(\"\\nChecking DataLoader output...\")\n",
    "    if len(train_loader) > 0:\n",
    "        sample_batch, sample_labels = next(iter(train_loader))\n",
    "        print(f\"Sample batch shape: {sample_batch.shape}\") # Should be [B, 1, 64, 64, 64]\n",
    "        print(f\"Sample labels shape: {sample_labels.shape}\")\n",
    "        print(f\"Sample labels: {sample_labels}\")\n",
    "        if torch.any(sample_labels == -1): print(\"WARNING: Error labels (-1) detected in first batch.\")\n",
    "    else: print(\"Train loader empty.\")\n",
    "    if len(val_loader) > 0:\n",
    "        sample_batch_val, _ = next(iter(val_loader))\n",
    "        print(f\"Validation batch shape: {sample_batch_val.shape}\")\n",
    "    else: print(\"Validation loader empty.\")\n",
    "except Exception as e: print(f\"Error checking DataLoader: {e}\")\n",
    "\n",
    "\n",
    "# %% --- Model Definition (Simp3DNet - PyTorch version of get_simp3d) ---\n",
    "\n",
    "# Filter numbers from the Keras model definition\n",
    "num_filters = [16, 32, 64, 128, 256, 1028]\n",
    "\n",
    "class Simp3DNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=1): # num_classes=1 for BCEWithLogitsLoss\n",
    "        super(Simp3DNet, self).__init__()\n",
    "\n",
    "        # Block 1: Input (B, 1, 64, 64, 64)\n",
    "        # Conv 9x9x9, valid -> (B, 16, 56, 56, 56)\n",
    "        self.conv1a = nn.Conv3d(input_channels, num_filters[0], kernel_size=9, padding=0)\n",
    "        self.bn1a = nn.BatchNorm3d(num_filters[0])\n",
    "        # Conv 3x3x3, valid -> (B, 16, 54, 54, 54)\n",
    "        self.conv1b = nn.Conv3d(num_filters[0], num_filters[0], kernel_size=3, padding=0)\n",
    "        self.bn1b = nn.BatchNorm3d(num_filters[0])\n",
    "        # Conv 5x5x5, valid -> (B, 16, 50, 50, 50)\n",
    "        self.conv1c = nn.Conv3d(num_filters[0], num_filters[0], kernel_size=5, padding=0)\n",
    "        self.bn1c = nn.BatchNorm3d(num_filters[0])\n",
    "        # Pool 2x2x2 -> (B, 16, 25, 25, 25)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 2\n",
    "        # Conv 3x3x3, valid -> (B, 32, 23, 23, 23)\n",
    "        self.conv2a = nn.Conv3d(num_filters[0], num_filters[1], kernel_size=3, padding=0)\n",
    "        self.bn2a = nn.BatchNorm3d(num_filters[1])\n",
    "        # Conv 3x3x3, valid -> (B, 32, 21, 21, 21)\n",
    "        self.conv2b = nn.Conv3d(num_filters[1], num_filters[1], kernel_size=3, padding=0)\n",
    "        self.bn2b = nn.BatchNorm3d(num_filters[1])\n",
    "        # Pool 2x2x2 -> (B, 32, 10, 10, 10) - Note: floor(21/2) = 10\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 3\n",
    "        # Conv 3x3x3, valid -> (B, 64, 8, 8, 8)\n",
    "        self.conv3a = nn.Conv3d(num_filters[1], num_filters[2], kernel_size=3, padding=0)\n",
    "        self.bn3a = nn.BatchNorm3d(num_filters[2])\n",
    "        # Conv 3x3x3, valid -> (B, 64, 6, 6, 6)\n",
    "        self.conv3b = nn.Conv3d(num_filters[2], num_filters[2], kernel_size=3, padding=0)\n",
    "        self.bn3b = nn.BatchNorm3d(num_filters[2])\n",
    "        # No pooling\n",
    "\n",
    "        # Block 4\n",
    "        # Conv 3x3x3, valid -> (B, 128, 4, 4, 4)\n",
    "        self.conv4a = nn.Conv3d(num_filters[2], num_filters[3], kernel_size=3, padding=0)\n",
    "        self.bn4a = nn.BatchNorm3d(num_filters[3])\n",
    "        # No pooling\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate flattened features: 128 filters * 4 * 4 * 4 volume\n",
    "        flattened_features = num_filters[3] * 4 * 4 * 4 # 128 * 64 = 8192\n",
    "        # print(f\"Calculated flattened features: {flattened_features}\") # For debugging\n",
    "\n",
    "        # Dense Layers\n",
    "        self.fc1 = nn.Linear(flattened_features, 256)\n",
    "        # Keras model applies BN *after* activation for Dense layer. Replicate this.\n",
    "        # Use BatchNorm1d for features after flattening.\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        # Output layer: 1 logit for BCEWithLogitsLoss\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1 - Using Conv -> BN -> ReLU pattern\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = F.relu(self.bn1c(self.conv1c(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Block 3\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "\n",
    "        # Block 4\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "\n",
    "        # Flatten and Dense Layers\n",
    "        x = self.flatten(x)\n",
    "        # print(\"Shape after flatten:\", x.shape) # Debug\n",
    "        x = self.fc1(x)\n",
    "        # print(\"Shape after fc1:\", x.shape) # Debug\n",
    "        # Apply ReLU then BN, matching Keras Dense -> activation -> BN\n",
    "        x = F.relu(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        # Final output layer (logits)\n",
    "        x = self.fc2(x)\n",
    "        # print(\"Shape after fc2:\", x.shape) # Debug\n",
    "        return x\n",
    "\n",
    "# Instantiate the new model\n",
    "patient_model = Simp3DNet(input_channels=1, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "print(\"\\n--- Using Simp3DNet Model ---\")\n",
    "print(patient_model)\n",
    "\n",
    "# Test model with a dummy input\n",
    "try:\n",
    "    # Use the correct FINAL_SCAN_SIZE for the dummy input\n",
    "    dummy_input = torch.randn(BATCH_SIZE, 1, *FINAL_SCAN_SIZE).to(DEVICE)\n",
    "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "    output = patient_model(dummy_input)\n",
    "    print(f\"Model output shape: {output.shape}\") # Should be [B, 1]\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during model test forward pass: {e}\")\n",
    "    # If you get size mismatch errors here, double-check flattened_features calculation\n",
    "    # based on the FINAL_SCAN_SIZE and padding='valid' convolution steps.\n",
    "\n",
    "# %% --- Loss and Optimizer ---\n",
    "\n",
    "# Calculate positive weight based on the *actual* training set composition\n",
    "train_labels_list = [patient_labels[pid] for pid in train_ids]\n",
    "count_0 = train_labels_list.count(0)\n",
    "count_1 = train_labels_list.count(1)\n",
    "if count_1 > 0 and count_0 > 0:\n",
    "    pos_weight_val = count_0 / count_1\n",
    "    print(f\"Calculated positive weight for training subset ({count_1} pos / {count_0} neg): {pos_weight_val:.4f}\")\n",
    "    pos_weight_tensor = torch.tensor([pos_weight_val], device=DEVICE)\n",
    "elif count_1 > 0: # Only positive samples\n",
    "     print(\"Warning: Training set only contains positive samples. Using pos_weight=1.\")\n",
    "     pos_weight_tensor = torch.tensor([1.0], device=DEVICE)\n",
    "elif count_0 > 0: # Only negative samples\n",
    "     print(\"Warning: Training set only contains negative samples. Using pos_weight=1.\")\n",
    "     pos_weight_tensor = torch.tensor([1.0], device=DEVICE)\n",
    "else: # Empty training set?\n",
    "    print(\"Warning: Training set appears empty. Using pos_weight=1.\")\n",
    "    pos_weight_tensor = torch.tensor([1.0], device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "# Use Adam optimizer with the LR from the Keras model definition\n",
    "optimizer = optim.Adam(patient_model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# %% --- Training and Validation Functions (Unchanged Logic) ---\n",
    "# These functions work with single logit output and BCEWithLogitsLoss\n",
    "def train_one_epoch_patient(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train(); running_loss = 0.0; total_samples = 0; correct_predictions = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for inputs, labels in progress_bar:\n",
    "        # Filter out error labels before moving to device\n",
    "        valid_indices = labels != -1\n",
    "        if not torch.any(valid_indices): continue # Skip batch if all are errors\n",
    "        inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].unsqueeze(1).to(device) # Add dim for BCE loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN loss encountered during training! Skipping batch.\")\n",
    "            continue # Skip this batch\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        total_samples += inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5 # Get binary predictions\n",
    "        correct_predictions += (preds == labels.bool()).sum().item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    if total_samples == 0: return 0.0, 0.0\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_patient(model, dataloader, criterion, device):\n",
    "    model.eval(); running_loss = 0.0; total_samples = 0; all_preds_proba = []; all_labels = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in progress_bar:\n",
    "            # Filter out error labels\n",
    "            valid_indices = labels != -1\n",
    "            if not torch.any(valid_indices): continue\n",
    "            inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].to(device) # Keep labels flat for eval metrics\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "                outputs = model(inputs)\n",
    "                # Calculate loss with unsqueezed labels for consistency\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                 print(\"NaN loss encountered during validation!\")\n",
    "                 # Don't add to running loss, but record predictions if needed? Or skip batch?\n",
    "                 continue\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Store predictions (probabilities) and true labels for metrics\n",
    "            all_preds_proba.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    if total_samples == 0: return 0.0, np.array([]), np.array([])\n",
    "    val_loss = running_loss / total_samples\n",
    "    return val_loss, np.array(all_labels), np.array(all_preds_proba)\n",
    "\n",
    "\n",
    "# %% --- Training Loop ---\n",
    "print(f\"\\nStarting Training with Simp3DNet Model ({len(final_patient_list)} scans)...\")\n",
    "best_val_loss = float('inf'); train_losses, val_losses, train_accs = [], [], []\n",
    "\n",
    "# <<< --- Save model in the new directory with appropriate name --- >>>\n",
    "MODEL_SAVE_PATH = os.path.join(PREPROCESSED_DSB_PATH, \"simp3dnet_model_50_each_64cube_best.pth\")\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch_patient(patient_model, train_loader, criterion, optimizer, DEVICE, scaler)\n",
    "    val_loss, val_labels_epoch, val_preds_proba_epoch = validate_patient(patient_model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss); train_accs.append(train_acc)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Calculate validation accuracy for reporting\n",
    "    val_acc_epoch = 0.0\n",
    "    if len(val_labels_epoch) > 0:\n",
    "        val_preds_binary_epoch = (val_preds_proba_epoch > 0.5).astype(int)\n",
    "        val_acc_epoch = accuracy_score(val_labels_epoch, val_preds_binary_epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: Duration: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc_epoch:.4f}\")\n",
    "\n",
    "    # Save best model based on validation loss\n",
    "    if val_loss < best_val_loss and len(val_labels_epoch) > 0: # Ensure validation wasn't empty\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            torch.save(patient_model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"  Best model saved to {MODEL_SAVE_PATH} (Val Loss: {best_val_loss:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache() # Clear cache per epoch\n",
    "\n",
    "print(\"\\nSimp3DNet Model Training Finished.\")\n",
    "\n",
    "\n",
    "# %% --- Plot Training History ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Simp3DNet Loss Curve (64cube)'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, EPOCHS + 1), train_accs, label='Train Acc')\n",
    "# You might want to plot validation accuracy too if calculated per epoch\n",
    "# plt.plot(range(1, EPOCHS + 1), val_accs_epoch, label='Val Acc') # Need to store val_acc_epoch\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Simp3DNet Train Accuracy Curve (64cube)'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the plot\n",
    "plot_save_path = os.path.join(PREPROCESSED_DSB_PATH, \"simp3dnet_training_curves_64cube.png\")\n",
    "plt.savefig(plot_save_path)\n",
    "print(f\"Training curves saved to {plot_save_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% --- Model Evaluation ---\n",
    "print(\"\\nEvaluating Simp3DNet Model on Validation Set...\")\n",
    "# Load the best model for evaluation\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    try:\n",
    "        # Re-initialize model structure before loading state_dict\n",
    "        patient_model_eval = Simp3DNet(input_channels=1, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "        patient_model_eval.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "        print(f\"Loaded best model from {MODEL_SAVE_PATH} for evaluation.\")\n",
    "        patient_model_eval.eval() # Set to evaluation mode\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load best model from {MODEL_SAVE_PATH}: {e}. Evaluating last epoch model instead.\")\n",
    "        # Ensure the current model is in eval mode\n",
    "        patient_model.eval()\n",
    "        patient_model_eval = patient_model # Use the model from the end of training\n",
    "else:\n",
    "    print(f\"Best model file not found at {MODEL_SAVE_PATH}. Evaluating model from the end of training.\")\n",
    "    patient_model.eval() # Set to evaluation mode\n",
    "    patient_model_eval = patient_model\n",
    "\n",
    "# Perform validation using the loaded/final model\n",
    "val_loss_final, final_val_labels, final_val_preds_proba = validate_patient(patient_model_eval, val_loader, criterion, DEVICE)\n",
    "\n",
    "if len(final_val_labels) == 0:\n",
    "    print(\"No valid validation predictions were generated. Cannot evaluate metrics.\")\n",
    "else:\n",
    "    print(f\"\\nFinal Validation Loss: {val_loss_final:.4f}\")\n",
    "\n",
    "    # Calculate metrics using a 0.5 threshold\n",
    "    final_val_preds_binary = (final_val_preds_proba > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(final_val_labels, final_val_preds_binary)\n",
    "    # Use zero_division=0 to avoid warnings if a class has no predictions/labels\n",
    "    precision = precision_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    recall = recall_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    f1 = f1_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "\n",
    "    # Calculate AUC-ROC, handle cases with only one class present\n",
    "    auc_roc = float('nan') # Default to NaN\n",
    "    if len(np.unique(final_val_labels)) > 1: # Check if both classes are in the true labels\n",
    "         try:\n",
    "             auc_roc = roc_auc_score(final_val_labels, final_val_preds_proba)\n",
    "         except ValueError as e:\n",
    "              print(f\"AUC-ROC Calculation Error: {e}\") # Should not happen if len(unique)>1, but good practice\n",
    "    else:\n",
    "         print(\"AUC-ROC cannot be calculated: Only one class present in validation labels.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Validation Metrics (Simp3DNet, 64cube) ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:   {auc_roc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report (Simp3DNet, 64cube):\")\n",
    "    target_names = ['Non-Cancer (0)', 'Cancer (1)']\n",
    "    # Ensure labels are integers for classification_report\n",
    "    print(classification_report(final_val_labels.astype(int), final_val_preds_binary, target_names=target_names, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (Simp3DNet, 64cube):\")\n",
    "    try:\n",
    "        cm = confusion_matrix(final_val_labels.astype(int), final_val_preds_binary, labels=[0, 1]) # Ensure labels are 0 and 1\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        # Save the plot\n",
    "        cm_save_path = os.path.join(PREPROCESSED_DSB_PATH, \"simp3dnet_confusion_matrix_64cube.png\")\n",
    "        plt.savefig(cm_save_path)\n",
    "        print(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying confusion matrix: {e}\")\n",
    "        print(\"Raw CM data:\", cm) # Print raw data if plot fails\n",
    "\n",
    "    # Plot ROC Curve if AUC is valid\n",
    "    if not np.isnan(auc_roc):\n",
    "        fpr, tpr, _ = roc_curve(final_val_labels, final_val_preds_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_roc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve (Simp3DNet, 64cube)')\n",
    "        plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "         # Save the plot\n",
    "        roc_save_path = os.path.join(PREPROCESSED_DSB_PATH, \"simp3dnet_roc_curve_64cube.png\")\n",
    "        plt.savefig(roc_save_path)\n",
    "        print(f\"ROC curve saved to {roc_save_path}\")\n",
    "        plt.show()\n",
    "# %% --- Model Definition (UNetPlusPlus_SE_Transformer) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Building Blocks ---\n",
    "\n",
    "class SEBlock3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Squeeze-and-Excitation Block.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock3D, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool3d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv3d(channels, channels // reduction, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(channels // reduction, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.squeeze(x)\n",
    "        y = self.excitation(y)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ConvBlock3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard 3D Convolutional Block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU (+ optional SE).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, use_se=True, se_reduction=16):\n",
    "        super(ConvBlock3D, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        if self.use_se:\n",
    "            self.se = SEBlock3D(out_channels, reduction=se_reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        if self.use_se:\n",
    "            x = self.se(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of a 3D Transformer Encoder.\n",
    "    Operates on sequences of patches (B, N_patches, E_dim).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim_factor=4, dropout=0.1):\n",
    "        super(TransformerEncoderLayer3D, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * ff_dim_factor),\n",
    "            nn.GELU(), # GELU is common in Transformers\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim * ff_dim_factor, embed_dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Multi-head Self-attention\n",
    "        attn_output, _ = self.attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(attn_output) # Add & Norm\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Feed-forward Network\n",
    "        ffn_output = self.ffn(src)\n",
    "        src = src + self.dropout2(ffn_output) # Add & Norm\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Bottleneck for 3D U-Net.\n",
    "    Takes feature map (B, C_in, D, H, W), projects to (B, N_patches, E_dim),\n",
    "    applies Transformer layers, and reshapes back to (B, E_dim, D, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, num_layers, num_heads, bottleneck_spatial_dims, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bottleneck_spatial_dims = bottleneck_spatial_dims # (D_b, H_b, W_b) at bottleneck\n",
    "        \n",
    "        num_patches = bottleneck_spatial_dims[0] * bottleneck_spatial_dims[1] * bottleneck_spatial_dims[2]\n",
    "\n",
    "        # Project input channels to embedding dimension if they differ\n",
    "        if in_channels != embed_dim:\n",
    "            self.patch_projection = nn.Conv3d(in_channels, embed_dim, kernel_size=1)\n",
    "        else:\n",
    "            self.patch_projection = nn.Identity() # No projection needed if channels match\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim)) # Learnable positional embedding\n",
    "        self.dropout_pos = nn.Dropout(dropout)\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer3D(embed_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm_out = nn.LayerNorm(embed_dim) # Final normalization on patch embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C_in, D_b, H_b, W_b)\n",
    "        x = self.patch_projection(x) # (B, E_dim, D_b, H_b, W_b)\n",
    "        \n",
    "        b, e, d, h, w = x.shape\n",
    "        if (d,h,w) != self.bottleneck_spatial_dims:\n",
    "            raise ValueError(f\"Spatial dimensions mismatch in TransformerBottleneck. Expected {self.bottleneck_spatial_dims}, got {(d,h,w)}\")\n",
    "        if e != self.embed_dim:\n",
    "             raise ValueError(f\"Embedding dimension mismatch after projection. Expected {self.embed_dim}, got {e}\")\n",
    "\n",
    "        # Flatten spatial dimensions to create sequence of patches\n",
    "        x = x.flatten(2) # (B, E_dim, N_patches) where N_patches = D_b*H_b*W_b\n",
    "        x = x.transpose(1, 2) # (B, N_patches, E_dim) - batch_first for MHA\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout_pos(x)\n",
    "\n",
    "        # Pass through Transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.norm_out(x)\n",
    "\n",
    "        # Reshape back to (B, E_dim, D_b, H_b, W_b)\n",
    "        x = x.transpose(1, 2) # (B, E_dim, N_patches)\n",
    "        x = x.view(b, e, d, h, w)\n",
    "        return x\n",
    "\n",
    "# --- Main Model: UNetPlusPlus_SE_Transformer ---\n",
    "class UNetPlusPlus_SE_Transformer(nn.Module):\n",
    "    def __init__(self, input_scan_size, in_channels=1, num_classes=1, initial_filters=16, depth=4,\n",
    "                 use_se=True, transformer_embed_dim=256, transformer_layers=2,\n",
    "                 transformer_heads=8, transformer_dropout=0.1, final_fc_units=128):\n",
    "        super(UNetPlusPlus_SE_Transformer, self).__init__()\n",
    "        \n",
    "        if not (len(input_scan_size) == 3 and all(isinstance(s, int) for s in input_scan_size)):\n",
    "            raise ValueError(\"input_scan_size must be a tuple of 3 integers (D, H, W)\")\n",
    "\n",
    "        self.depth = depth # Number of pooling operations. Levels = depth + 1.\n",
    "        self.use_se = use_se\n",
    "        nf = initial_filters\n",
    "\n",
    "        # Encoder path (X_i,0)\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        encoder_output_channels = [] \n",
    "        \n",
    "        current_channels_enc = in_channels\n",
    "        for i in range(depth + 1): # Iterates 0 to depth (e.g., 0, 1, 2, 3, 4 if depth=4)\n",
    "            out_ch_enc = nf * (2**i)\n",
    "            conv = ConvBlock3D(current_channels_enc, out_ch_enc, use_se=use_se)\n",
    "            self.encoder_blocks.append(conv)\n",
    "            encoder_output_channels.append(out_ch_enc)\n",
    "            if i < depth: \n",
    "                self.pools.append(nn.MaxPool3d(2, 2))\n",
    "            current_channels_enc = out_ch_enc\n",
    "        \n",
    "        # Transformer Bottleneck\n",
    "        bottleneck_in_channels = encoder_output_channels[-1] # Channels of X_depth,0\n",
    "        s_d, s_h, s_w = (input_scan_size[0]//(2**depth), \n",
    "                         input_scan_size[1]//(2**depth), \n",
    "                         input_scan_size[2]//(2**depth))\n",
    "        \n",
    "        if not (s_d > 0 and s_h > 0 and s_w > 0):\n",
    "             raise ValueError(f\"Input scan size {input_scan_size} with depth {depth} results in non-positive bottleneck dims: {(s_d,s_h,s_w)}\")\n",
    "\n",
    "        self.transformer_bottleneck = TransformerBottleneck(\n",
    "            in_channels=bottleneck_in_channels,\n",
    "            embed_dim=transformer_embed_dim,\n",
    "            num_layers=transformer_layers,\n",
    "            num_heads=transformer_heads,\n",
    "            bottleneck_spatial_dims=(s_d, s_h, s_w),\n",
    "            dropout=transformer_dropout\n",
    "        )\n",
    "        \n",
    "        # Decoder path\n",
    "        self.decoder_conv_modulelist = nn.ModuleList() # List of ModuleLists for X_i,j blocks\n",
    "        self.upsamplers = nn.ModuleList() # For Up(X_{i+1, j-1})\n",
    "\n",
    "        # Create upsamplers: one for each level transition.\n",
    "        # self.upsamplers[i] upsamples from level i+1 to level i.\n",
    "        for i in range(depth): # i from 0 to depth-1\n",
    "            ch_from_level_below = transformer_embed_dim if (i + 1) == depth else encoder_output_channels[i+1]\n",
    "            ch_to_level_current = encoder_output_channels[i]\n",
    "            self.upsamplers.append(\n",
    "                nn.ConvTranspose3d(ch_from_level_below, ch_to_level_current, kernel_size=2, stride=2)\n",
    "            )\n",
    "\n",
    "        # Create decoder convolutional blocks (X_i,j for j > 0)\n",
    "        for i in range(depth): # Level index for X_i,j (0 to depth-1, e.g. X0,j, X1,j, X2,j, X3,j)\n",
    "            level_i_decoder_blocks = nn.ModuleList()\n",
    "            for j in range(1, depth - i + 1): # Dense block index (j in X_i,j, from 1 up to depth-i)\n",
    "                # Inputs to X_i,j: (X_i,0...X_i,j-1) + Up(X_{i+1,j-1})\n",
    "                # All X_i,k features (including upsampled) should have encoder_output_channels[i]\n",
    "                num_concat_features = j + 1 # j from same level, 1 from upsampled\n",
    "                in_ch_for_Xij = encoder_output_channels[i] * num_concat_features\n",
    "                out_ch_for_Xij = encoder_output_channels[i]\n",
    "                level_i_decoder_blocks.append(ConvBlock3D(in_ch_for_Xij, out_ch_for_Xij, use_se=use_se))\n",
    "            self.decoder_conv_modulelist.append(level_i_decoder_blocks)\n",
    "        \n",
    "        # Classification Head\n",
    "        final_decoder_out_channels = encoder_output_channels[0] # From X_0,depth\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1,1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(final_decoder_out_channels, final_fc_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(final_fc_units, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        X_features = [[None for _ in range(self.depth + 1)] for _ in range(self.depth + 1)]\n",
    "\n",
    "        # Encoder path (computes X_i,0)\n",
    "        current_feature_map = x\n",
    "        for i in range(self.depth + 1): # 0 to depth\n",
    "            current_feature_map = self.encoder_blocks[i](current_feature_map)\n",
    "            X_features[i][0] = current_feature_map\n",
    "            if i < self.depth:\n",
    "                current_feature_map = self.pools[i](current_feature_map)\n",
    "        \n",
    "        # Transformer Bottleneck acts on X_depth,0\n",
    "        X_features[self.depth][0] = self.transformer_bottleneck(X_features[self.depth][0])\n",
    "\n",
    "        # Decoder path (computes X_i,j for j > 0)\n",
    "        # i: level index (from depth-1 down to 0)\n",
    "        # j: dense block index in skip connections (from 1 up to depth-i)\n",
    "        for i in range(self.depth - 1, -1, -1): # Level: depth-1, depth-2, ..., 0\n",
    "            for j in range(1, self.depth - i + 1): # j in X_i,j: 1, 2, ..., (depth-i)\n",
    "                # Inputs from same level (X_i,0 ... X_i,j-1)\n",
    "                inputs_same_level = [X_features[i][k] for k in range(j)]\n",
    "                \n",
    "                # Upsampled input from level below (X_{i+1, j-1})\n",
    "                # self.upsamplers[i] upsamples from level i+1 to i.\n",
    "                feature_from_below = X_features[i+1][j-1]\n",
    "                upsampled_input = self.upsamplers[i](feature_from_below)\n",
    "                \n",
    "                # Ensure spatial dimensions match for concatenation (target is X_i,0's shape)\n",
    "                target_spatial_size = X_features[i][0].shape[2:]\n",
    "                if upsampled_input.shape[2:] != target_spatial_size:\n",
    "                     upsampled_input = F.interpolate(upsampled_input, size=target_spatial_size, mode='trilinear', align_corners=False)\n",
    "                \n",
    "                combined_inputs = torch.cat(inputs_same_level + [upsampled_input], dim=1)\n",
    "                \n",
    "                # self.decoder_conv_modulelist[i] is the ModuleList for level i.\n",
    "                # self.decoder_conv_modulelist[i][j-1] is the ConvBlock for X_i,j.\n",
    "                X_features[i][j] = self.decoder_conv_modulelist[i][j-1](combined_inputs)\n",
    "\n",
    "        # Final output for classification is from X_0,depth\n",
    "        final_decoder_output = X_features[0][self.depth]\n",
    "        \n",
    "        logits = self.classification_head(final_decoder_output)\n",
    "        return logits\n",
    "    \n",
    "# %% Imports\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Use standard tqdm if not in a notebook environment\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "import random\n",
    "import scipy.ndimage\n",
    "from skimage.measure import label as skimage_label, regionprops\n",
    "from skimage.morphology import disk, binary_closing\n",
    "from skimage.segmentation import clear_border\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F # Needed for UNet++ model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, auc, f1_score,\n",
    "                             precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay)\n",
    "\n",
    "# %% Configuration\n",
    "# --- MODIFY THESE PATHS ---\n",
    "DSB_PATH = r'F:/DSB3/stage1' # Base directory of DSB 2017 Stage 1 scans\n",
    "DSB_LABELS_CSV = r'F:\\DSB3\\stage1_labels.csv' # Path to DSB patient cancer labels CSV\n",
    "# --- Path for preprocessed data (MUST match the previous run) ---\n",
    "PREPROCESSED_DSB_PATH = './preprocessed_dsb_50_each_64cube/'\n",
    "MODEL_OUTPUT_DIR = './model_unetpp_se_transformer_64cube/' # New directory for this specific model's outputs\n",
    "\n",
    "\n",
    "# Preprocessing & Model Params\n",
    "TARGET_SPACING = [1.5, 1.5, 1.5]\n",
    "FINAL_SCAN_SIZE = (64, 64, 64) # MUST match the preprocessed data\n",
    "CLIP_BOUND_HU = [-1000.0, 400.0]\n",
    "PIXEL_MEAN = 0.25\n",
    "\n",
    "# Training Params\n",
    "NUM_CLASSES = 1 # For BCEWithLogitsLoss\n",
    "# !!! VERY IMPORTANT: Reduce BATCH_SIZE for the complex model !!!\n",
    "BATCH_SIZE = 2  # Start low (e.g., 1 or 2) and increase if memory allows\n",
    "LEARNING_RATE = 1e-5 # Start with a potentially smaller LR for complex models\n",
    "EPOCHS = 50 # Reduce epochs initially to test, increase later if needed (was 150)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Scan limit per class (used during initial data selection phase)\n",
    "SCAN_LIMIT_PER_CLASS = 50\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(PREPROCESSED_DSB_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True) # Create specific dir for this model\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# %% --- Data Loading and Selection (Verification Stage) ---\n",
    "# This section assumes preprocessing was already done and verifies the files exist.\n",
    "# It re-reads the labels and identifies the successfully preprocessed files.\n",
    "\n",
    "print(f\"--- Verifying Preprocessed Data in: {PREPROCESSED_DSB_PATH} ---\")\n",
    "\n",
    "# --- Load Labels ---\n",
    "try:\n",
    "    dsb_labels_df = pd.read_csv(DSB_LABELS_CSV)\n",
    "    dsb_labels_df = dsb_labels_df.rename(columns={'id': 'patient_id'})\n",
    "    patient_labels_all = dsb_labels_df.set_index('patient_id')['cancer'].to_dict()\n",
    "    print(f\"Loaded {len(patient_labels_all)} total DSB patient labels.\")\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"ERROR: Failed to load labels CSV: {e}\")\n",
    "\n",
    "# --- Find existing preprocessed files ---\n",
    "existing_files = glob.glob(os.path.join(PREPROCESSED_DSB_PATH, \"*.npz\"))\n",
    "if not existing_files:\n",
    "    raise SystemExit(f\"ERROR: No preprocessed .npz files found in {PREPROCESSED_DSB_PATH}. Run the preprocessing step first.\")\n",
    "\n",
    "# Extract patient IDs from filenames and filter based on available labels\n",
    "available_processed_ids = []\n",
    "for f_path in existing_files:\n",
    "    p_id = os.path.basename(f_path).replace('.npz', '')\n",
    "    if p_id in patient_labels_all:\n",
    "        available_processed_ids.append(p_id)\n",
    "    else:\n",
    "        print(f\"Warning: Preprocessed file found for patient {p_id}, but no label exists. Skipping.\")\n",
    "\n",
    "print(f\"Found {len(available_processed_ids)} preprocessed scans with labels.\")\n",
    "\n",
    "# --- Use only the available processed IDs ---\n",
    "final_patient_list = available_processed_ids\n",
    "if not final_patient_list:\n",
    "    raise SystemExit(\"No usable preprocessed scans found. Cannot continue.\")\n",
    "\n",
    "# Filter labels dictionary\n",
    "patient_labels = {pid: patient_labels_all[pid] for pid in final_patient_list}\n",
    "print(f\"Final patient count for training/validation: {len(final_patient_list)}\")\n",
    "final_cancer_count = sum(1 for pid in final_patient_list if patient_labels[pid] == 1)\n",
    "final_non_cancer_count = len(final_patient_list) - final_cancer_count\n",
    "print(f\"  Cancerous: {final_cancer_count}, Non-Cancerous: {final_non_cancer_count}\")\n",
    "\n",
    "\n",
    "# %% --- Preprocessing Functions (Required for Dataset Class checks, but not executed again) ---\n",
    "# Keep these definitions available in case the Dataset class needs them implicitly,\n",
    "# but the main preprocessing execution loop is skipped as we assume data exists.\n",
    "def load_scan_series(dicom_folder_path): # Dummy definition if not needed elsewhere\n",
    "    pass\n",
    "def resample(image, original_spacing, new_spacing=TARGET_SPACING): pass\n",
    "def get_segmented_lungs(im_slice, hu_threshold=-320): pass\n",
    "def normalize_hu(image, clip_bounds=CLIP_BOUND_HU): pass\n",
    "def zero_center(image, pixel_mean=PIXEL_MEAN): pass\n",
    "def resize_scan_to_target(image, target_shape=FINAL_SCAN_SIZE): pass\n",
    "def preprocess_scan_dsb(patient_id, input_base_path, output_base_path, force_preprocess=False): pass\n",
    "print(\"Preprocessing functions defined (but execution skipped as data should exist).\")\n",
    "\n",
    "\n",
    "# %% --- Dataset and DataLoader ---\n",
    "\n",
    "class PatientLevelDataset(Dataset):\n",
    "    def __init__(self, patient_ids, labels_dict, preprocessed_path):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.labels_dict = labels_dict\n",
    "        self.preprocessed_path = preprocessed_path\n",
    "        self.target_size = FINAL_SCAN_SIZE # Store target size for checks\n",
    "\n",
    "    def __len__(self): return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        label = self.labels_dict[patient_id]\n",
    "        scan_path = os.path.join(self.preprocessed_path, f\"{patient_id}.npz\")\n",
    "        try:\n",
    "            with np.load(scan_path) as npz_data:\n",
    "                image = npz_data['image']\n",
    "            # Verify shape\n",
    "            if image.shape != self.target_size:\n",
    "                print(f\"ERROR: Shape mismatch for {patient_id}. Expected {self.target_size}, got {image.shape}. Returning error data.\")\n",
    "                dummy = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "                return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "\n",
    "            image_tensor = torch.from_numpy(image).float().unsqueeze(0) # Add channel dim\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            return image_tensor, label_tensor\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERROR: File not found {scan_path}. Returning error data.\")\n",
    "             dummy = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "             return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading {patient_id}: {e}. Returning error data.\")\n",
    "            dummy = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "            return dummy, torch.tensor(-1, dtype=torch.float32) # Error label\n",
    "\n",
    "# --- Split Data (Train/Validation) ---\n",
    "train_ids, val_ids = train_test_split(\n",
    "    final_patient_list,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=[patient_labels[pid] for pid in final_patient_list] # Stratify\n",
    ")\n",
    "print(f\"\\nTraining patients: {len(train_ids)}\")\n",
    "print(f\"Validation patients: {len(val_ids)}\")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = PatientLevelDataset(train_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "val_dataset = PatientLevelDataset(val_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "\n",
    "NUM_WORKERS = 0 # Set to 0 for easier debugging\n",
    "print(f\"Using {NUM_WORKERS} workers for DataLoader.\")\n",
    "# Adjust batch size here if needed\n",
    "print(f\"Using Batch Size: {BATCH_SIZE}\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# --- Check DataLoader Output ---\n",
    "try:\n",
    "    print(\"\\nChecking DataLoader output...\")\n",
    "    if len(train_loader) > 0:\n",
    "        sample_batch, sample_labels = next(iter(train_loader))\n",
    "        print(f\"Sample batch shape: {sample_batch.shape}\") # Should be [B, 1, 64, 64, 64]\n",
    "        print(f\"Sample labels shape: {sample_labels.shape}\")\n",
    "        print(f\"Sample labels: {sample_labels}\")\n",
    "        if torch.any(sample_labels == -1): print(\"WARNING: Error labels (-1) detected in first batch.\")\n",
    "    else: print(\"Train loader empty.\")\n",
    "except Exception as e: print(f\"Error checking DataLoader: {e}\")\n",
    "\n",
    "\n",
    "# %% --- Model Definition (UNetPlusPlus_SE_Transformer) ---\n",
    "# --- Building Blocks ---\n",
    "\n",
    "class SEBlock3D(nn.Module):\n",
    "    \"\"\"3D Squeeze-and-Excitation Block.\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock3D, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool3d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv3d(channels, channels // reduction, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(channels // reduction, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.squeeze(x)\n",
    "        y = self.excitation(y)\n",
    "        return x * y\n",
    "\n",
    "class ConvBlock3D(nn.Module):\n",
    "    \"\"\"Conv -> BN -> ReLU -> Conv -> BN -> ReLU (+ optional SE).\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, use_se=True, se_reduction=16):\n",
    "        super(ConvBlock3D, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        if self.use_se:\n",
    "            self.se = SEBlock3D(out_channels, reduction=se_reduction)\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        if self.use_se:\n",
    "            x = self.se(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer3D(nn.Module):\n",
    "    \"\"\"Single layer of a 3D Transformer Encoder (operates on sequence).\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim_factor=4, dropout=0.1):\n",
    "        super(TransformerEncoderLayer3D, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * ff_dim_factor), nn.GELU(),\n",
    "            nn.Dropout(dropout), nn.Linear(embed_dim * ff_dim_factor, embed_dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        attn_output, _ = self.attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        ffn_output = self.ffn(src)\n",
    "        src = src + self.dropout2(ffn_output)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    \"\"\"Transformer Bottleneck: projects feature map to sequence, applies transformer, reshapes back.\"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, num_layers, num_heads, bottleneck_spatial_dims, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bottleneck_spatial_dims = bottleneck_spatial_dims\n",
    "        num_patches = bottleneck_spatial_dims[0] * bottleneck_spatial_dims[1] * bottleneck_spatial_dims[2]\n",
    "        self.patch_projection = nn.Conv3d(in_channels, embed_dim, kernel_size=1) if in_channels != embed_dim else nn.Identity()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.dropout_pos = nn.Dropout(dropout)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer3D(embed_dim, num_heads, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm_out = nn.LayerNorm(embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.patch_projection(x)\n",
    "        b, e, d, h, w = x.shape\n",
    "        if (d,h,w) != self.bottleneck_spatial_dims: raise ValueError(f\"Spatial dim mismatch. Expected {self.bottleneck_spatial_dims}, got {(d,h,w)}\")\n",
    "        if e != self.embed_dim: raise ValueError(f\"Embed dim mismatch. Expected {self.embed_dim}, got {e}\")\n",
    "        x = x.flatten(2).transpose(1, 2) # (B, N_patches, E_dim)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout_pos(x)\n",
    "        for layer in self.transformer_layers: x = layer(x)\n",
    "        x = self.norm_out(x)\n",
    "        x = x.transpose(1, 2).view(b, e, d, h, w) # Reshape back\n",
    "        return x\n",
    "\n",
    "# --- Main Model: UNetPlusPlus_SE_Transformer ---\n",
    "class UNetPlusPlus_SE_Transformer(nn.Module):\n",
    "    def __init__(self, input_scan_size, in_channels=1, num_classes=1, initial_filters=16, depth=4,\n",
    "                 use_se=True, transformer_embed_dim=256, transformer_layers=2,\n",
    "                 transformer_heads=8, transformer_dropout=0.1, final_fc_units=128):\n",
    "        super(UNetPlusPlus_SE_Transformer, self).__init__()\n",
    "        if not (len(input_scan_size) == 3 and all(isinstance(s, int) for s in input_scan_size)):\n",
    "            raise ValueError(\"input_scan_size must be tuple (D, H, W)\")\n",
    "        self.depth = depth\n",
    "        self.use_se = use_se\n",
    "        nf = initial_filters\n",
    "\n",
    "        # Encoder (X_i,0)\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        encoder_output_channels = []\n",
    "        current_channels_enc = in_channels\n",
    "        for i in range(depth + 1):\n",
    "            out_ch_enc = nf * (2**i)\n",
    "            self.encoder_blocks.append(ConvBlock3D(current_channels_enc, out_ch_enc, use_se=use_se))\n",
    "            encoder_output_channels.append(out_ch_enc)\n",
    "            if i < depth: self.pools.append(nn.MaxPool3d(2, 2))\n",
    "            current_channels_enc = out_ch_enc\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        bottleneck_in_channels = encoder_output_channels[-1]\n",
    "        s_d, s_h, s_w = (input_scan_size[i] // (2**depth) for i in range(3))\n",
    "        if not all(s > 0 for s in (s_d, s_h, s_w)): raise ValueError(f\"Non-positive bottleneck dims: {(s_d,s_h,s_w)}\")\n",
    "        self.transformer_bottleneck = TransformerBottleneck(\n",
    "            in_channels=bottleneck_in_channels, embed_dim=transformer_embed_dim, num_layers=transformer_layers,\n",
    "            num_heads=transformer_heads, bottleneck_spatial_dims=(s_d, s_h, s_w), dropout=transformer_dropout)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_conv_modulelist = nn.ModuleList()\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        for i in range(depth): # Upsamplers from level i+1 to i\n",
    "            ch_from_below = transformer_embed_dim if (i + 1) == depth else encoder_output_channels[i+1]\n",
    "            ch_to_current = encoder_output_channels[i]\n",
    "            self.upsamplers.append(nn.ConvTranspose3d(ch_from_below, ch_to_current, kernel_size=2, stride=2))\n",
    "\n",
    "        for i in range(depth): # Decoder blocks X_i,j (j>0)\n",
    "            level_i_decoder_blocks = nn.ModuleList()\n",
    "            for j in range(1, depth - i + 1):\n",
    "                num_concat = j + 1\n",
    "                in_ch_Xij = encoder_output_channels[i] * num_concat\n",
    "                out_ch_Xij = encoder_output_channels[i]\n",
    "                level_i_decoder_blocks.append(ConvBlock3D(in_ch_Xij, out_ch_Xij, use_se=use_se))\n",
    "            self.decoder_conv_modulelist.append(level_i_decoder_blocks)\n",
    "\n",
    "        # Classification Head\n",
    "        final_decoder_out_channels = encoder_output_channels[0] # From X_0,depth\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1,1,1)), nn.Flatten(),\n",
    "            nn.Linear(final_decoder_out_channels, final_fc_units), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5), nn.Linear(final_fc_units, num_classes) )\n",
    "\n",
    "    def forward(self, x):\n",
    "        X_features = [[None] * (self.depth + 1) for _ in range(self.depth + 1)]\n",
    "        # Encoder\n",
    "        current = x\n",
    "        for i in range(self.depth + 1):\n",
    "            X_features[i][0] = self.encoder_blocks[i](current)\n",
    "            if i < self.depth: current = self.pools[i](X_features[i][0])\n",
    "            else: current = X_features[i][0] # Last encoder output\n",
    "        # Bottleneck\n",
    "        X_features[self.depth][0] = self.transformer_bottleneck(X_features[self.depth][0])\n",
    "        # Decoder\n",
    "        for i in range(self.depth - 1, -1, -1): # Level i\n",
    "            for j in range(1, self.depth - i + 1): # Dense block j at level i (X_i,j)\n",
    "                inputs_same_level = [X_features[i][k] for k in range(j)]\n",
    "                upsampled_input = self.upsamplers[i](X_features[i+1][j-1])\n",
    "                target_spatial = X_features[i][0].shape[2:] # Match X_i,0 shape\n",
    "                if upsampled_input.shape[2:] != target_spatial:\n",
    "                    upsampled_input = F.interpolate(upsampled_input, size=target_spatial, mode='trilinear', align_corners=False)\n",
    "                combined = torch.cat(inputs_same_level + [upsampled_input], dim=1)\n",
    "                X_features[i][j] = self.decoder_conv_modulelist[i][j-1](combined)\n",
    "        # Final output from X_0,depth\n",
    "        logits = self.classification_head(X_features[0][self.depth])\n",
    "        return logits\n",
    "\n",
    "# %% --- Instantiate the UNetPlusPlus_SE_Transformer Model ---\n",
    "\n",
    "print(\"\\n--- Instantiating UNetPlusPlus_SE_Transformer Model ---\")\n",
    "\n",
    "# Define hyperparameters for the UNet++ Transformer model\n",
    "# Adjust these based on your GPU memory and dataset size. These are starting points.\n",
    "unetpp_initial_filters = 16     # Filters in the first layer (e.g., 16)\n",
    "unetpp_depth = 3                # Number of pooling layers (e.g., 3 for 64->32->16->8)\n",
    "                                # Bottleneck spatial size: 64 / (2^3) = 8x8x8\n",
    "unetpp_transformer_embed_dim = unetpp_initial_filters * (2**unetpp_depth) # Match bottleneck channels (16 * 8 = 128)\n",
    "unetpp_transformer_layers = 1   # Number of transformer layers (e.g., 1 or 2)\n",
    "unetpp_transformer_heads = 4    # Number of attention heads (must divide embed_dim, e.g., 4)\n",
    "unetpp_final_fc_units = 64      # Size of the final FC layer before output (e.g., 64)\n",
    "\n",
    "# Clear any previous model from memory (important if running cells multiple times)\n",
    "if 'patient_model' in locals():\n",
    "    del patient_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Cleared previous model instance.\")\n",
    "\n",
    "try:\n",
    "    patient_model = UNetPlusPlus_SE_Transformer(\n",
    "        input_scan_size=FINAL_SCAN_SIZE,       # (64, 64, 64)\n",
    "        in_channels=1,\n",
    "        num_classes=NUM_CLASSES,               # 1 for BCEWithLogitsLoss\n",
    "        initial_filters=unetpp_initial_filters,\n",
    "        depth=unetpp_depth,\n",
    "        use_se=True,                           # Use SE blocks\n",
    "        transformer_embed_dim=unetpp_transformer_embed_dim,\n",
    "        transformer_layers=unetpp_transformer_layers,\n",
    "        transformer_heads=unetpp_transformer_heads,\n",
    "        transformer_dropout=0.1,\n",
    "        final_fc_units=unetpp_final_fc_units\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    print(f\"Instantiated UNetPlusPlus_SE_Transformer with:\")\n",
    "    print(f\"  Initial Filters: {unetpp_initial_filters}, Depth: {unetpp_depth}\")\n",
    "    print(f\"  Transformer Embed Dim: {unetpp_transformer_embed_dim}, Layers: {unetpp_transformer_layers}, Heads: {unetpp_transformer_heads}\")\n",
    "    print(f\"  Final FC Units: {unetpp_final_fc_units}\")\n",
    "    # print(patient_model) # Optional: Print model structure (can be very long)\n",
    "\n",
    "    # Optional: Count parameters to gauge model size\n",
    "    total_params = sum(p.numel() for p in patient_model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "    # Optional: Dry run with a dummy batch (on the specified device)\n",
    "    # This helps catch CUDA errors or shape mismatches before training.\n",
    "    print(\"Performing a quick model check with a dummy batch...\")\n",
    "    _dummy_check_batch_size = 1 # Use 1 to minimize memory check impact\n",
    "    dummy_input = torch.randn(_dummy_check_batch_size, 1, *FINAL_SCAN_SIZE).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = patient_model(dummy_input)\n",
    "    print(f\"Model check successful. Output shape: {output.shape}\") # Should be [1, 1]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during model instantiation or check: {e}\")\n",
    "    print(\"Check hyperparameters, input_scan_size, and available memory.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    # Stop execution if model instantiation fails\n",
    "    raise SystemExit(\"Model instantiation failed.\")\n",
    "\n",
    "\n",
    "# %% --- Loss and Optimizer ---\n",
    "\n",
    "# Calculate positive weight based on the *training* set composition\n",
    "train_labels_list = [patient_labels[pid] for pid in train_ids]\n",
    "count_0 = train_labels_list.count(0)\n",
    "count_1 = train_labels_list.count(1)\n",
    "pos_weight_tensor = torch.tensor([1.0], device=DEVICE) # Default\n",
    "if count_1 > 0 and count_0 > 0:\n",
    "    pos_weight_val = count_0 / count_1\n",
    "    print(f\"Calculated positive weight for training subset ({count_1} pos / {count_0} neg): {pos_weight_val:.4f}\")\n",
    "    pos_weight_tensor = torch.tensor([pos_weight_val], device=DEVICE)\n",
    "elif count_1 == 0 and count_0 > 0: print(\"Warning: Training set only contains negative samples.\")\n",
    "elif count_0 == 0 and count_1 > 0: print(\"Warning: Training set only contains positive samples.\")\n",
    "else: print(\"Warning: Training set appears empty or invalid. Using default pos_weight=1.\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "# Use Adam optimizer, potentially with adjusted LR\n",
    "print(f\"Using Learning Rate: {LEARNING_RATE}\")\n",
    "optimizer = optim.Adam(patient_model.parameters(), lr=LEARNING_RATE) # Use the defined LEARNING_RATE\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# %% --- Training and Validation Functions (Unchanged Logic) ---\n",
    "def train_one_epoch_patient(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train(); running_loss = 0.0; total_samples = 0; correct_predictions = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for i, (inputs, labels) in enumerate(progress_bar):\n",
    "        valid_indices = labels != -1\n",
    "        if not torch.any(valid_indices): continue\n",
    "        inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=scaler.is_enabled()):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss encountered during training batch {i}! Skipping batch.\")\n",
    "            optimizer.zero_grad() # Zero grad again before skipping\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # Optional: Gradient clipping can help stabilize training for complex models\n",
    "        # scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        total_samples += inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        correct_predictions += (preds == labels.bool()).sum().item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    if total_samples == 0: return 0.0, 0.0\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_patient(model, dataloader, criterion, device):\n",
    "    model.eval(); running_loss = 0.0; total_samples = 0; all_preds_proba = []; all_labels = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "        for inputs, labels in progress_bar:\n",
    "            valid_indices = labels != -1\n",
    "            if not torch.any(valid_indices): continue\n",
    "            inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "                 outputs = model(inputs)\n",
    "                 # Ensure labels have channel dim for loss calculation consistency\n",
    "                 loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                 print(\"NaN loss encountered during validation! Skipping batch contribution.\")\n",
    "                 continue\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "            all_preds_proba.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    if total_samples == 0: return 0.0, np.array([]), np.array([])\n",
    "    val_loss = running_loss / total_samples\n",
    "    return val_loss, np.array(all_labels), np.array(all_preds_proba)\n",
    "\n",
    "\n",
    "# %% --- Training Loop ---\n",
    "print(f\"\\n--- Starting Training: UNetPlusPlus_SE_Transformer Model ---\")\n",
    "print(f\"Dataset size: {len(final_patient_list)} scans ({len(train_ids)} train, {len(val_ids)} val)\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}, Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Saving outputs to: {MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], [] # Add val_accs\n",
    "\n",
    "# <<< --- Save model in the NEW model-specific directory --- >>>\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_OUTPUT_DIR, \"unetpp_se_transformer_64cube_best.pth\")\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch_patient(patient_model, train_loader, criterion, optimizer, DEVICE, scaler)\n",
    "    val_loss, val_labels_epoch, val_preds_proba_epoch = validate_patient(patient_model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "    # Calculate validation accuracy for reporting\n",
    "    val_acc_epoch = 0.0\n",
    "    if len(val_labels_epoch) > 0:\n",
    "        val_preds_binary_epoch = (val_preds_proba_epoch > 0.5).astype(int)\n",
    "        val_acc_epoch = accuracy_score(val_labels_epoch, val_preds_binary_epoch)\n",
    "        val_accs.append(val_acc_epoch) # Store validation accuracy\n",
    "    else:\n",
    "        val_accs.append(0.0) # Append 0 if validation was empty\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: Duration: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc_epoch:.4f}\")\n",
    "\n",
    "    # Save best model based on validation loss\n",
    "    # Ensure validation produced results before saving\n",
    "    if val_loss < best_val_loss and len(val_labels_epoch) > 0:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            torch.save(patient_model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"  Best model saved to {MODEL_SAVE_PATH} (Val Loss: {best_val_loss:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    elif len(val_labels_epoch) == 0:\n",
    "        print(\"  Skipping save, validation set was empty this epoch.\")\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache() # Clear cache per epoch\n",
    "\n",
    "print(\"\\n--- UNetPlusPlus_SE_Transformer Model Training Finished ---\")\n",
    "\n",
    "\n",
    "# %% --- Plot Training History ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, EPOCHS + 1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('U-Net++ SE Transformer Loss'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, EPOCHS + 1), train_accs, label='Train Acc')\n",
    "plt.plot(range(1, EPOCHS + 1), val_accs, label='Val Acc') # Plot validation accuracy\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('U-Net++ SE Transformer Accuracy'); plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the plot to the model-specific directory\n",
    "plot_save_path = os.path.join(MODEL_OUTPUT_DIR, \"unetpp_se_transformer_training_curves.png\")\n",
    "plt.savefig(plot_save_path)\n",
    "print(f\"Training curves saved to {plot_save_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% --- Model Evaluation ---\n",
    "print(\"\\n--- Evaluating UNetPlusPlus_SE_Transformer Model on Validation Set ---\")\n",
    "\n",
    "# Load the best model for evaluation\n",
    "best_model_loaded = False\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    try:\n",
    "        # Re-initialize model structure before loading state_dict\n",
    "        # Ensure hyperparameters match the saved model\n",
    "        patient_model_eval = UNetPlusPlus_SE_Transformer(\n",
    "            input_scan_size=FINAL_SCAN_SIZE, in_channels=1, num_classes=NUM_CLASSES,\n",
    "            initial_filters=unetpp_initial_filters, depth=unetpp_depth, use_se=True,\n",
    "            transformer_embed_dim=unetpp_transformer_embed_dim, transformer_layers=unetpp_transformer_layers,\n",
    "            transformer_heads=unetpp_transformer_heads, transformer_dropout=0.1,\n",
    "            final_fc_units=unetpp_final_fc_units\n",
    "        ).to(DEVICE)\n",
    "        patient_model_eval.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "        print(f\"Loaded best model from {MODEL_SAVE_PATH} for evaluation.\")\n",
    "        patient_model_eval.eval() # Set to evaluation mode\n",
    "        best_model_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load best model from {MODEL_SAVE_PATH}: {e}. Evaluating last epoch model instead.\")\n",
    "        # Ensure the current model is in eval mode\n",
    "        patient_model.eval()\n",
    "        patient_model_eval = patient_model # Use the model from the end of training\n",
    "else:\n",
    "    print(f\"Best model file not found at {MODEL_SAVE_PATH}. Evaluating model from the end of training.\")\n",
    "    patient_model.eval() # Set to evaluation mode\n",
    "    patient_model_eval = patient_model\n",
    "\n",
    "# Perform validation using the loaded/final model\n",
    "val_loss_final, final_val_labels, final_val_preds_proba = validate_patient(patient_model_eval, val_loader, criterion, DEVICE)\n",
    "\n",
    "if len(final_val_labels) == 0:\n",
    "    print(\"No valid validation predictions were generated. Cannot evaluate metrics.\")\n",
    "else:\n",
    "    print(f\"\\nFinal Validation Loss (Best Model={best_model_loaded}): {val_loss_final:.4f}\")\n",
    "\n",
    "    # Calculate metrics using a 0.5 threshold\n",
    "    final_val_preds_binary = (final_val_preds_proba > 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(final_val_labels, final_val_preds_binary)\n",
    "    precision = precision_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    recall = recall_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    f1 = f1_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "\n",
    "    auc_roc = float('nan')\n",
    "    if len(np.unique(final_val_labels)) > 1:\n",
    "         try: auc_roc = roc_auc_score(final_val_labels, final_val_preds_proba)\n",
    "         except ValueError as e: print(f\"AUC-ROC Calculation Error: {e}\")\n",
    "    else: print(\"AUC-ROC cannot be calculated: Only one class present in validation labels.\")\n",
    "\n",
    "    print(\"\\n--- Final Validation Metrics (U-Net++ SE Transformer) ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"AUC-ROC:   {auc_roc:.4f if not np.isnan(auc_roc) else 'N/A'}\") # Handle NaN display\n",
    "\n",
    "    print(\"\\nClassification Report (U-Net++ SE Transformer):\")\n",
    "    target_names = ['Non-Cancer (0)', 'Cancer (1)']\n",
    "    print(classification_report(final_val_labels.astype(int), final_val_preds_binary, target_names=target_names, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix (U-Net++ SE Transformer):\")\n",
    "    try:\n",
    "        cm = confusion_matrix(final_val_labels.astype(int), final_val_preds_binary, labels=[0, 1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        cm_save_path = os.path.join(MODEL_OUTPUT_DIR, \"unetpp_se_transformer_confusion_matrix.png\")\n",
    "        plt.savefig(cm_save_path)\n",
    "        print(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "        plt.show()\n",
    "    except Exception as e: print(f\"Error displaying/saving confusion matrix: {e}\")\n",
    "\n",
    "    if not np.isnan(auc_roc):\n",
    "        fpr, tpr, _ = roc_curve(final_val_labels, final_val_preds_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_roc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve (U-Net++ SE Transformer)')\n",
    "        plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "        roc_save_path = os.path.join(MODEL_OUTPUT_DIR, \"unetpp_se_transformer_roc_curve.png\")\n",
    "        plt.savefig(roc_save_path)\n",
    "        print(f\"ROC curve saved to {roc_save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
