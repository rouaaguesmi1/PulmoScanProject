{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf7bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type function is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 181\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# Save config\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;66;03m# Path objects are not JSON serializable directly\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# ## 3. Data Loading and Preprocessing\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Placeholder for LIDC-IDRI (or similar) data parsing utilities\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Assume annotations give nodule centroids, bounding boxes, and possibly malignancy labels.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type function is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # End-to-End Lung Nodule Detection and Classification Pipeline\n",
    "#\n",
    "# This notebook outlines a cascade approach for lung nodule analysis:\n",
    "# 1. **Segmentation:** A 3D U-Net (nnU-Net inspired) segments potential nodule regions.\n",
    "# 2. **Detection & Proposal:** Sliding window on segmentation output to propose nodule candidates.\n",
    "# 3. **Classification:** A 3D DenseNet121 (with SE blocks, pre-trained on MedicalNet) classifies candidate cubes.\n",
    "#\n",
    "# **Features:**\n",
    "# - Hard Negative Mining for classifier training.\n",
    "# - Focal Loss for imbalanced classification.\n",
    "# - Mixed-precision training.\n",
    "# - Cosine Learning Rate schedule.\n",
    "# - Early Stopping based on validation AUC.\n",
    "# - Evaluation: Detection Dice, Classification AUC, Sensitivity @ 95% Specificity.\n",
    "# - Visualization: 3D nodule overlays, GradCAM.\n",
    "# - Comparison: This cascade vs. a simpler direct 3D CNN classifier.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import json # For saving configs or reports\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Medical Imaging\n",
    "import SimpleITK as sitk\n",
    "# from radiomics import featureextractor # If you were to use radiomics for comparison\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn for metrics and utilities\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, auc, f1_score,\n",
    "    precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay,\n",
    "    # For Dice\n",
    "    jaccard_score # IoU, Dice = 2*TP / (2*TP + FP + FN) or 2*Intersection / (Union + Intersection)\n",
    ")\n",
    "from skimage.measure import label as skimage_label, regionprops\n",
    "from skimage.morphology import disk, binary_closing, ball # For 3D morphology\n",
    "from skimage.segmentation import clear_border\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "# Visualization\n",
    "# import itkwidgets # For interactive 3D plotting\n",
    "# from IPython.display import display\n",
    "\n",
    "# For nnU-Net like preprocessing/data handling (if not using full framework)\n",
    "# from batchgenerators.utilities.file_and_folder_operations import *\n",
    "# from batchgenerators.transforms.spatial_transforms import SpatialTransform, MirrorTransform\n",
    "# from batchgenerators.transforms.color_transforms import BrightnessMultiplicativeTransform, GammaTransform\n",
    "# from batchgenerators.transforms.noise_transforms import GaussianNoiseTransform, GaussianBlurTransform\n",
    "# from batchgenerators.transforms.utility_transforms import RemoveLabelTransform, RenameTransform, NumpyToTensor\n",
    "\n",
    "# For Logging (optional, but recommended)\n",
    "# import wandb\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom modules (you'll create these)\n",
    "# import utils\n",
    "# import models_segmentation\n",
    "# import models_classification\n",
    "# import data_handling\n",
    "# import training_loops\n",
    "# import evaluation_metrics\n",
    "# import visualization_tools\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False # Can be True for speed if input sizes don't change\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configuration\n",
    "\n",
    "# %%\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    \"data_dir\": Path(\"/path/to/your/LIDC-IDRI-like/dataset\"), # Contains CTs, masks, annotations\n",
    "    \"output_dir\": Path(\"./output_nodule_pipeline\"),\n",
    "    \"medicalnet_weights_path\": Path(\"/path/to/your/MedicalNet_DenseNet121_weights.pth\"), # Or ResNet50\n",
    "\n",
    "    # --- General Preprocessing ---\n",
    "    \"target_spacing_seg\": [1.5, 1.0, 1.0], # For segmentation model input\n",
    "    \"target_spacing_clf\": [1.0, 1.0, 1.0], # For classifier input cubes\n",
    "    \"hu_clip_bounds\": [-1000, 400],\n",
    "    \"norm_mean_std\": {\"mean\": 0.25, \"std\": 0.25}, # Example, calculate from your data\n",
    "\n",
    "    # --- Segmentation Model (nnU-Net inspired 3D U-Net) ---\n",
    "    \"seg_model_name\": \"Custom3DUNet\",\n",
    "    \"seg_in_channels\": 1, # CT\n",
    "    \"seg_out_channels\": 1, # Nodule mask (binary)\n",
    "    \"seg_patch_size\": [96, 128, 128], # D, H, W\n",
    "    \"seg_batch_size\": 2,\n",
    "    \"seg_lr\": 1e-4,\n",
    "    \"seg_epochs\": 100, # Example\n",
    "    \"seg_loss_weights\": {\"dice\": 0.6, \"bce\": 0.4}, # For combined loss\n",
    "\n",
    "    # --- Nodule Detection/Proposal ---\n",
    "    \"seg_prob_threshold\": 0.5, # Threshold for segmentation map\n",
    "    \"min_nodule_size_voxels\": 20, # Minimum size for a connected component to be a nodule\n",
    "    \"sliding_window_stride\": [32, 32, 32], # For dense proposal if needed, or use CCs directly\n",
    "\n",
    "    # --- Candidate Cube Generation ---\n",
    "    \"clf_cube_size_raw\": [48, 64, 64], # Before resizing to final classifier input\n",
    "    \"clf_cube_size_final\": [32, 48, 48], # Final input size for classifier\n",
    "\n",
    "    # --- Classification Model (3D DenseNet121 + SE) ---\n",
    "    \"clf_model_name\": \"DenseNet121_3D_SE\",\n",
    "    \"clf_in_channels\": 1,\n",
    "    \"clf_num_classes\": 1, # Malignancy (binary or score) -> for sigmoid output\n",
    "    \"clf_use_medicalnet_pretrained\": True,\n",
    "    \"clf_batch_size\": 16, # Can be larger for smaller cubes\n",
    "    \"clf_lr\": 1e-4, # Initial LR for classifier\n",
    "    \"clf_epochs\": 75,\n",
    "    \"clf_focal_loss_alpha\": 0.25, # Focal loss parameters\n",
    "    \"clf_focal_loss_gamma\": 2.0,\n",
    "    \"clf_early_stopping_patience\": 10, # For validation AUC\n",
    "    \"clf_cosine_lr_t_max\": 75, # Total epochs for cosine schedule\n",
    "\n",
    "    # --- Hard Negative Mining ---\n",
    "    \"hnm_start_epoch\": 5, # Epoch to start HNM\n",
    "    \"hnm_ratio_neg_to_pos\": 3, # e.g., 3 hard negatives for every 1 positive\n",
    "    \"hnm_num_hard_negatives_per_batch\": lambda bs, ratio: int(bs * ratio / (1 + ratio)), # Dynamically calculate\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    \"eval_sensitivity_at_specificity\": 0.95,\n",
    "\n",
    "    # --- Logging & Reporting ---\n",
    "    \"use_wandb\": False, # Set to True to use wandb\n",
    "    \"wandb_project_name\": \"lung_nodule_pipeline\",\n",
    "\n",
    "    # --- Comparison Model ---\n",
    "    \"comp_model_name\": \"Simple3DCNN_Classifier\", # For comparison\n",
    "    \"comp_lr\": 1e-4,\n",
    "    \"comp_epochs\": 75,\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"segmentation_models\").mkdir(exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"classification_models\").mkdir(exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"comparison_models\").mkdir(exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"visualizations\").mkdir(exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"reports\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "with open(CONFIG[\"output_dir\"] / \"config.json\", 'w') as f:\n",
    "    # Path objects are not JSON serializable directly\n",
    "    json.dump({k: str(v) if isinstance(v, Path) else v for k,v in CONFIG.items()}, f, indent=4)\n",
    "\n",
    "print(\"Configuration saved.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Loading and Preprocessing\n",
    "#\n",
    "# This section involves:\n",
    "# - Identifying patient scan folders.\n",
    "# - Loading CT images, lung masks (if available, or segment them first), and nodule annotations.\n",
    "# - Preprocessing:\n",
    "#     - Resampling to target spacing.\n",
    "#     - HU intensity windowing/clipping.\n",
    "#     - Normalization.\n",
    "#     - Creating 3D binary masks for nodules (from annotations) for segmentation training.\n",
    "# - Splitting data into train, validation, (and optionally test) sets.\n",
    "\n",
    "# %%\n",
    "# Placeholder for LIDC-IDRI (or similar) data parsing utilities\n",
    "# Assume annotations give nodule centroids, bounding boxes, and possibly malignancy labels.\n",
    "\n",
    "def load_patient_data(patient_id, data_root_dir):\n",
    "    \"\"\"\n",
    "    Loads CT, lung mask (optional), and nodule annotations for a patient.\n",
    "    Returns:\n",
    "        sitk_ct_image, sitk_lung_mask (or None), nodule_annotations_list\n",
    "    \"\"\"\n",
    "    # ct_path = data_root_dir / patient_id / \"ct_scan.mha\" # Example path\n",
    "    # lung_mask_path = data_root_dir / patient_id / \"lung_mask.mha\" # Example path\n",
    "    # annotations_path = data_root_dir / patient_id / \"annotations.xml\" # Example for LIDC\n",
    "\n",
    "    # sitk_ct_image = sitk.ReadImage(str(ct_path))\n",
    "    # sitk_lung_mask = sitk.ReadImage(str(lung_mask_path)) if lung_mask_path.exists() else None\n",
    "    # nodule_annotations_list = parse_lidc_xml(annotations_path) # You'd need this parser\n",
    "\n",
    "    # Dummy example:\n",
    "    print(f\"Loading data for patient {patient_id} (dummy implementation)\")\n",
    "    # Create a dummy CT image\n",
    "    dummy_ct_array = np.random.rand(128, 256, 256).astype(np.float32) * 1000 - 500 # Dummy HU values\n",
    "    sitk_ct_image = sitk.GetImageFromArray(dummy_ct_array)\n",
    "    sitk_ct_image.SetSpacing([1.0, 0.7, 0.7]) # Dummy spacing\n",
    "\n",
    "    # Create a dummy lung mask\n",
    "    dummy_lung_mask_array = np.zeros_like(dummy_ct_array, dtype=np.uint8)\n",
    "    dummy_lung_mask_array[30:100, 50:200, 50:200] = 1 # Dummy lung region\n",
    "    sitk_lung_mask = sitk.GetImageFromArray(dummy_lung_mask_array)\n",
    "    sitk_lung_mask.SetSpacing(sitk_ct_image.GetSpacing())\n",
    "\n",
    "\n",
    "    # Dummy nodule annotations: list of dicts\n",
    "    # Each dict: {'centroid_world': [x,y,z], 'diameter_mm': d, 'malignancy': m (0-5 or binary)}\n",
    "    # For segmentation, we'd convert these to voxel coordinates and create a mask\n",
    "    nodule_annotations_list = [\n",
    "        {'centroid_world': np.array(sitk_ct_image.TransformContinuousIndexToPhysicalPoint([60,100,100])),\n",
    "         'diameter_mm': 10, 'malignancy': 4, 'id': 'nod1'},\n",
    "        {'centroid_world': np.array(sitk_ct_image.TransformContinuousIndexToPhysicalPoint([70,150,150])),\n",
    "         'diameter_mm': 5, 'malignancy': 1, 'id': 'nod2'}\n",
    "    ]\n",
    "    return sitk_ct_image, sitk_lung_mask, nodule_annotations_list\n",
    "\n",
    "def preprocess_image(sitk_img, target_spacing, clip_bounds, norm_stats, is_mask=False):\n",
    "    \"\"\" Resample, clip, normalize. \"\"\"\n",
    "    # Resample\n",
    "    original_spacing = sitk_img.GetSpacing()\n",
    "    original_size = sitk_img.GetSize()\n",
    "    new_size = [\n",
    "        int(round(osz * ospc / tspc))\n",
    "        for osz, ospc, tspc in zip(original_size, original_spacing, target_spacing)\n",
    "    ]\n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetOutputSpacing(target_spacing)\n",
    "    resampler.SetSize(new_size)\n",
    "    resampler.SetOutputDirection(sitk_img.GetDirection())\n",
    "    resampler.SetOutputOrigin(sitk_img.GetOrigin())\n",
    "    resampler.SetTransform(sitk.Transform())\n",
    "    resampler.SetInterpolator(sitk.sitkLinear if not is_mask else sitk.sitkNearestNeighbor)\n",
    "    resampled_img = resampler.Execute(sitk_img)\n",
    "\n",
    "    img_array = sitk.GetArrayFromImage(resampled_img).astype(np.float32)\n",
    "\n",
    "    if not is_mask:\n",
    "        # Clip HU\n",
    "        img_array = np.clip(img_array, clip_bounds[0], clip_bounds[1])\n",
    "        # Normalize (example: (x - min) / (max - min) then (x - mean) / std)\n",
    "        img_array = (img_array - clip_bounds[0]) / (clip_bounds[1] - clip_bounds[0])\n",
    "        img_array = (img_array - norm_stats[\"mean\"]) / norm_stats[\"std\"]\n",
    "    else:\n",
    "        img_array = img_array.astype(np.uint8) # Ensure mask is int\n",
    "\n",
    "    return img_array # Returns numpy array\n",
    "\n",
    "def create_nodule_segmentation_mask(ct_image_sitk, nodule_annotations, target_shape_voxels):\n",
    "    \"\"\"\n",
    "    Creates a 3D binary mask for nodules based on annotations for a specific resampled CT.\n",
    "    ct_image_sitk: The resampled SimpleITK image to get physical to voxel coordinate transforms.\n",
    "    nodule_annotations: List of nodule dicts with 'centroid_world' and 'diameter_mm'.\n",
    "    target_shape_voxels: The D,H,W shape of the numpy array for the mask.\n",
    "    \"\"\"\n",
    "    nodule_mask_np = np.zeros(target_shape_voxels, dtype=np.uint8)\n",
    "    target_spacing = ct_image_sitk.GetSpacing() # Spacing of the resampled CT\n",
    "\n",
    "    for nod in nodule_annotations:\n",
    "        centroid_world = nod['centroid_world']\n",
    "        diameter_mm = nod['diameter_mm']\n",
    "        radius_mm = diameter_mm / 2.0\n",
    "\n",
    "        # Convert world centroid to voxel centroid in the resampled image\n",
    "        centroid_voxel_continuous = ct_image_sitk.TransformPhysicalPointToContinuousIndex(centroid_world)\n",
    "        centroid_voxel = [int(round(c)) for c in centroid_voxel_continuous]\n",
    "\n",
    "        # Define bounding box in voxel coordinates\n",
    "        # Convert radius in mm to radius in voxels for each dimension\n",
    "        radius_voxels = [radius_mm / spc for spc in target_spacing]\n",
    "\n",
    "        z_min = max(0, int(round(centroid_voxel[2] - radius_voxels[2]))) # SimpleITK index order is x,y,z\n",
    "        z_max = min(target_shape_voxels[0] -1, int(round(centroid_voxel[2] + radius_voxels[2]))) # Numpy array is D,H,W so z is index 0\n",
    "        y_min = max(0, int(round(centroid_voxel[1] - radius_voxels[1])))\n",
    "        y_max = min(target_shape_voxels[1] -1, int(round(centroid_voxel[1] + radius_voxels[1])))\n",
    "        x_min = max(0, int(round(centroid_voxel[0] - radius_voxels[0])))\n",
    "        x_max = min(target_shape_voxels[2] -1, int(round(centroid_voxel[0] + radius_voxels[0])))\n",
    "\n",
    "        # Create a spherical mask (approximate by drawing in a 3D grid)\n",
    "        # This is a simplification; more accurate rasterization might be needed\n",
    "        for z_idx in range(z_min, z_max + 1):\n",
    "            for y_idx in range(y_min, y_max + 1):\n",
    "                for x_idx in range(x_min, x_max + 1):\n",
    "                    # Check if point (z_idx, y_idx, x_idx) is within the sphere\n",
    "                    # Convert voxel indices back to continuous for distance check\n",
    "                    dist_sq = ( ((z_idx - centroid_voxel_continuous[2]) * target_spacing[2])**2 +\n",
    "                                ((y_idx - centroid_voxel_continuous[1]) * target_spacing[1])**2 +\n",
    "                                ((x_idx - centroid_voxel_continuous[0]) * target_spacing[0])**2 )\n",
    "                    if dist_sq <= radius_mm**2:\n",
    "                        # Check bounds before assignment\n",
    "                        if 0 <= z_idx < target_shape_voxels[0] and \\\n",
    "                           0 <= y_idx < target_shape_voxels[1] and \\\n",
    "                           0 <= x_idx < target_shape_voxels[2]:\n",
    "                            nodule_mask_np[z_idx, y_idx, x_idx] = 1 # Numpy order D, H, W\n",
    "    return nodule_mask_np\n",
    "\n",
    "\n",
    "# --- Prepare lists of patient IDs ---\n",
    "# all_patient_ids = [f.name for f in CONFIG[\"data_dir\"].iterdir() if f.is_dir()]\n",
    "all_patient_ids = [f\"patient_{i:03d}\" for i in range(20)] # Dummy patient IDs\n",
    "train_ids, val_test_ids = train_test_split(all_patient_ids, test_size=0.3, random_state=SEED)\n",
    "val_ids, test_ids = train_test_split(val_test_ids, test_size=0.5, random_state=SEED)\n",
    "\n",
    "print(f\"Train IDs: {len(train_ids)}, Val IDs: {len(val_ids)}, Test IDs: {len(test_ids)}\")\n",
    "\n",
    "# --- Example of processing one patient (for segmentation data) ---\n",
    "# This would typically be done inside a Dataset class's __getitem__ or a preprocessing script\n",
    "\n",
    "# For a patient_id in train_ids:\n",
    "#   sitk_ct, sitk_lm, annotations = load_patient_data(patient_id, CONFIG[\"data_dir\"])\n",
    "#   # Preprocess CT for segmentation model\n",
    "#   processed_ct_np = preprocess_image(sitk_ct, CONFIG[\"target_spacing_seg\"],\n",
    "#                                       CONFIG[\"hu_clip_bounds\"], CONFIG[\"norm_mean_std\"])\n",
    "#\n",
    "#   # Create a resampled sitk_ct to pass to nodule mask creation for coordinate transformation\n",
    "#   # This is a bit redundant here, ideally preprocess_image returns sitk object or transform info\n",
    "#   _ , resampled_size_seg = sitk.ResampleImageFilter().Compute μετα# ... (previous code)\n",
    "\n",
    "#   # Calculate the size of the resampled CT image for segmentation\n",
    "#   original_spacing_ct = sitk_ct.GetSpacing()\n",
    "#   original_size_ct = sitk_ct.GetSize()\n",
    "#   target_spacing_seg = CONFIG[\"target_spacing_seg\"]\n",
    "#   resampled_size_for_seg_mask_sitk = [\n",
    "#       int(round(osz * ospc / tspc))\n",
    "#       for osz, ospc, tspc in zip(original_size_ct, original_spacing_ct, target_spacing_seg)\n",
    "#   ]\n",
    "#   # Create a dummy resampled sitk_ct for coordinate transformation purposes\n",
    "#   # (In a real pipeline, preprocess_image would ideally handle this or return necessary info)\n",
    "#   resampler_ref = sitk.ResampleImageFilter()\n",
    "#   resampler_ref.SetOutputSpacing(target_spacing_seg)\n",
    "#   resampler_ref.SetSize(resampled_size_for_seg_mask_sitk)\n",
    "#   resampler_ref.SetOutputDirection(sitk_ct.GetDirection())\n",
    "#   resampler_ref.SetOutputOrigin(sitk_ct.GetOrigin())\n",
    "#   resampled_ct_for_coords_sitk = resampler_ref.Execute(sitk_ct) # just need its geometry\n",
    "#\n",
    "#   # Create nodule segmentation mask based on the geometry of the resampled CT\n",
    "#   # The shape of the numpy array from preprocess_image must match here\n",
    "#   nodule_seg_mask_np = create_nodule_segmentation_mask(\n",
    "#       resampled_ct_for_coords_sitk,\n",
    "#       annotations,\n",
    "#       target_shape_voxels=processed_ct_np.shape # (D, H, W)\n",
    "#   )\n",
    "#   # Save processed_ct_np and nodule_seg_mask_np for training/validation\n",
    "#   # e.g., to CONFIG[\"output_dir\"] / \"preprocessed_seg\" / f\"{patient_id}_ct.npy\"\n",
    "#   #      CONFIG[\"output_dir\"] / \"preprocessed_seg\" / f\"{patient_id}_seg_mask.npy\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Segmentation Model (3D nnU-Net Inspired U-Net)\n",
    "#\n",
    "# - Define a 3D U-Net architecture.\n",
    "# - Define Dataset and DataLoader for segmentation.\n",
    "# - Define loss function (e.g., Dice + BCE).\n",
    "# - Training loop for segmentation.\n",
    "\n",
    "# %%\n",
    "# Placeholder for 3D U-Net definition (models_segmentation.py)\n",
    "# e.g., using blocks like: Conv3D -> InstanceNorm3D -> LeakyReLU\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
    "            nn.InstanceNorm3d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False),\n",
    "            nn.InstanceNorm3d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class DownSample3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "    def forward(self, x):\n",
    "        skip_connection = self.conv(x)\n",
    "        pooled = self.pool(skip_connection)\n",
    "        return pooled, skip_connection\n",
    "\n",
    "class UpSample3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Use ConvTranspose3d for upsampling\n",
    "        self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels) # in_channels because of skip connection concat\n",
    "    def forward(self, x1, x2): # x1 from previous layer, x2 is skip connection\n",
    "        x1 = self.up(x1)\n",
    "        # Pad if dimensions mismatch (common in U-Nets)\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class Custom3DUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[32, 64, 128, 256]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv3D(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose3d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConv3D(feature * 2, feature)) # After concat with skip\n",
    "\n",
    "        self.bottleneck = DoubleConv3D(features[-1], features[-1] * 2)\n",
    "        self.final_conv = nn.Conv3d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for i in range(len(self.downs)):\n",
    "            x = self.downs[i](x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1] # Reverse for up-sampling\n",
    "\n",
    "        for i in range(0, len(self.ups), 2): # Step by 2 (ConvTranspose, DoubleConv)\n",
    "            x = self.ups[i](x) # Upsample\n",
    "            skip_connection = skip_connections[i//2]\n",
    "            # Pad if necessary\n",
    "            if x.shape != skip_connection.shape:\n",
    "                # print(f\"Padding needed. x: {x.shape}, skip: {skip_connection.shape}\")\n",
    "                diffZ = skip_connection.size()[2] - x.size()[2]\n",
    "                diffY = skip_connection.size()[3] - x.size()[3]\n",
    "                diffX = skip_connection.size()[4] - x.size()[4]\n",
    "                x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
    "                                diffY // 2, diffY - diffY // 2,\n",
    "                                diffZ // 2, diffZ - diffZ // 2])\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[i+1](concat_skip) # DoubleConv\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, patient_ids, config, preprocessed_data_dir, transform=None):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.config = config\n",
    "        self.preprocessed_data_dir = Path(preprocessed_data_dir)\n",
    "        self.transform = transform # For patch extraction and augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids) # Or num_patches if patch-based\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        # ct_path = self.preprocessed_data_dir / f\"{patient_id}_ct.npy\"\n",
    "        # mask_path = self.preprocessed_data_dir / f\"{patient_id}_seg_mask.npy\"\n",
    "        # ct_array = np.load(ct_path)\n",
    "        # mask_array = np.load(mask_path)\n",
    "\n",
    "        # Dummy data for dataset\n",
    "        ct_array = np.random.rand(*self.config[\"seg_patch_size\"]).astype(np.float32)\n",
    "        mask_array = (np.random.rand(*self.config[\"seg_patch_size\"]) > 0.8).astype(np.uint8) # Sparse mask\n",
    "\n",
    "        # Apply transforms (e.g., patching, nnU-Net style augmentations)\n",
    "        # if self.transform:\n",
    "        #     data_dict = self.transform(data={'data': ct_array[None], 'seg': mask_array[None]}) # Add channel dim\n",
    "        #     ct_array, mask_array = data_dict['data'][0], data_dict['seg'][0]\n",
    "        # else: # Simple patch extraction or use full image if small enough\n",
    "        # This needs to be adapted for patch-based training if images are large\n",
    "        # For now, assume ct_array and mask_array are already patch-sized or full images ready for training\n",
    "        pass\n",
    "\n",
    "        ct_tensor = torch.from_numpy(ct_array).float().unsqueeze(0) # Add channel dim (B, C, D, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask_array).float().unsqueeze(0)\n",
    "\n",
    "        return ct_tensor, mask_tensor\n",
    "\n",
    "# --- Loss Function ---\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, pred_probs, target):\n",
    "        pred_probs = torch.sigmoid(pred_probs) # If model outputs logits\n",
    "        intersection = (pred_probs * target).sum(dim=(2,3,4)) # Sum over D, H, W\n",
    "        union = pred_probs.sum(dim=(2,3,4)) + target.sum(dim=(2,3,4))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1. - dice.mean() # Average over batch\n",
    "\n",
    "class CombinedSegLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5, bce_weight=0.5, smooth_dice=1e-6):\n",
    "        super().__init__()\n",
    "        self.dice_loss = DiceLoss(smooth=smooth_dice)\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss() # Takes logits directly\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_weight = bce_weight\n",
    "    def forward(self, pred_logits, target_mask):\n",
    "        loss_dice = self.dice_loss(pred_logits, target_mask)\n",
    "        loss_bce = self.bce_loss(pred_logits, target_mask)\n",
    "        return self.dice_weight * loss_dice + self.bce_weight * loss_bce\n",
    "\n",
    "# --- Training Loop for Segmentation ---\n",
    "def train_segmentation_epoch(model, dataloader, optimizer, loss_fn, scaler, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader, desc=\"Seg Training Epoch\")):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=torch.cuda.is_available()): # Mixed precision\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, target)\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN/Inf loss in seg train: {loss.item()}. Skipping batch.\")\n",
    "            continue\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate_segmentation_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc=\"Seg Validation Epoch\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                predictions = model(data)\n",
    "                loss = loss_fn(predictions, target)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"NaN/Inf loss in seg val: {loss.item()}. Skipping batch.\")\n",
    "                continue\n",
    "            epoch_loss += loss.item()\n",
    "            # Store predictions and targets for Dice calculation\n",
    "            # For simplicity, assuming binary segmentation for Dice here\n",
    "            all_preds.append(torch.sigmoid(predictions).cpu().numpy() > 0.5)\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "\n",
    "    if not all_preds: return 0.0, 0.0 # Handle empty validation\n",
    "\n",
    "    all_preds_np = np.concatenate(all_preds, axis=0).flatten()\n",
    "    all_targets_np = np.concatenate(all_targets, axis=0).flatten()\n",
    "\n",
    "    # Calculate Dice score for the entire validation set\n",
    "    # Note: This is a flattened Dice. Voxel-wise. For object-level Dice, need connected components.\n",
    "    intersection = np.sum(all_preds_np * all_targets_np)\n",
    "    val_dice = (2. * intersection) / (np.sum(all_preds_np) + np.sum(all_targets_np) + 1e-6)\n",
    "\n",
    "    return epoch_loss / len(dataloader), val_dice\n",
    "\n",
    "# Initialize segmentation model, optimizer, loss\n",
    "seg_model = Custom3DUNet(\n",
    "    in_channels=CONFIG[\"seg_in_channels\"],\n",
    "    out_channels=CONFIG[\"seg_out_channels\"]\n",
    ").to(DEVICE)\n",
    "seg_optimizer = optim.Adam(seg_model.parameters(), lr=CONFIG[\"seg_lr\"])\n",
    "seg_loss_fn = CombinedSegLoss(\n",
    "    dice_weight=CONFIG[\"seg_loss_weights\"][\"dice\"],\n",
    "    bce_weight=CONFIG[\"seg_loss_weights\"][\"bce\"]\n",
    ")\n",
    "seg_scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "seg_lr_scheduler = CosineAnnealingLR(seg_optimizer, T_max=CONFIG[\"seg_epochs\"])\n",
    "\n",
    "\n",
    "# Dummy preprocessed data path for seg dataset\n",
    "# In reality, you'd run the preprocessing step from Section 3 first\n",
    "dummy_preprocessed_seg_dir = CONFIG[\"output_dir\"] / \"preprocessed_seg_dummy\"\n",
    "dummy_preprocessed_seg_dir.mkdir(exist_ok=True)\n",
    "\n",
    "seg_train_dataset = SegmentationDataset(train_ids, CONFIG, dummy_preprocessed_seg_dir) # Pass actual path\n",
    "seg_val_dataset = SegmentationDataset(val_ids, CONFIG, dummy_preprocessed_seg_dir)     # Pass actual path\n",
    "seg_train_loader = DataLoader(seg_train_dataset, batch_size=CONFIG[\"seg_batch_size\"], shuffle=True, num_workers=0)\n",
    "seg_val_loader = DataLoader(seg_val_dataset, batch_size=CONFIG[\"seg_batch_size\"], shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\n--- Training Segmentation Model ({CONFIG['seg_model_name']}) ---\")\n",
    "best_val_dice_seg = -1.0\n",
    "for epoch in range(CONFIG[\"seg_epochs\"]):\n",
    "    train_loss_seg = train_segmentation_epoch(seg_model, seg_train_loader, seg_optimizer, seg_loss_fn, seg_scaler, DEVICE)\n",
    "    val_loss_seg, val_dice_seg = validate_segmentation_epoch(seg_model, seg_val_loader, seg_loss_fn, DEVICE)\n",
    "    seg_lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['seg_epochs']}: Seg Train Loss: {train_loss_seg:.4f}, Seg Val Loss: {val_loss_seg:.4f}, Seg Val Dice: {val_dice_seg:.4f}\")\n",
    "\n",
    "    if val_dice_seg > best_val_dice_seg:\n",
    "        best_val_dice_seg = val_dice_seg\n",
    "        torch.save(seg_model.state_dict(), CONFIG[\"output_dir\"] / \"segmentation_models\" / f\"{CONFIG['seg_model_name']}_best.pth\")\n",
    "        print(f\"  Saved best segmentation model with Val Dice: {best_val_dice_seg:.4f}\")\n",
    "\n",
    "print(\"Segmentation training finished.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Sliding-Window Detection & Candidate Proposal\n",
    "#\n",
    "# - Load trained segmentation model.\n",
    "# - Iterate through (test/validation) scans.\n",
    "# - Apply segmentation model to get probability maps.\n",
    "# - Threshold probability maps.\n",
    "# - Use connected components (`skimage.measure.label`) to identify discrete nodule candidates.\n",
    "# - Filter candidates by size or other criteria.\n",
    "# - Output bounding boxes or centroids of these candidates.\n",
    "\n",
    "# %%\n",
    "def get_nodule_candidates_from_segmentation(\n",
    "    ct_sitk_original, # Original SITK CT for coordinate mapping\n",
    "    seg_model,\n",
    "    config,\n",
    "    device):\n",
    "    \"\"\"\n",
    "    Takes a CT scan, applies segmentation model, and returns candidate nodule info.\n",
    "    \"\"\"\n",
    "    seg_model.eval()\n",
    "\n",
    "    # 1. Preprocess CT for segmentation model input\n",
    "    processed_ct_np = preprocess_image(\n",
    "        ct_sitk_original,\n",
    "        config[\"target_spacing_seg\"],\n",
    "        config[\"hu_clip_bounds\"],\n",
    "        config[\"norm_mean_std\"]\n",
    "    )\n",
    "    # This processed_ct_np might need to be broken into patches if model expects patches\n",
    "    # For simplicity, assume model can take the whole resampled volume (or it's handled internally)\n",
    "    # If patch-based, you'd need a sliding window inference here.\n",
    "    ct_tensor = torch.from_numpy(processed_ct_np).float().unsqueeze(0).unsqueeze(0).to(device) # B, C, D, H, W\n",
    "\n",
    "    with torch.no_grad(), autocast(enabled=torch.cuda.is_available()):\n",
    "        seg_logits = seg_model(ct_tensor)\n",
    "        seg_probs_np = torch.sigmoid(seg_logits).squeeze().cpu().numpy() # D, H, W\n",
    "\n",
    "    # 2. Threshold and get connected components\n",
    "    binary_mask = (seg_probs_np > config[\"seg_prob_threshold\"]).astype(np.uint8)\n",
    "    labeled_mask, num_labels = skimage_label(binary_mask, connectivity=3, return_num=True) # 3D connectivity\n",
    "\n",
    "    candidates = []\n",
    "    # Create a resampled sitk_ct for coordinate transformation (geometry of seg_probs_np)\n",
    "    original_spacing = ct_sitk_original.GetSpacing()\n",
    "    original_size = ct_sitk_original.GetSize()\n",
    "    target_spacing_seg = config[\"target_spacing_seg\"]\n",
    "    resampled_size_for_coords = [\n",
    "        int(round(osz * ospc / tspc))\n",
    "        for osz, ospc, tspc in zip(original_size, original_spacing, target_spacing_seg)\n",
    "    ]\n",
    "    resampler_ref = sitk.ResampleImageFilter()\n",
    "    resampler_ref.SetOutputSpacing(target_spacing_seg)\n",
    "    resampler_ref.SetSize(resampled_size_for_coords) # Should match seg_probs_np.shape if D,H,W order is consistent\n",
    "    resampler_ref.SetOutputDirection(ct_sitk_original.GetDirection())\n",
    "    resampler_ref.SetOutputOrigin(ct_sitk_original.GetOrigin())\n",
    "    # Execute with a dummy image just to get the transform object, or use the resampled ct if available\n",
    "    # Ensure the geometry matches seg_probs_np for correct coordinate transforms\n",
    "    # For this dummy execution, assume resampled_ct_for_coords_sitk has this geometry\n",
    "    resampled_ct_for_coords_sitk = resampler_ref.Execute(sitk.Image(processed_ct_np.shape[::-1], sitk.sitkFloat32)) # Use shape of seg_probs_np\n",
    "    resampled_ct_for_coords_sitk.SetSpacing(target_spacing_seg)\n",
    "    resampled_ct_for_coords_sitk.SetOrigin(ct_sitk_original.GetOrigin()) # This might need adjustment based on how resampling affects origin\n",
    "\n",
    "    for i in range(1, num_labels + 1):\n",
    "        props = regionprops(labeled_mask == i, intensity_image=seg_probs_np)\n",
    "        if props: # Should always be one region for label i\n",
    "            prop = props[0]\n",
    "            if prop.area >= config[\"min_nodule_size_voxels\"]:\n",
    "                centroid_voxel_seg = prop.centroid # (z, y, x) for numpy array\n",
    "                # Convert centroid from seg_probs_np voxel coords to world coords\n",
    "                # seg_probs_np is (D,H,W), SimpleITK continuous index is (x,y,z)\n",
    "                centroid_world = resampled_ct_for_coords_sitk.TransformContinuousIndexToPhysicalPoint(\n",
    "                    (centroid_voxel_seg[2], centroid_voxel_seg[1], centroid_voxel_seg[0]) # x,y,z for SITK\n",
    "                )\n",
    "                candidates.append({\n",
    "                    \"centroid_voxel_seg\": centroid_voxel_seg, # (z,y,x) in seg_probs_np space\n",
    "                    \"centroid_world\": np.array(centroid_world),\n",
    "                    \"bbox_voxel_seg\": prop.bbox, # (min_z, min_y, min_x, max_z, max_y, max_x)\n",
    "                    \"mean_intensity_seg_prob\": prop.mean_intensity, # Avg seg prob in candidate\n",
    "                    \"id\": f\"cand_{i}\"\n",
    "                })\n",
    "    return candidates, seg_probs_np # Return seg_probs for Dice eval if needed\n",
    "\n",
    "# Load best segmentation model\n",
    "best_seg_model_path = CONFIG[\"output_dir\"] / \"segmentation_models\" / f\"{CONFIG['seg_model_name']}_best.pth\"\n",
    "if best_seg_model_path.exists():\n",
    "    seg_model.load_state_dict(torch.load(best_seg_model_path, map_location=DEVICE))\n",
    "    print(f\"Loaded best segmentation model from {best_seg_model_path}\")\n",
    "else:\n",
    "    print(\"WARNING: Best segmentation model not found. Using last epoch model for proposals.\")\n",
    "\n",
    "# Example: Get candidates for one validation patient\n",
    "# patient_to_test_seg = val_ids[0]\n",
    "# sitk_ct_test, _, true_annotations_test = load_patient_data(patient_to_test_seg, CONFIG[\"data_dir\"])\n",
    "# proposed_nodules, seg_map_for_dice = get_nodule_candidates_from_segmentation(\n",
    "#     sitk_ct_test, seg_model, CONFIG, DEVICE\n",
    "# )\n",
    "# print(f\"Proposed {len(proposed_nodules)} nodules for patient {patient_to_test_seg}.\")\n",
    "# for nod in proposed_nodules[:2]: print(nod)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Candidate Cube Generation for Classifier\n",
    "#\n",
    "# - For each proposed nodule candidate (and ground truth nodules):\n",
    "#     - Crop a 3D cube around its centroid from the **original CT scan, resampled to classifier's target spacing**.\n",
    "#     - Resize/pad this cube to the classifier's fixed input size (`clf_cube_size_final`).\n",
    "#     - Store these cubes and their labels (malignancy for GT, pseudo-labels for proposals if needed).\n",
    "#     - For Hard Negative Mining, proposals that don't overlap with GT nodules are initially \"negatives\".\n",
    "\n",
    "# %%\n",
    "def extract_cube(\n",
    "    sitk_ct_original, # Original CT to crop from\n",
    "    world_centroid,   # World coordinates of the center of the cube\n",
    "    raw_cube_size_voxels_clf, # Physical size of cube to extract, in voxels at *classifier* spacing\n",
    "    target_spacing_clf, # Classifier's target spacing\n",
    "    final_cube_shape_clf, # Target D,H,W for classifier input\n",
    "    config,\n",
    "    is_mask=False # Not used here, but for consistency\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Extracts a 3D cube, resamples it to classifier spacing, normalizes, and resizes.\n",
    "    raw_cube_size_voxels_clf: (Depth, Height, Width) number of voxels to aim for at target_spacing_clf\n",
    "                              This defines the *physical* size of the cube to extract.\n",
    "    \"\"\"\n",
    "    # 1. Determine physical size of the cube based on raw_cube_size_voxels_clf and target_spacing_clf\n",
    "    physical_size_D = raw_cube_size_voxels_clf[0] * target_spacing_clf[0]\n",
    "    physical_size_H = raw_cube_size_voxels_clf[1] * target_spacing_clf[1]\n",
    "    physical_size_W = raw_cube_size_voxels_clf[2] * target_spacing_clf[2]\n",
    "\n",
    "    # 2. Create a resampling grid centered at world_centroid with target_spacing_clf\n",
    "    #    and size defined by physical_size / target_spacing_clf (which is raw_cube_size_voxels_clf)\n",
    "    resampler = sitk.ResampleImageFilter()\n",
    "    resampler.SetOutputSpacing(target_spacing_clf)\n",
    "    resampler.SetSize(raw_cube_size_voxels_clf[::-1]) # SITK size is x,y,z\n",
    "\n",
    "    # Calculate origin for the resampling grid\n",
    "    # Center of the output cube in physical space is world_centroid\n",
    "    # Origin = center - (size_physical / 2)\n",
    "    # But SimpleITK origin is at the corner of the first voxel\n",
    "    # So, origin = world_centroid - ( (size_voxels - 1) * spacing / 2 )\n",
    "    # Or, more simply: if the output cube has 'N' voxels along an axis with 'spc' spacing,\n",
    "    # its physical extent is N*spc. The origin for SITK resampler should make world_centroid\n",
    "    # the center of this physical extent.\n",
    "    origin_x = world_centroid[0] - (physical_size_W / 2.0) + (target_spacing_clf[2] / 2.0)\n",
    "    origin_y = world_centroid[1] - (physical_size_H / 2.0) + (target_spacing_clf[1] / 2.0)\n",
    "    origin_z = world_centroid[2] - (physical_size_D / 2.0) + (target_spacing_clf[0] / 2.0)\n",
    "    resampler.SetOutputOrigin([origin_x, origin_y, origin_z])\n",
    "\n",
    "    resampler.SetOutputDirection(sitk_ct_original.GetDirection()) # Assume same orientation\n",
    "    resampler.SetInterpolator(sitk.sitkLinear)\n",
    "    resampled_cube_sitk = resampler.Execute(sitk_ct_original)\n",
    "\n",
    "    # 3. Convert to numpy, clip, normalize\n",
    "    cube_np_raw_spacing = sitk.GetArrayFromImage(resampled_cube_sitk) # D, H, W\n",
    "    cube_np_normalized = preprocess_image(\n",
    "        resampled_cube_sitk, # Pass SITK image to reuse existing preprocess\n",
    "        target_spacing=target_spacing_clf, # Already at this spacing, but preprocess_image expects it\n",
    "        clip_bounds=config[\"hu_clip_bounds\"],\n",
    "        norm_stats=config[\"norm_mean_std\"],\n",
    "        is_mask=False\n",
    "    ) # This will re-resample if target_spacing_clf is different, ensure it's the same.\n",
    "      # Or, just do clip & norm on cube_np_raw_spacing directly.\n",
    "      # For simplicity, let's assume preprocess_image called with same target_spacing doesn't resample again.\n",
    "      # A cleaner way:\n",
    "    # cube_np_raw_spacing = np.clip(cube_np_raw_spacing, config[\"hu_clip_bounds\"][0], config[\"hu_clip_bounds\"][1])\n",
    "    # cube_np_raw_spacing = (cube_np_raw_spacing - config[\"hu_clip_bounds\"][0]) / (config[\"hu_clip_bounds\"][1] - config[\"hu_clip_bounds\"][0])\n",
    "    # cube_np_normalized = (cube_np_raw_spacing - config[\"norm_mean_std\"][\"mean\"]) / config[\"norm_mean_std\"][\"std\"]\n",
    "\n",
    "\n",
    "    # 4. Resize/pad to final_cube_shape_clf (e.g., 32, 48, 48)\n",
    "    # Scipy.ndimage.zoom for resizing\n",
    "    current_shape = cube_np_normalized.shape\n",
    "    zoom_factors = [f_dim / c_dim for f_dim, c_dim in zip(final_cube_shape_clf, current_shape)]\n",
    "    final_cube_np = ndi.zoom(cube_np_normalized, zoom_factors, order=1, mode='nearest') # order=1 for linear\n",
    "\n",
    "    # Ensure exact final shape (due to rounding in zoom) with padding/cropping\n",
    "    # This is a simplified crop/pad. More robust padding might be needed.\n",
    "    shape_diff = np.array(final_cube_shape_clf) - np.array(final_cube_np.shape)\n",
    "    pad_dims = []\n",
    "    for i in range(3): # D, H, W\n",
    "        if shape_diff[i] >= 0: # Pad\n",
    "            pad_before = shape_diff[i] // 2\n",
    "            pad_after = shape_diff[i] - pad_before\n",
    "            pad_dims.append((pad_before, pad_after))\n",
    "        else: # Crop (should not happen if zoom is correct, but for robustness)\n",
    "            crop_before = -shape_diff[i] // 2\n",
    "            crop_after = -shape_diff[i] - crop_before\n",
    "            final_cube_np = np.take(final_cube_np, range(crop_before, final_cube_np.shape[i] - crop_after), axis=i)\n",
    "            pad_dims.append((0,0)) # No padding then\n",
    "\n",
    "    if any(s[0]>0 or s[1]>0 for s in pad_dims): # Check if any padding is needed\n",
    "         final_cube_np = np.pad(final_cube_np, pad_dims, mode='constant', constant_values=final_cube_np.min()) # Pad with min value\n",
    "\n",
    "    # Final crop to exact size if over-padded/zoomed\n",
    "    final_cube_np = final_cube_np[\n",
    "        :final_cube_shape_clf[0],\n",
    "        :final_cube_shape_clf[1],\n",
    "        :final_cube_shape_clf[2]\n",
    "    ]\n",
    "    assert final_cube_np.shape == tuple(final_cube_shape_clf), \\\n",
    "        f\"Cube shape mismatch: {final_cube_np.shape} vs {final_cube_shape_clf}\"\n",
    "\n",
    "    return final_cube_np.astype(np.float32)\n",
    "\n",
    "\n",
    "# --- Generate lists of positive and negative cube paths/info for classifier training ---\n",
    "# This is a complex step. You need to:\n",
    "# 1. For GT nodules: Extract cubes, assign malignancy label (e.g., from LIDC 1-5 scale to binary).\n",
    "#    Store cube path and label. These are your \"positives\" (if malignant) or \"easy negatives\" (if benign GT).\n",
    "# 2. For proposed candidates from segmentation:\n",
    "#    - Match them to GT nodules (e.g., by IoU of bounding boxes or distance of centroids).\n",
    "#    - If a proposal matches a GT malignant nodule -> positive sample.\n",
    "#    - If a proposal matches a GT benign nodule -> easy negative sample.\n",
    "#    - If a proposal does NOT match any GT nodule -> potential hard negative sample.\n",
    "#\n",
    "# For now, a simplified placeholder:\n",
    "# all_gt_nodule_cubes_info = [] # list of {'cube_path': path, 'label': 0/1, 'patient_id':pid}\n",
    "# all_proposed_negative_cubes_info = [] # list of {'cube_path': path, 'label': 0, 'patient_id':pid}\n",
    "\n",
    "# Example of processing and saving cubes (run this in a loop for all patients and nodules)\n",
    "# for patient_id in all_patient_ids:\n",
    "#     sitk_ct, _, annotations = load_patient_data(patient_id, CONFIG[\"data_dir\"])\n",
    "#     # For GT nodules\n",
    "#     for annot in annotations:\n",
    "#         if 'malignancy' in annot: # Assume malignancy is available\n",
    "#             label = 1 if annot['malignancy'] >= 3 else 0 # Example threshold for binary\n",
    "#             cube_np = extract_cube(sitk_ct, annot['centroid_world'],\n",
    "#                                    CONFIG[\"clf_cube_size_raw\"], CONFIG[\"target_spacing_clf\"],\n",
    "#                                    CONFIG[\"clf_cube_size_final\"], CONFIG)\n",
    "#             # Save cube_np and store info\n",
    "#             # cube_path = CONFIG[\"output_dir\"] / \"classifier_cubes\" / f\"{patient_id}_{annot['id']}_gt_l{label}.npy\"\n",
    "#             # np.save(cube_path, cube_np)\n",
    "#             # all_gt_nodule_cubes_info.append({'cube_path': cube_path, 'label': label, ...})\n",
    "\n",
    "#     # For proposed nodules (run segmentation first)\n",
    "#     # proposed_nodules_this_patient, _ = get_nodule_candidates_from_segmentation(...)\n",
    "#     # For each proposed_nodule:\n",
    "#         # Check if it's a GT match. If not, it's a potential negative.\n",
    "#         # is_gt_match = check_match(proposed_nodule, annotations)\n",
    "#         # if not is_gt_match:\n",
    "#         #     cube_np = extract_cube(sitk_ct, proposed_nodule['centroid_world'], ...)\n",
    "#             # Save cube_np for negatives\n",
    "#             # all_proposed_negative_cubes_info.append({'cube_path': ..., 'label': 0, ...})\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Classifier Model (3D DenseNet121 + SE Blocks)\n",
    "#\n",
    "# - Define 3D DenseNet architecture with SE blocks.\n",
    "# - Function to load MedicalNet pre-trained weights.\n",
    "# - Define Focal Loss.\n",
    "# - Define Dataset for classifier (handles cube loading, HNM sampling).\n",
    "# - Training loop for classifier (incorporating HNM, cosine LR, early stopping).\n",
    "\n",
    "# %%\n",
    "# Placeholder for 3D DenseNet + SE (models_classification.py)\n",
    "\n",
    "class SEBlock3D(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock3D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class _DenseLayer3D(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, use_se=True):\n",
    "        super(_DenseLayer3D, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm3d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv3d(num_input_features, bn_size * growth_rate,\n",
    "                                           kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv3d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        if use_se:\n",
    "            self.add_module('se', SEBlock3D(growth_rate))\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer3D, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "class _DenseBlock3D(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, use_se=True):\n",
    "        super(_DenseBlock3D, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer3D(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate, use_se)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "class _Transition3D(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition3D, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm3d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv3d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet3D_SE(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), # DenseNet-121 like\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1, use_se=True, in_channels=1):\n",
    "        super(DenseNet3D_SE, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        # Initial convolution\n",
    "        self.features.add_module('conv0', nn.Conv3d(in_channels, num_init_features, kernel_size=7, stride=2, padding=3, bias=False))\n",
    "        self.features.add_module('norm0', nn.BatchNorm3d(num_init_features))\n",
    "        self.features.add_module('relu0', nn.ReLU(inplace=True))\n",
    "        self.features.add_module('pool0', nn.MaxPool3d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock3D(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, use_se=use_se)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition3D(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool3d(out, (1, 1, 1)).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "def load_medicalnet_weights(model, weights_path, device):\n",
    "    if weights_path and weights_path.exists():\n",
    "        print(f\"Loading MedicalNet weights from: {weights_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(weights_path, map_location=device)\n",
    "            state_dict = checkpoint.get('state_dict', checkpoint) # Handle different checkpoint formats\n",
    "\n",
    "            # Adapt keys if needed (e.g., MedicalNet might have 'module.' prefix from DataParallel)\n",
    "            # Or if layer names differ (e.g. 'features.conv0' vs 'conv0')\n",
    "            # This is a common pain point and requires inspecting both model's state_dict keys.\n",
    "            # Example adaptation:\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                # name = k.replace(\"module.\", \"\") # Remove `module.` prefix\n",
    "                name = k # Assume keys match for now or adapt as per your MedicalNet checkpoint\n",
    "                # Further adaptation may be needed depending on your DenseNet3D_SE vs MedicalNet's DenseNet\n",
    "                new_state_dict[name] = v\n",
    "\n",
    "            # Filter out mismatched keys (e.g. classifier if num_classes differs)\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in new_state_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "            \n",
    "            missing_keys, unexpected_keys = model.load_state_dict(pretrained_dict, strict=False)\n",
    "            print(f\"Weights loaded. Missing keys: {missing_keys}\")\n",
    "            print(f\"Unexpected keys in pretrained: {unexpected_keys}\")\n",
    "            if not pretrained_dict:\n",
    "                 print(\"Warning: No weights were loaded from MedicalNet checkpoint. Check layer names and shapes.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading MedicalNet weights: {e}\")\n",
    "    else:\n",
    "        print(\"MedicalNet weights path not found or not provided. Training from scratch.\")\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs_logits, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs_logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(inputs_logits)\n",
    "        pt = torch.exp(-BCE_loss) # prevents nans when probability 0\n",
    "        # pt = probs * targets + (1 - probs) * (1 - targets) # This is more direct for pt\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, positive_samples_info, negative_samples_info_all, epoch, config, transform=None):\n",
    "        \"\"\"\n",
    "        positive_samples_info: list of dicts {'cube_path': path, 'label': 1}\n",
    "        negative_samples_info_all: list of dicts {'cube_path': path, 'label': 0} (all available negatives)\n",
    "        epoch: current epoch, for HNM logic\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.epoch = epoch\n",
    "        self.transform = transform # For augmentations on cubes\n",
    "\n",
    "        self.positive_samples = positive_samples_info\n",
    "        self.current_negative_samples = []\n",
    "\n",
    "        if self.epoch < self.config[\"hnm_start_epoch\"] or not negative_samples_info_all:\n",
    "            # Randomly sample negatives or use all if few\n",
    "            num_neg_to_sample = min(len(negative_samples_info_all),\n",
    "                                    len(self.positive_samples) * self.config[\"hnm_ratio_neg_to_pos\"])\n",
    "            self.current_negative_samples = random.sample(negative_samples_info_all, num_neg_to_sample) \\\n",
    "                                            if negative_samples_info_all else []\n",
    "        else:\n",
    "            # HNM logic will be applied by the training loop before creating this dataset object for the epoch\n",
    "            # This dataset will be created with the *already mined* hard negatives for this epoch\n",
    "            self.current_negative_samples = negative_samples_info_all # Assume these are the hard ones\n",
    "\n",
    "        self.all_samples = self.positive_samples + self.current_negative_samples\n",
    "        random.shuffle(self.all_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.all_samples[idx]\n",
    "        # cube_array = np.load(sample_info['cube_path'])\n",
    "        # label = sample_info['label']\n",
    "        # Dummy data for classifier dataset\n",
    "        cube_array = np.random.rand(*self.config[\"clf_cube_size_final\"]).astype(np.float32)\n",
    "        label = random.choice([0,1]) # Dummy label\n",
    "\n",
    "        # Augment cube_array if self.transform is defined\n",
    "        # if self.transform: cube_array = self.transform(cube_array)\n",
    "\n",
    "        cube_tensor = torch.from_numpy(cube_array).float().unsqueeze(0) # C, D, H, W\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32) # .unsqueeze(0) for BCEWithLogits\n",
    "\n",
    "        return cube_tensor, label_tensor\n",
    "\n",
    "\n",
    "# --- Training Loop for Classifier (with HNM) ---\n",
    "def train_classifier_epoch(model, dataloader, optimizer, loss_fn, scaler, device, epoch_num, config):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (cubes, labels) in enumerate(tqdm(dataloader, desc=f\"Clf Training Epoch {epoch_num+1}\")):\n",
    "        cubes, labels = cubes.to(device), labels.to(device).unsqueeze(1) # Ensure label is [B,1]\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            predictions_logits = model(cubes)\n",
    "            loss = loss_fn(predictions_logits, labels)\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN/Inf loss in clf train: {loss.item()}. Skipping batch.\")\n",
    "            continue\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate_classifier_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds_probs_clf, all_labels_clf = [], []\n",
    "    with torch.no_grad():\n",
    "        for cubes, labels in tqdm(dataloader, desc=\"Clf Validation Epoch\"):\n",
    "            cubes, labels = cubes.to(device), labels.to(device).unsqueeze(1)\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                predictions_logits = model(cubes)\n",
    "                loss = loss_fn(predictions_logits, labels)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"NaN/Inf loss in clf val: {loss.item()}. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds_probs_clf.extend(torch.sigmoid(predictions_logits).cpu().numpy())\n",
    "            all_labels_clf.extend(labels.cpu().numpy())\n",
    "\n",
    "    if not all_labels_clf: return 0.0, 0.0 # Handle empty validation\n",
    "\n",
    "    all_labels_clf_np = np.array(all_labels_clf).flatten()\n",
    "    all_preds_probs_clf_np = np.array(all_preds_probs_clf).flatten()\n",
    "\n",
    "    val_auc = 0.0\n",
    "    if len(np.unique(all_labels_clf_np)) > 1: # Check if more than one class present\n",
    "        val_auc = roc_auc_score(all_labels_clf_np, all_preds_probs_clf_np)\n",
    "    else:\n",
    "        print(\"Warning: Only one class present in validation labels, AUC cannot be calculated.\")\n",
    "\n",
    "    return epoch_loss / len(dataloader), val_auc\n",
    "\n",
    "\n",
    "# Initialize Classifier Model\n",
    "clf_model = DenseNet3D_SE(\n",
    "    num_classes=CONFIG[\"clf_num_classes\"],\n",
    "    in_channels=CONFIG[\"clf_in_channels\"],\n",
    "    # growth_rate, block_config, etc. can be adjusted\n",
    ").to(DEVICE)\n",
    "\n",
    "if CONFIG[\"clf_use_medicalnet_pretrained\"]:\n",
    "    load_medicalnet_weights(clf_model, CONFIG[\"medicalnet_weights_path\"], DEVICE)\n",
    "\n",
    "clf_optimizer = optim.AdamW(clf_model.parameters(), lr=CONFIG[\"clf_lr\"], weight_decay=1e-5)\n",
    "clf_loss_fn = FocalLoss(alpha=CONFIG[\"clf_focal_loss_alpha\"], gamma=CONFIG[\"clf_focal_loss_gamma\"])\n",
    "clf_scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "clf_lr_scheduler = CosineAnnealingLR(clf_optimizer, T_max=CONFIG[\"clf_cosine_lr_t_max\"])\n",
    "# For early stopping:\n",
    "# early_stopper = ReduceLROnPlateau(clf_optimizer, mode='max', factor=0.5, patience=CONFIG[\"clf_early_stopping_patience\"] // 2, verbose=True)\n",
    "# Or custom early stopping logic based on val_auc not improving. For now, simplified.\n",
    "best_val_auc_clf = -1.0\n",
    "epochs_no_improve_clf = 0\n",
    "\n",
    "\n",
    "# --- Dummy data for classifier training ---\n",
    "# In a real scenario, these lists would be populated after cube extraction (Section 6)\n",
    "# And `all_proposed_negative_cubes_info` would be updated with HNM results each epoch\n",
    "num_dummy_pos = 50\n",
    "num_dummy_neg_total = 200\n",
    "dummy_positive_cubes_info = [{'cube_path': f'dummy_pos_{i}.npy', 'label': 1, 'patient_id': 'p_pos'} for i in range(num_dummy_pos)]\n",
    "dummy_all_negative_cubes_info = [{'cube_path': f'dummy_neg_{i}.npy', 'label': 0, 'patient_id': 'p_neg'} for i in range(num_dummy_neg_total)]\n",
    "\n",
    "# Split positives and negatives for validation to maintain similar distribution\n",
    "dummy_pos_train, dummy_pos_val = train_test_split(dummy_positive_cubes_info, test_size=0.2, random_state=SEED)\n",
    "dummy_neg_train_pool, dummy_neg_val_pool = train_test_split(dummy_all_negative_cubes_info, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "print(f\"\\n--- Training Classifier Model ({CONFIG['clf_model_name']}) ---\")\n",
    "for epoch in range(CONFIG[\"clf_epochs\"]):\n",
    "    current_negatives_for_epoch = []\n",
    "    if epoch < CONFIG[\"hnm_start_epoch\"]:\n",
    "        # Randomly sample negatives before HNM starts\n",
    "        num_to_sample = min(len(dummy_neg_train_pool), len(dummy_pos_train) * CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "        current_negatives_for_epoch = random.sample(dummy_neg_train_pool, num_to_sample) if dummy_neg_train_pool else []\n",
    "    else:\n",
    "        # --- HNM Step ---\n",
    "        print(f\"Epoch {epoch+1}: Performing Hard Negative Mining...\")\n",
    "        # Create a dataset of ALL available training negatives to score them\n",
    "        temp_hnm_dataset = ClassifierDataset([], dummy_neg_train_pool, epoch, CONFIG) # Pass epoch for consistency\n",
    "        temp_hnm_loader = DataLoader(temp_hnm_dataset, batch_size=CONFIG[\"clf_batch_size\"] * 2, shuffle=False) # Larger batch for inference\n",
    "\n",
    "        clf_model.eval() # Set model to evaluation mode for HNM scoring\n",
    "        scored_negatives = [] # List of (score, negative_info_dict)\n",
    "        with torch.no_grad():\n",
    "            for cubes_neg, _ in tqdm(temp_hnm_loader, desc=\"HNM Scoring Negatives\"):\n",
    "                cubes_neg = cubes_neg.to(DEVICE)\n",
    "                preds_logits_neg = clf_model(cubes_neg)\n",
    "                preds_probs_neg = torch.sigmoid(preds_logits_neg).cpu().numpy().flatten()\n",
    "                # Associate probs with their original info (requires careful indexing if dataset shuffles)\n",
    "                # For simplicity, assume temp_hnm_dataset.all_samples retains order or map back\n",
    "                start_idx = len(scored_negatives) # This simple indexing works if loader doesn't shuffle\n",
    "                for i in range(len(preds_probs_neg)):\n",
    "                     # This assumes temp_hnm_dataset.all_samples are the negatives from dummy_neg_train_pool in order\n",
    "                    if start_idx + i < len(temp_hnm_dataset.all_samples):\n",
    "                        neg_info = temp_hnm_dataset.all_samples[start_idx + i]\n",
    "                        scored_negatives.append((preds_probs_neg[i], neg_info))\n",
    "\n",
    "\n",
    "        scored_negatives.sort(key=lambda x: x[0], reverse=True) # Sort by prob (score), highest first (hardest)\n",
    "        num_hard_neg = CONFIG[\"hnm_num_hard_negatives_per_batch\"](len(dummy_pos_train), CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "        # The above lambda is num hard neg per batch, we need for whole epoch\n",
    "        # Correct: num_hard_neg = len(dummy_pos_train) * CONFIG[\"hnm_ratio_neg_to_pos\"]\n",
    "        num_hard_neg = min(len(scored_negatives), len(dummy_pos_train) * CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "\n",
    "        current_negatives_for_epoch = [info for score, info in scored_negatives[:num_hard_neg]]\n",
    "        print(f\"  Selected {len(current_negatives_for_epoch)} hard negatives for this epoch.\")\n",
    "        clf_model.train() # Set model back to training mode\n",
    "\n",
    "    clf_train_dataset = ClassifierDataset(dummy_pos_train, current_negatives_for_epoch, epoch, CONFIG)\n",
    "    clf_train_loader = DataLoader(clf_train_dataset, batch_size=CONFIG[\"clf_batch_size\"], shuffle=True, num_workers=0)\n",
    "\n",
    "    # Validation dataset (fixed negatives, not from HNM pool unless you design it that way)\n",
    "    num_val_neg_to_sample = min(len(dummy_neg_val_pool), len(dummy_pos_val) * CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "    val_neg_samples = random.sample(dummy_neg_val_pool, num_val_neg_to_sample) if dummy_neg_val_pool else []\n",
    "    clf_val_dataset = ClassifierDataset(dummy_pos_val, val_neg_samples, epoch, CONFIG) # epoch not used for val sampling logic here\n",
    "    clf_val_loader = DataLoader(clf_val_dataset, batch_size=CONFIG[\"clf_batch_size\"], shuffle=False, num_workers=0)\n",
    "\n",
    "    if not clf_train_loader or len(clf_train_loader.dataset) == 0:\n",
    "        print(f\"Skipping training for epoch {epoch+1} due to empty train_loader.\")\n",
    "        continue\n",
    "\n",
    "    train_loss_clf = train_classifier_epoch(clf_model, clf_train_loader, clf_optimizer, clf_loss_fn, clf_scaler, DEVICE, epoch, CONFIG)\n",
    "\n",
    "    if not clf_val_loader or len(clf_val_loader.dataset) == 0:\n",
    "        print(f\"Skipping validation for epoch {epoch+1} due to empty val_loader.\")\n",
    "        val_loss_clf, val_auc_clf = float('inf'), 0.0 # Or previous values\n",
    "    else:\n",
    "        val_loss_clf, val_auc_clf = validate_classifier_epoch(clf_model, clf_val_loader, clf_loss_fn, DEVICE)\n",
    "\n",
    "    clf_lr_scheduler.step()\n",
    "    # early_stopper.step(val_auc_clf) # If using ReduceLROnPlateau\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['clf_epochs']}: Clf Train Loss: {train_loss_clf:.4f}, Clf Val Loss: {val_loss_clf:.4f}, Clf Val AUC: {val_auc_clf:.4f}\")\n",
    "\n",
    "    if val_auc_clf > best_val_auc_clf:\n",
    "        best_val_auc_clf = val_auc_clf\n",
    "        torch.save(clf_model.state_dict(), CONFIG[\"output_dir\"] / \"classification_models\" / f\"{CONFIG['clf_model_name']}_best.pth\")\n",
    "        print(f\"  Saved best classifier model with Val AUC: {best_val_auc_clf:.4f}\")\n",
    "        epochs_no_improve_clf = 0\n",
    "    else:\n",
    "        epochs_no_improve_clf += 1\n",
    "\n",
    "    if epochs_no_improve_clf >= CONFIG[\"clf_early_stopping_patience\"]:\n",
    "        print(f\"Early stopping triggered for classifier at epoch {epoch+1} due to no improvement in Val AUC.\")\n",
    "        break\n",
    "    # if clf_optimizer.param_groups[0]['lr'] < 1e-7: # Another early stopping condition if LR gets too small\n",
    "    #     print(\"Learning rate too small, stopping early.\")\n",
    "    #     break\n",
    "\n",
    "print(\"Classifier training finished.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. End-to-End Evaluation\n",
    "#\n",
    "# - On a test set of patients:\n",
    "#   1. Run segmentation model to get probability map.\n",
    "#   2. Calculate Detection Dice:\n",
    "#      - Compare predicted nodule segmentation map (after thresholding) with ground truth nodule segmentation map.\n",
    "#      - This requires GT nodule *masks*, not just centroids.\n",
    "#   3. Perform candidate proposal (as in section 5).\n",
    "#   4. For each proposed candidate, extract cube and run classifier.\n",
    "#   5. For classification performance:\n",
    "#      - Match proposals to GT nodules to assign true labels to proposals.\n",
    "#      - Calculate AUC on these matched proposals.\n",
    "#      - Calculate overall Sensitivity @ 95% Specificity (FROC-like analysis might be more appropriate here).\n",
    "#\n",
    "# This is complex. A simplified version might be:\n",
    "# - Evaluate segmentation Dice on test set.\n",
    "# - Evaluate classifier AUC on pre-extracted GT test cubes + proposed hard negatives from test set.\n",
    "# - A true end-to-end metric would consider detection recall and then classification accuracy on correctly detected nodules.\n",
    "\n",
    "# %%\n",
    "# Placeholder for end-to-end evaluation logic.\n",
    "# This would involve iterating through test_ids, running the full pipeline.\n",
    "\n",
    "# --- Detection Dice Evaluation ---\n",
    "# Requires GT segmentation masks for nodules on the test set.\n",
    "# Create these similar to how `nodule_seg_mask_np` was created for training.\n",
    "# Then, for each test patient:\n",
    "#   pred_seg_map_np = get_segmentation_output(test_ct_sitk, seg_model, ...)\n",
    "#   pred_binary_map = (pred_seg_map_np > CONFIG[\"seg_prob_threshold\"])\n",
    "#   dice = calculate_dice(pred_binary_map, gt_nodule_mask_np)\n",
    "#   Store and average Dice scores.\n",
    "\n",
    "# --- Classification Metrics on Test Set ---\n",
    "# Load best classifier model\n",
    "best_clf_model_path = CONFIG[\"output_dir\"] / \"classification_models\" / f\"{CONFIG['clf_model_name']}_best.pth\"\n",
    "if best_clf_model_path.exists():\n",
    "    clf_model.load_state_dict(torch.load(best_clf_model_path, map_location=DEVICE))\n",
    "    print(f\"Loaded best classifier model from {best_clf_model_path}\")\n",
    "else:\n",
    "    print(\"WARNING: Best classifier model not found. Using last epoch model for final eval.\")\n",
    "\n",
    "\n",
    "# Generate test cubes (GT positives and proposed negatives from test set scans)\n",
    "# dummy_test_pos_cubes_info = ...\n",
    "# dummy_test_neg_cubes_info_pool = ... (all negatives found on test scans)\n",
    "# For a fair AUC, you might want a balanced set of test negatives or use all FPs from detection.\n",
    "\n",
    "# For this example, re-use validation cube lists as dummy test lists\n",
    "clf_test_dataset = ClassifierDataset(dummy_pos_val, val_neg_samples, CONFIG[\"clf_epochs\"], CONFIG) # epoch doesn't matter for fixed test set\n",
    "clf_test_loader = DataLoader(clf_test_dataset, batch_size=CONFIG[\"clf_batch_size\"], shuffle=False)\n",
    "\n",
    "test_loss_clf, test_auc_clf = validate_classifier_epoch(clf_model, clf_test_loader, clf_loss_fn, DEVICE) # Reuses validation function\n",
    "print(f\"\\n--- Classifier Test Set Performance ---\")\n",
    "print(f\"Test Loss: {test_loss_clf:.4f}, Test AUC: {test_auc_clf:.4f}\")\n",
    "\n",
    "# For Sensitivity @ Specificity:\n",
    "# Need all_preds_probs and all_labels from the test set (returned by validate_classifier_epoch if adapted)\n",
    "# Example: (Assuming validate_classifier_epoch is modified to return probs and labels)\n",
    "# _, final_test_probs, final_test_labels = validate_classifier_epoch_for_eval(clf_model, clf_test_loader, clf_loss_fn, DEVICE)\n",
    "# fpr, tpr, thresholds = roc_curve(final_test_labels, final_test_probs)\n",
    "# # Find threshold for 95% specificity (Specificity = 1 - FPR)\n",
    "# target_fpr = 1.0 - CONFIG[\"eval_sensitivity_at_specificity\"]\n",
    "# closest_fpr_idx = np.argmin(np.abs(fpr - target_fpr))\n",
    "# sensitivity_at_target_spec = tpr[closest_fpr_idx]\n",
    "# threshold_at_target_spec = thresholds[closest_fpr_idx]\n",
    "# print(f\"Sensitivity at {CONFIG['eval_sensitivity_at_specificity']*100}% Specificity: {sensitivity_at_target_spec:.4f} (Threshold: {threshold_at_target_spec:.4f})\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Visualization\n",
    "#\n",
    "# - **3D Overlay:** Show CT scan with predicted nodule segmentations (or bounding boxes) overlaid.\n",
    "#   - `itkwidgets` or slice-by-slice `matplotlib`.\n",
    "# - **GradCAM:** For the classifier, on input cubes.\n",
    "#   - Adapt standard GradCAM implementations for 3D and your DenseNet architecture.\n",
    "#   - This requires access to the model's convolutional feature maps and gradients.\n",
    "\n",
    "# %%\n",
    "# Placeholder for 3D overlay visualization (visualization_tools.py)\n",
    "def visualize_3d_overlay(ct_np_array, seg_mask_np_array, slice_idx=None, title=\"\"):\n",
    "    \"\"\" Simple slice-wise overlay \"\"\"\n",
    "    if slice_idx is None:\n",
    "        slice_idx = ct_np_array.shape[0] // 2 # Middle slice for Depth\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(ct_np_array[slice_idx, :, :], cmap='gray')\n",
    "    plt.title(f\"CT Slice {slice_idx}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(ct_np_array[slice_idx, :, :], cmap='gray')\n",
    "    plt.imshow(seg_mask_np_array[slice_idx, :, :], cmap='jet', alpha=0.5, vmin=0, vmax=1) # Overlay mask\n",
    "    plt.title(f\"CT + Seg Overlay Slice {slice_idx}\")\n",
    "    plt.axis('off')\n",
    "    if title: plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "# test_patient_ct_np = preprocess_image(sitk_ct_test, CONFIG[\"target_spacing_seg\"], ...)\n",
    "# test_patient_pred_seg_map_np = seg_map_for_dice # from get_nodule_candidates\n",
    "# visualize_3d_overlay(test_patient_ct_np, test_patient_pred_seg_map_np > CONFIG[\"seg_prob_threshold\"], title=f\"Pred Seg for {patient_to_test_seg}\")\n",
    "\n",
    "\n",
    "# Placeholder for GradCAM (visualization_tools.py)\n",
    "# This is non-trivial to implement fully here.\n",
    "# Key steps:\n",
    "# 1. Hook the target convolutional layer in your classifier.\n",
    "# 2. Forward pass with the cube to get activations and logits.\n",
    "# 3. Backward pass from the target class logit to get gradients w.r.t activations.\n",
    "# 4. Compute weights (global average pool of gradients).\n",
    "# 5. Compute weighted sum of activation maps.\n",
    "# 6. ReLU and normalize heatmap.\n",
    "# 7. Upsample heatmap to cube size and overlay.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Logging Metrics, ROC Curves, and Final Report\n",
    "#\n",
    "# - Log all relevant metrics (Dice, AUC, Loss, Acc, Sens/Spec) for train/val/test.\n",
    "# - Plot and save ROC curves for the classifier.\n",
    "# - **Final Report Generation:**\n",
    "#   - Summarize dataset characteristics.\n",
    "#   - Segmentation model performance (Dice).\n",
    "#   - Candidate proposal statistics (e.g., number of candidates per scan, FPs).\n",
    "#   - Classifier performance (AUC, Sens@Spec, other metrics from classification_report).\n",
    "#   - **Comparison: Pure 3D CNN vs. Cascade Approach:**\n",
    "#     - To do this properly, you'd need to define, train, and evaluate a \"Pure 3D CNN\" for direct nodule classification (e.g., on the same extracted cubes, or an end-to-end detection model).\n",
    "#     - For this notebook, we can describe what such a model would be and qualitatively compare the expected pros/cons based on the cascade's performance.\n",
    "\n",
    "# %%\n",
    "# --- Plot ROC Curve for Classifier ---\n",
    "# Assuming final_test_labels, final_test_probs are available from evaluation\n",
    "# fpr, tpr, thresholds = roc_curve(final_test_labels, final_test_probs)\n",
    "# roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "# plt.title('Classifier ROC Curve (Test Set)')\n",
    "# plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "# plt.savefig(CONFIG[\"output_dir\"] / \"reports\" / \"classifier_roc_curve.png\")\n",
    "# plt.show()\n",
    "\n",
    "# --- Final Report (Markdown or text file) ---\n",
    "report_content = f\"\"\"\n",
    "# Lung Nodule Detection and Classification Pipeline Report\n",
    "\n",
    "## 1. Dataset\n",
    "- Source: (e.g., LIDC-IDRI subset)\n",
    "- Number of patients (Train/Val/Test): {len(train_ids)}/{len(val_ids)}/{len(test_ids)}\n",
    "- Annotation details: (e.g., nodule centroids, diameters, malignancy scores)\n",
    "\n",
    "## 2. Preprocessing\n",
    "- CT Resampling Spacing (Seg): {CONFIG[\"target_spacing_seg\"]}\n",
    "- CT Resampling Spacing (Clf): {CONFIG[\"target_spacing_clf\"]}\n",
    "- HU Clipping: {CONFIG[\"hu_clip_bounds\"]}\n",
    "- Normalization: Mean={CONFIG[\"norm_mean_std\"][\"mean\"]}, Std={CONFIG[\"norm_mean_std\"][\"std\"]}\n",
    "\n",
    "## 3. Cascade Pipeline Performance\n",
    "\n",
    "### 3.1. Segmentation Model ({CONFIG['seg_model_name']})\n",
    "- Architecture: Custom 3D U-Net\n",
    "- Training Epochs: {CONFIG[\"seg_epochs\"]}\n",
    "- Best Validation Dice: {best_val_dice_seg:.4f} (if tracked)\n",
    "- Test Set Dice: (Calculate this on test set)\n",
    "\n",
    "### 3.2. Candidate Proposal\n",
    "- Method: Thresholding segmentation map ({CONFIG[\"seg_prob_threshold\"]}) + Connected Components\n",
    "- Min Nodule Size: {CONFIG[\"min_nodule_size_voxels\"]} voxels\n",
    "- Statistics: (e.g., Avg candidates/scan, False Positive Rate of proposals - requires matching to GT)\n",
    "\n",
    "### 3.3. Classification Model ({CONFIG['clf_model_name']})\n",
    "- Architecture: 3D DenseNet121 with SE Blocks\n",
    "- Pre-trained weights: {\"MedicalNet\" if CONFIG[\"clf_use_medicalnet_pretrained\"] else \"From Scratch\"}\n",
    "- Loss Function: Focal Loss (alpha={CONFIG[\"clf_focal_loss_alpha\"]}, gamma={CONFIG[\"clf_focal_loss_gamma\"]})\n",
    "- Training Strategy: Hard Negative Mining (ratio={CONFIG[\"hnm_ratio_neg_to_pos\"]}), Cosine LR, Early Stopping (patience={CONFIG[\"clf_early_stopping_patience\"]})\n",
    "- Training Epochs: {CONFIG[\"clf_epochs\"]} (or actual if early stopped)\n",
    "- Best Validation AUC: {best_val_auc_clf:.4f}\n",
    "- **Test Set Performance:**\n",
    "    - AUC: {test_auc_clf:.4f} (if calculated)\n",
    "    - Sensitivity @ {CONFIG[\"eval_sensitivity_at_specificity\"]*100}% Specificity: (Calculate this)\n",
    "    - Other metrics: (Add Precision, Recall, F1 from classification_report on test proposals)\n",
    "\n",
    "## 4. Comparison: Cascade vs. Pure 3D CNN Approach\n",
    "\n",
    "### 4.1. Pure 3D CNN (Hypothetical or Implemented)\n",
    "- Define the architecture (e.g., {CONFIG['comp_model_name']} - a 3D ResNet or simpler CNN).\n",
    "- Training: On extracted cubes directly for malignancy classification.\n",
    "- Performance: (Report its AUC, Sens/Spec if implemented and trained).\n",
    "\n",
    "### 4.2. Discussion\n",
    "- **Cascade Approach Pros:**\n",
    "    - Modular: Can optimize segmentation and classification separately.\n",
    "    - Segmentation can find a wide range of candidates.\n",
    "    - Classifier focuses only on small, relevant cubes.\n",
    "    - May handle varying nodule sizes well if segmentation is robust.\n",
    "- **Cascade Approach Cons:**\n",
    "    - Error propagation: Segmentation errors directly impact classification.\n",
    "    - Complex pipeline to manage and tune.\n",
    "    - Candidate generation step can be critical and hard to optimize (many FPs).\n",
    "- **Pure 3D CNN (Direct Classification/Detection) Pros:**\n",
    "    - Potentially simpler end-to-end training.\n",
    "    - Learns features directly for the final task.\n",
    "    - No intermediate hard-to-tune proposal stage if using an end-to-end detector.\n",
    "- **Pure 3D CNN Cons:**\n",
    "    - If classifying cubes directly, still needs a good proposal mechanism or processes whole scan patches (computationally expensive).\n",
    "    - End-to-end detectors (like 3D RetinaNet) are complex to implement and train, very data-hungry.\n",
    "    - May struggle with highly imbalanced data if not handled carefully (many non-nodule regions).\n",
    "\n",
    "## 5. Conclusion and Future Work\n",
    "- Summary of findings.\n",
    "- Limitations of the current pipeline.\n",
    "- Potential improvements (e.g., better data augmentation, more advanced HNM, different model architectures, full end-to-end model).\n",
    "\"\"\"\n",
    "\n",
    "report_path = CONFIG[\"output_dir\"] / \"reports\" / \"final_pipeline_report.md\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "print(f\"Final report structure saved to {report_path}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
