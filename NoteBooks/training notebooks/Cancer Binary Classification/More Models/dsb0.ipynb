{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7474f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Placeholder: Loading and selecting data...\n",
      "Selected 100 scans (50 class 0, 50 class 1)\n",
      "Placeholder: Preprocessing scans...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Simulation: 100%|██████████| 100/100 [00:00<00:00, 14248.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed/found 100 scans.\n",
      "Placeholder: Creating dataloaders...\n",
      "Created DataLoaders: Train batches=40, Val batches=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated pos_weight for BCEWithLogitsLoss: 1.00\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape mismatch for 1fdbc07019192de4a114e090389c8330: Expected (32, 32, 32), got (64, 64, 64). Attempting resize.\n",
      "Shape mismatch for 4b351d0c19be183cc880f5af3fe5abee: Expected (32, 32, 32), got (64, 64, 64). Attempting resize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- An error occurred during execution ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rouaa\\AppData\\Local\\Temp\\ipykernel_21784\\2919523034.py\", line 986, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\rouaa\\AppData\\Local\\Temp\\ipykernel_21784\\2919523034.py\", line 822, in main\n",
      "    train_loss_cls, train_loss_seg, train_loss_total, train_acc = train_one_epoch(\n",
      "                                                                  ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\rouaa\\AppData\\Local\\Temp\\ipykernel_21784\\2919523034.py\", line 624, in train_one_epoch\n",
      "    loss_seg = criterion_seg(seg_outputs_probs, masks)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 697, in forward\n",
      "    return F.binary_cross_entropy(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\rouaa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3554, in binary_cross_entropy\n",
      "    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\n",
      "Many models use a sigmoid layer right before the binary cross entropy layer.\n",
      "In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\n",
      "or torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\n",
      "safe to autocast.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Refined Advanced End-to-End Lung Nodule Detection and Classification Pipeline\n",
    "#\n",
    "# This notebook implements a refined advanced classifier model balancing novel components with practical trainability\n",
    "# for limited datasets.\n",
    "#\n",
    "# **Classifier Architecture (`AdvancedHybridNetRefined`):**\n",
    "# 1. **Anisotropic Convolutional Stem.**\n",
    "# 2. **Hierarchical Backbone:** Stages of custom \"DenseResLayers\" with SE and spatial dropout,\n",
    "#    followed by downsampling. Simplified attention applied at one or two scales.\n",
    "# 3. **Convolution-Free Transformer Head.**\n",
    "#\n",
    "# **Training Strategies:** As previously outlined (Focal Loss, Augmentations, AdamW, Cosine LR).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports (Same as previous)\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns # Optional\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, auc, f1_score,\n",
    "    precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay\n",
    ")\n",
    "from skimage.measure import label as skimage_label, regionprops\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "seed_everything(SEED)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Refined Configuration\n",
    "\n",
    "# %%\n",
    "CONFIG = {\n",
    "    # --- Paths ---\n",
    "    \"data_dir\": Path(\"/path/to/your/LIDC-IDRI-like/dataset\"), # Placeholder\n",
    "    \"output_dir\": Path(\"./output_nodule_pipeline_refined\"),\n",
    "    # \"medicalnet_weights_path\": Path(\"...\"), # Unlikely to be directly compatible\n",
    "\n",
    "    # --- General Preprocessing ---\n",
    "    \"target_spacing_clf\": [1.0, 1.0, 1.0],\n",
    "    \"hu_clip_bounds\": [-1000, 400],\n",
    "    \"norm_mean_std\": {\"mean\": 0.25, \"std\": 0.25},\n",
    "\n",
    "    # --- Segmentation & Proposal (Assume reused/pre-computed for classifier focus) ---\n",
    "    \"seg_model_name\": \"Pretrained_Custom3DUNet\",\n",
    "    \"seg_output_dir\": Path(\"./output_nodule_pipeline/segmentation_models\"),\n",
    "    \"seg_prob_threshold\": 0.5,\n",
    "    \"min_nodule_size_voxels\": 20,\n",
    "\n",
    "    # --- Candidate Cube Generation ---\n",
    "    \"clf_cube_size_final\": [40, 48, 48], # D, H, W input to classifier\n",
    "\n",
    "    # --- Refined Classifier Model (`AdvancedHybridNetRefined`) ---\n",
    "    \"clf_model_name\": \"AdvancedHybridNetRefined\",\n",
    "    \"clf_in_channels\": 1,\n",
    "    \"clf_num_classes\": 1,\n",
    "    \"clf_anisotropic_stem_out_channels\": 32,\n",
    "    # Backbone Stage Config: list of tuples (num_dense_res_layers, out_channels_stage, use_attention_after_stage)\n",
    "    \"clf_backbone_stages_config\": [\n",
    "        (2, 64, False), # Stage 1: 2 DenseResLayers, output 64 channels, no specific attention here\n",
    "        (3, 128, True), # Stage 2: 3 DenseResLayers, output 128 channels, apply attention\n",
    "        (4, 256, True), # Stage 3: 4 DenseResLayers, output 256 channels, apply attention\n",
    "    ],\n",
    "    \"clf_dense_res_growth_rate\": 16,\n",
    "    \"clf_dense_res_bn_size\": 4, # Bottleneck factor in dense_res_layer\n",
    "    \"clf_dense_res_se_reduction\": 8,\n",
    "    \"clf_dense_res_spatial_dropout\": 0.1,\n",
    "    \"clf_dense_res_stochastic_depth_prob\": 0.1, # For DropPath around the layer output\n",
    "    \"clf_attention_module_heads\": 4, # For attention modules after stages\n",
    "    # Transformer Head (operates on output of last backbone stage)\n",
    "    \"clf_transformer_head_patch_size\": (4, 4, 4), # (D,H,W) for tokenization\n",
    "    \"clf_transformer_head_embed_dim\": 128,\n",
    "    \"clf_transformer_head_depth\": 2,\n",
    "    \"clf_transformer_head_num_heads\": 4,\n",
    "\n",
    "    # --- Training & Optimization ---\n",
    "    \"clf_batch_size\": 8,\n",
    "    \"clf_lr_initial\": 1e-4, # Slightly reduced from 1e-3\n",
    "    \"clf_weight_decay\": 1e-5,\n",
    "    \"clf_epochs\": 75, # Adjusted\n",
    "    \"clf_focal_loss_alpha\": 0.25,\n",
    "    \"clf_focal_loss_gamma\": 2.0,\n",
    "    \"clf_early_stopping_patience\": 15,\n",
    "    \"clf_cosine_lr_t_max\": 75,\n",
    "\n",
    "    # Augmentation\n",
    "    \"aug_elastic_alpha_sigma\": ((0, 360.0), (7.0, 9.0)), # (alpha_range, sigma_range) for elastic\n",
    "    \"aug_intensity_scale_range\": (0.8, 1.2),\n",
    "    \"aug_mixup_alpha\": 0.2,\n",
    "\n",
    "    # HNM\n",
    "    \"hnm_start_epoch\": 10,\n",
    "    \"hnm_ratio_neg_to_pos\": 2,\n",
    "\n",
    "    # Evaluation\n",
    "    \"eval_sensitivity_at_specificity\": 0.95,\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"classification_models_refined\").mkdir(exist_ok=True) # New folder\n",
    "(CONFIG[\"output_dir\"] / \"visualizations_refined\").mkdir(exist_ok=True)\n",
    "(CONFIG[\"output_dir\"] / \"reports_refined\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(CONFIG[\"output_dir\"] / \"config_refined.json\", 'w') as f:\n",
    "    json.dump({k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}, f, indent=4)\n",
    "print(\"Refined Configuration saved.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Handling (Dataset, Augmentation, Cube Extraction - Reuse & Adapt)\n",
    "\n",
    "# %%\n",
    "# --- Preprocessing & Cube Extraction (load_patient_data, preprocess_image, extract_cube etc. are assumed defined)\n",
    "# --- For brevity, these functions are not repeated here. Import them or define them from previous cells.\n",
    "\n",
    "# --- DropPath (Stochastic Depth) ---\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training: return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "# --- Augmentation Functions (Placeholders - Implement using libraries like TorchIO or batchgenerators) ---\n",
    "def augment_elastic_deformation_3d_placeholder(volume, alpha_sigma_ranges, random_state=None):\n",
    "    # print(\"Placeholder: Elastic Deformation\")\n",
    "    # Example: alpha = random.uniform(*alpha_sigma_ranges[0])\n",
    "    #          sigma = random.uniform(*alpha_sigma_ranges[1])\n",
    "    #          # Apply transform...\n",
    "    return volume\n",
    "\n",
    "def augment_intensity_scaling_3d_placeholder(volume, scale_range, random_state=None):\n",
    "    # print(\"Placeholder: Intensity Scaling\")\n",
    "    if random_state is None: random_state = np.random.RandomState(None)\n",
    "    scale = random_state.uniform(scale_range[0], scale_range[1])\n",
    "    return volume * scale\n",
    "\n",
    "\n",
    "class RefinedClassifierDataset(Dataset): # Adapted from AdvancedClassifierDataset\n",
    "    def __init__(self, positive_samples_info, negative_samples_info_for_epoch, epoch, config, is_train=True):\n",
    "        self.config = config\n",
    "        self.epoch = epoch\n",
    "        self.is_train = is_train\n",
    "        self.all_samples = positive_samples_info + negative_samples_info_for_epoch\n",
    "        if is_train: random.shuffle(self.all_samples)\n",
    "\n",
    "        self.aug_elastic_alpha_sigma = config.get(\"aug_elastic_alpha_sigma\", ((0,0),(1,1)))\n",
    "        self.aug_intensity_scale_range = config.get(\"aug_intensity_scale_range\", (1.0, 1.0))\n",
    "\n",
    "    def __len__(self): return len(self.all_samples)\n",
    "    def __getitem__(self, idx):\n",
    "        sample_info = self.all_samples[idx]\n",
    "        # cube_array = np.load(sample_info['cube_path']) # Load actual cube\n",
    "        # label = sample_info['label']\n",
    "\n",
    "        # Dummy data for execution\n",
    "        cube_array = np.random.rand(*self.config[\"clf_cube_size_final\"]).astype(np.float32) - 0.5 # Centered around 0\n",
    "        label = random.choice([0, 1])\n",
    "\n",
    "        if self.is_train:\n",
    "            if random.random() < 0.3: # Apply elastic with some probability\n",
    "                 cube_array = augment_elastic_deformation_3d_placeholder(cube_array, self.aug_elastic_alpha_sigma)\n",
    "            if random.random() < 0.3: # Apply intensity scaling\n",
    "                 cube_array = augment_intensity_scaling_3d_placeholder(cube_array, self.aug_intensity_scale_range)\n",
    "            # Add other augmentations: gamma, noise, blur, mirror as in nnU-Net\n",
    "\n",
    "        cube_tensor = torch.from_numpy(cube_array.copy()).float().unsqueeze(0)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return cube_tensor, label_tensor, sample_info\n",
    "\n",
    "# --- Candidate lists & splits (Same dummy data generation as before for now) ---\n",
    "num_dummy_pos_ref = 60\n",
    "num_dummy_neg_total_ref = 250\n",
    "dummy_positive_cubes_info_ref = [{'cube_path': f'dummy_pos_ref_{i}.npy', 'label': 1, 'id':f'pos{i}'} for i in range(num_dummy_pos_ref)]\n",
    "dummy_all_negative_cubes_info_ref = [{'cube_path': f'dummy_neg_ref_{i}.npy', 'label': 0, 'id':f'neg{i}'} for i in range(num_dummy_neg_total_ref)]\n",
    "dummy_pos_train_ref, dummy_pos_val_ref = train_test_split(dummy_positive_cubes_info_ref, test_size=0.2, random_state=SEED)\n",
    "dummy_neg_train_pool_ref, dummy_neg_val_pool_ref = train_test_split(dummy_all_negative_cubes_info_ref, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Refined Classifier Model Definition (`AdvancedHybridNetRefined`)\n",
    "\n",
    "# %%\n",
    "# --- Component Blocks (SEBlock3D, AnisotropicConvModule, AttentionModule, TransformerEncoderLayer, ConvFreeTransformerHead)\n",
    "# --- Assume these are defined as in the previous \"Advanced\" blueprint. For brevity, not repeating all.\n",
    "# --- We will define a new \"DenseResLayer\" suitable for the backbone.\n",
    "\n",
    "class SEBlock3D(nn.Module): # Copied for self-containment\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock3D, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size(); y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1); return x * y.expand_as(x)\n",
    "\n",
    "class AnisotropicConvModule(nn.Module): # Copied\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_xy = nn.Conv3d(in_channels, out_channels // 2, kernel_size=(1,3,3), padding=(0,1,1), bias=False)\n",
    "        self.bn_xy = nn.BatchNorm3d(out_channels // 2)\n",
    "        self.conv_z = nn.Conv3d(in_channels, out_channels // 2, kernel_size=(3,1,1), padding=(1,0,0), bias=False)\n",
    "        self.bn_z = nn.BatchNorm3d(out_channels // 2)\n",
    "        self.relu = nn.LeakyReLU(0.01, inplace=True) # Use LeakyReLU\n",
    "        self.fuse_conv = nn.Conv3d(out_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn_fuse = nn.BatchNorm3d(out_channels)\n",
    "    def forward(self, x):\n",
    "        x_xy = self.relu(self.bn_xy(self.conv_xy(x)))\n",
    "        x_z = self.relu(self.bn_z(self.conv_z(x)))\n",
    "        x_cat = torch.cat([x_xy, x_z], dim=1)\n",
    "        x_fused = self.relu(self.bn_fuse(self.fuse_conv(x_cat)))\n",
    "        return x_fused\n",
    "\n",
    "class AttentionModule(nn.Module): # Copied (Simplified Spatial Attention)\n",
    "    def __init__(self, in_channels, num_heads=4, proj_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads; self.head_dim = in_channels // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv_conv = nn.Conv3d(in_channels, in_channels * 3, kernel_size=1, bias=False)\n",
    "        self.proj_conv = nn.Conv3d(in_channels, in_channels, kernel_size=1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x): # x: B, C, D, H, W\n",
    "        B, C, D, H, W = x.shape\n",
    "        qkv = self.qkv_conv(x).reshape(B, 3, self.num_heads, self.head_dim, D*H*W).permute(1,0,2,4,3) # 3,B,nH,L,hd\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale; attn = attn.softmax(dim=-1)\n",
    "        x_attn = (attn @ v).transpose(-2,-1).reshape(B,C,D,H,W)\n",
    "        return self.proj_drop(self.proj_conv(x_attn))\n",
    "\n",
    "\n",
    "# --- New DenseResLayer for Backbone Stages ---\n",
    "class DenseResLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A layer inspired by DenseNet and ResNet:\n",
    "    - BN -> ReLU -> Conv1x1x1 (bottleneck) -> BN -> ReLU -> Conv3x3x3\n",
    "    - Output of Conv3x3x3 is `growth_rate` channels.\n",
    "    - This output is concatenated with the input to the layer (feature reuse from DenseNet).\n",
    "    - A residual connection (1x1x1 conv if channels change) around this new concatenated feature map.\n",
    "    - Includes SE, Spatial Dropout, Stochastic Depth (DropPath).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, growth_rate, bn_size, se_reduction, spatial_dropout, stochastic_depth_prob):\n",
    "        super().__init__()\n",
    "        self.stochastic_depth = DropPath(stochastic_depth_prob)\n",
    "        bottleneck_channels = bn_size * growth_rate\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm3d(in_channels),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv3d(in_channels, bottleneck_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(bottleneck_channels),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv3d(bottleneck_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "        self.se = SEBlock3D(growth_rate, reduction=se_reduction)\n",
    "        self.spatial_dropout = nn.Dropout3d(spatial_dropout) if spatial_dropout > 0 else nn.Identity()\n",
    "\n",
    "        # For the residual connection around the (input + new_features_from_block)\n",
    "        # The output of this layer will be input_channels + growth_rate\n",
    "        # If we want a residual connection that sums with the input, projection is needed.\n",
    "        # For simplicity of a \"DenseRes\" idea, we'll make the output be input_channels + growth_rate\n",
    "        # and the \"residual\" aspect is more in spirit of robust feature propagation.\n",
    "        # Or, let the block output be growth_rate channels and add it to input after projection.\n",
    "\n",
    "        # Let's go with: Layer computes `growth_rate` features.\n",
    "        # These `growth_rate` features are then added to the input (if channels match, or via projection)\n",
    "        # AND concatenated for the next layer if part of a \"dense\" sequence.\n",
    "\n",
    "        # Simpler for this context: Each layer produces 'growth_rate' new features,\n",
    "        # which are concatenated to the input to form the output of this \"dense_res_layer\"\n",
    "        # The 'residual' part is the DropPath making it sometimes an identity for the new features.\n",
    "\n",
    "    def forward(self, x): # x is input tensor to this specific layer\n",
    "        new_features = self.conv_block(x)\n",
    "        new_features = self.se(new_features)\n",
    "        new_features = self.spatial_dropout(new_features)\n",
    "        new_features_maybe_dropped = self.stochastic_depth(new_features)\n",
    "        \n",
    "        # DenseNet-style: concatenate input with new features\n",
    "        output = torch.cat([x, new_features_maybe_dropped], dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "# --- Convolution-Free Transformer Head (Copied, ensure it's loaded) ---\n",
    "class TransformerEncoderLayer(nn.Module): # Copied\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1, act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = norm_layer(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)), act_layer(), nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim), nn.Dropout(dropout))\n",
    "        self.dropout = nn.Dropout(dropout) # For residual path\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_norm = self.norm1(src)\n",
    "        src_attn, _ = self.attn(src_norm, src_norm, src_norm, need_weights=False)\n",
    "        src = src + self.dropout(src_attn)\n",
    "        src_norm = self.norm2(src)\n",
    "        src_mlp = self.mlp(src_norm)\n",
    "        src = src + self.dropout(src_mlp)\n",
    "        return src\n",
    "\n",
    "class ConvFreeTransformerHead(nn.Module): # Copied\n",
    "    def __init__(self, in_channels, feature_map_size_dhw, patch_size_dhw, embed_dim, depth, num_heads, num_classes):\n",
    "        super().__init__()\n",
    "        self.patch_d, self.patch_h, self.patch_w = patch_size_dhw\n",
    "        self.feat_d, self.feat_h, self.feat_w = feature_map_size_dhw\n",
    "        assert self.feat_d % self.patch_d == 0 and self.feat_h % self.patch_h == 0 and self.feat_w % self.patch_w == 0, \"Feature map not divisible by patch size\"\n",
    "        self.num_patches = (self.feat_d//self.patch_d) * (self.feat_h//self.patch_h) * (self.feat_w//self.patch_w)\n",
    "        patch_dim_flat = in_channels * self.patch_d * self.patch_h * self.patch_w\n",
    "        self.patch_proj = nn.Linear(patch_dim_flat, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim)); nn.init.trunc_normal_(self.cls_token,std=.02)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,self.num_patches+1,embed_dim)); nn.init.trunc_normal_(self.pos_embed,std=.02)\n",
    "        self.transformer_layers = nn.Sequential(*[TransformerEncoderLayer(embed_dim,num_heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim,num_classes)\n",
    "    def forward(self,x): # B,C,Df,Hf,Wf\n",
    "        B,C,_,_,_ = x.shape\n",
    "        # unfold in PyTorch can be tricky for multiple dimensions simultaneously for patching.\n",
    "        # Alternative: view + permute\n",
    "        # B, C, nPd*pD, nPh*pH, nPw*pW -> B, C, nPd, pD, nPh, pH, nPw, pW\n",
    "        x_patched = x.view(B, C,\n",
    "                           self.feat_d // self.patch_d, self.patch_d,\n",
    "                           self.feat_h // self.patch_h, self.patch_h,\n",
    "                           self.feat_w // self.patch_w, self.patch_w)\n",
    "        x_patched = x_patched.permute(0,2,4,6,1,3,5,7).contiguous() # B,nPd,nPh,nPw, C,pD,pH,pW\n",
    "        x_tokens = x_patched.view(B, self.num_patches, -1) # B, num_patches, C*pD*pH*pW\n",
    "        x_proj = self.patch_proj(x_tokens)\n",
    "        cls = self.cls_token.expand(B,-1,-1)\n",
    "        x_emb = torch.cat((cls,x_proj),dim=1) + self.pos_embed\n",
    "        x_tf = self.transformer_layers(x_emb)\n",
    "        return self.head(self.norm(x_tf[:,0]))\n",
    "\n",
    "\n",
    "# --- Refined Main Classifier Model ---\n",
    "class AdvancedHybridNetRefined(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        in_c = config[\"clf_in_channels\"]\n",
    "        stem_out_c = config[\"clf_anisotropic_stem_out_channels\"]\n",
    "\n",
    "        self.stem = AnisotropicConvModule(in_c, stem_out_c)\n",
    "        current_channels = stem_out_c\n",
    "        \n",
    "        # D, H, W after stem (AnisotropicConvModule doesn't change spatial dims with padding)\n",
    "        # But check your implementation if it does include strides.\n",
    "        current_dhw = list(config[\"clf_cube_size_final\"])\n",
    "\n",
    "        self.backbone_stages = nn.ModuleList()\n",
    "        self.attention_stages = nn.ModuleList() # Store attention modules if used\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "\n",
    "        for i, (num_layers, stage_out_c, use_attention) in enumerate(config[\"clf_backbone_stages_config\"]):\n",
    "            stage_layers = nn.ModuleList()\n",
    "            # Input to the first DenseResLayer of this stage is `current_channels`\n",
    "            stage_in_channels = current_channels\n",
    "            for _ in range(num_layers):\n",
    "                layer = DenseResLayer(\n",
    "                    in_channels=current_channels, # Input to this specific layer\n",
    "                    growth_rate=config[\"clf_dense_res_growth_rate\"],\n",
    "                    bn_size=config[\"clf_dense_res_bn_size\"],\n",
    "                    se_reduction=config[\"clf_dense_res_se_reduction\"],\n",
    "                    spatial_dropout=config[\"clf_dense_res_spatial_dropout\"],\n",
    "                    stochastic_depth_prob=config[\"clf_dense_res_stochastic_depth_prob\"]\n",
    "                )\n",
    "                stage_layers.append(layer)\n",
    "                current_channels += config[\"clf_dense_res_growth_rate\"] # Output of DenseResLayer\n",
    "            self.backbone_stages.append(stage_layers) # Add list of layers for this stage\n",
    "\n",
    "            if use_attention:\n",
    "                # Apply attention to the output of the stage (after all DenseResLayers in it)\n",
    "                # The input to attention is `current_channels`\n",
    "                self.attention_stages.append(AttentionModule(current_channels, config[\"clf_attention_module_heads\"]))\n",
    "            else:\n",
    "                self.attention_stages.append(nn.Identity()) # Placeholder if no attention\n",
    "\n",
    "            if i < len(config[\"clf_backbone_stages_config\"]) - 1: # Add downsampling except for last stage\n",
    "                # Transition layer halves channels from current_channels\n",
    "                # We use a simple MaxPool3D for downsampling for robustness of channel numbers.\n",
    "                # A _Transition3D could also be used but ensure `stage_out_c` is respected.\n",
    "                # For simplicity, let's make `stage_out_c` the target *after* downsampling for next stage.\n",
    "                # However, DenseResLayer keeps adding. So current_channels is output of dense stage.\n",
    "                # The config \"stage_out_c\" might be better interpreted as the channel dim *after* transition for the *next* stage\n",
    "                # Or, simpler: the transition layer brings `current_channels` to `next_stage_in_channels`.\n",
    "                \n",
    "                # Using MaxPool + 1x1 Conv to control channels and downsample\n",
    "                ds_layer = nn.Sequential(\n",
    "                    nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "                    nn.Conv3d(current_channels, config[\"clf_backbone_stages_config\"][i+1][1], # target channels for next stage\n",
    "                              kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm3d(config[\"clf_backbone_stages_config\"][i+1][1]),\n",
    "                    nn.LeakyReLU(0.01, inplace=True)\n",
    "                )\n",
    "                self.downsample_layers.append(ds_layer)\n",
    "                current_channels = config[\"clf_backbone_stages_config\"][i+1][1] # Update for next stage's input\n",
    "                current_dhw = [d // 2 for d in current_dhw]\n",
    "            else: # Last stage\n",
    "                # No downsampling layer after the final backbone stage feeding the head\n",
    "                pass\n",
    "\n",
    "\n",
    "        self.final_feature_map_channels = current_channels\n",
    "        self.final_feature_map_dhw = tuple(current_dhw)\n",
    "        print(f\"DEBUG: Final CNN feature map D,H,W before head: {self.final_feature_map_dhw}\")\n",
    "        print(f\"DEBUG: Final CNN feature map Channels before head: {self.final_feature_map_channels}\")\n",
    "\n",
    "\n",
    "        self.transformer_head = ConvFreeTransformerHead(\n",
    "            in_channels=self.final_feature_map_channels,\n",
    "            feature_map_size_dhw=self.final_feature_map_dhw,\n",
    "            patch_size_dhw=config[\"clf_transformer_head_patch_size\"],\n",
    "            embed_dim=config[\"clf_transformer_head_embed_dim\"],\n",
    "            depth=config[\"clf_transformer_head_depth\"],\n",
    "            num_heads=config[\"clf_transformer_head_num_heads\"],\n",
    "            num_classes=config[\"clf_num_classes\"]\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm3d, nn.LayerNorm)):\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        for i, stage_module_list in enumerate(self.backbone_stages):\n",
    "            for layer in stage_module_list: # Iterate through DenseResLayers in the stage\n",
    "                x = layer(x)\n",
    "            \n",
    "            x = self.attention_stages[i](x) # Apply attention (or Identity)\n",
    "            \n",
    "            if i < len(self.downsample_layers):\n",
    "                x = self.downsample_layers[i](x)\n",
    "        \n",
    "        # x is now the final feature map from the CNN backbone\n",
    "        logits = self.transformer_head(x)\n",
    "        return logits\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Training Setup & Loop (Refined)\n",
    "\n",
    "# %%\n",
    "# --- Loss Functions (FocalLoss, CombinedClfLoss - Assume defined as before) ---\n",
    "class FocalLoss(nn.Module): # Copied\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__(); self.alpha=alpha; self.gamma=gamma; self.reduction=reduction\n",
    "    def forward(self, inputs_logits, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs_logits, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss); F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        if self.reduction == 'mean': return torch.mean(F_loss)\n",
    "        return F_loss # Or sum\n",
    "\n",
    "# --- MixUp (Assume defined as before) ---\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    if alpha > 0: lam = np.random.beta(alpha, alpha)\n",
    "    else: lam = 1.0\n",
    "    batch_size = x.size(0); index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1-lam) * x[index, :]; y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# --- Training/Validation Epoch Functions (train_classifier_epoch_advanced, validate_classifier_epoch - Reuse/Adapt) ---\n",
    "# Make sure train_classifier_epoch_advanced handles mixup as shown previously.\n",
    "# For brevity, not repeating them here but ensure they are defined using CONFIG.\n",
    "\n",
    "def train_classifier_epoch_refined(model, dataloader, optimizer, loss_fn, scaler, device, epoch_num, config):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    if num_batches == 0: return 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"RefClf Train E{epoch_num+1}\", leave=False)\n",
    "    for cubes, labels, _ in progress_bar:\n",
    "        cubes, labels = cubes.to(device), labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        use_mixup = config.get(\"aug_mixup_alpha\", 0) > 0 and random.random() < 0.5\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            if use_mixup:\n",
    "                mixed_cubes, labels_a, labels_b, lam = mixup_data(cubes, labels, config[\"aug_mixup_alpha\"], device)\n",
    "                predictions_logits = model(mixed_cubes)\n",
    "                loss = mixup_criterion(loss_fn, predictions_logits, labels_a, labels_b, lam)\n",
    "            else:\n",
    "                predictions_logits = model(cubes)\n",
    "                loss = loss_fn(predictions_logits, labels)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Invalid loss: {loss.item()}. Skipping batch.\"); continue\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "\n",
    "def validate_classifier_epoch_refined(model, dataloader, loss_fn, device): # Can reuse prev validate\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0; all_preds_probs, all_labels = [], []\n",
    "    num_batches = len(dataloader)\n",
    "    if num_batches == 0: return 0.0, 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"RefClf Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for cubes, labels, _ in progress_bar:\n",
    "            cubes, labels = cubes.to(device), labels.to(device).unsqueeze(1)\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                predictions_logits = model(cubes)\n",
    "                loss = loss_fn(predictions_logits, labels)\n",
    "            if torch.isnan(loss) or torch.isinf(loss): print(f\"Invalid val_loss: {loss.item()}.\"); continue\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds_probs.extend(torch.sigmoid(predictions_logits).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    all_labels_np = np.array(all_labels).flatten()\n",
    "    all_preds_probs_np = np.array(all_preds_probs).flatten()\n",
    "    val_auc = 0.0\n",
    "    if len(np.unique(all_labels_np)) > 1 and len(all_labels_np) > 0:\n",
    "        val_auc = roc_auc_score(all_labels_np, all_preds_probs_np)\n",
    "    return epoch_loss / num_batches, val_auc\n",
    "\n",
    "\n",
    "# --- Initialize Refined Classifier Model ---\n",
    "ref_clf_model = AdvancedHybridNetRefined(CONFIG).to(DEVICE)\n",
    "print(f\"Refined Classifier Model Instantiated. Params: {sum(p.numel() for p in ref_clf_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "ref_clf_optimizer = optim.AdamW(ref_clf_model.parameters(), lr=CONFIG[\"clf_lr_initial\"], weight_decay=CONFIG[\"clf_weight_decay\"])\n",
    "ref_clf_loss_fn = FocalLoss(alpha=CONFIG[\"clf_focal_loss_alpha\"], gamma=CONFIG[\"clf_focal_loss_gamma\"])\n",
    "ref_clf_scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "ref_clf_lr_scheduler = CosineAnnealingLR(ref_clf_optimizer, T_max=CONFIG[\"clf_cosine_lr_t_max\"])\n",
    "\n",
    "best_val_auc_ref_clf = -1.0\n",
    "epochs_no_improve_ref_clf = 0\n",
    "\n",
    "# --- Training Loop (Refined) ---\n",
    "print(f\"\\n--- Training Refined Classifier Model ({CONFIG['clf_model_name']}) ---\")\n",
    "for epoch in range(CONFIG[\"clf_epochs\"]):\n",
    "    # HNM Logic (Simplified placeholder - use full HNM scoring logic as needed)\n",
    "    current_negatives_for_epoch_ref = []\n",
    "    if epoch < CONFIG[\"hnm_start_epoch\"] or not dummy_neg_train_pool_ref:\n",
    "        num_to_sample = min(len(dummy_neg_train_pool_ref), len(dummy_pos_train_ref) * CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "        current_negatives_for_epoch_ref = random.sample(dummy_neg_train_pool_ref, num_to_sample) if dummy_neg_train_pool_ref else []\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}: HNM Placeholder - using random subset of negatives.\") # Replace with actual HNM\n",
    "        num_hard_neg = min(len(dummy_neg_train_pool_ref), len(dummy_pos_train_ref) * CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "        current_negatives_for_epoch_ref = random.sample(dummy_neg_train_pool_ref, num_hard_neg) if dummy_neg_train_pool_ref else []\n",
    "\n",
    "\n",
    "    ref_clf_train_dataset = RefinedClassifierDataset(dummy_pos_train_ref, current_negatives_for_epoch_ref, epoch, CONFIG, is_train=True)\n",
    "    ref_clf_train_loader = DataLoader(ref_clf_train_dataset, batch_size=CONFIG[\"clf_batch_size\"], shuffle=True, num_workers=0)\n",
    "\n",
    "    num_val_neg_to_sample_ref = min(len(dummy_neg_val_pool_ref), len(dummy_pos_val_ref) * CONFIG[\"hnm_ratio_neg_to_pos\"])\n",
    "    val_neg_samples_ref = random.sample(dummy_neg_val_pool_ref, num_val_neg_to_sample_ref) if dummy_neg_val_pool_ref else []\n",
    "    ref_clf_val_dataset = RefinedClassifierDataset(dummy_pos_val_ref, val_neg_samples_ref, epoch, CONFIG, is_train=False)\n",
    "    ref_clf_val_loader = DataLoader(ref_clf_val_dataset, batch_size=CONFIG[\"clf_batch_size\"], shuffle=False, num_workers=0)\n",
    "\n",
    "    if len(ref_clf_train_loader.dataset) == 0: print(f\"Skipping train epoch {epoch+1}, empty train_loader.\"); ref_clf_lr_scheduler.step(); continue\n",
    "    \n",
    "    train_loss_ref_clf = train_classifier_epoch_refined(ref_clf_model, ref_clf_train_loader, ref_clf_optimizer, ref_clf_loss_fn, ref_clf_scaler, DEVICE, epoch, CONFIG)\n",
    "    \n",
    "    val_loss_ref_clf, val_auc_ref_clf = 0.0, 0.0\n",
    "    if len(ref_clf_val_loader.dataset) > 0:\n",
    "        val_loss_ref_clf, val_auc_ref_clf = validate_classifier_epoch_refined(ref_clf_model, ref_clf_val_loader, ref_clf_loss_fn, DEVICE)\n",
    "    else: print(f\"Skipping val epoch {epoch+1}, empty val_loader.\")\n",
    "\n",
    "    ref_clf_lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['clf_epochs']}: RefClf Train Loss: {train_loss_ref_clf:.4f}, RefClf Val Loss: {val_loss_ref_clf:.4f}, RefClf Val AUC: {val_auc_ref_clf:.4f}\")\n",
    "\n",
    "    if val_auc_ref_clf > best_val_auc_ref_clf and not (np.isinf(val_loss_ref_clf) or np.isnan(val_loss_ref_clf)):\n",
    "        best_val_auc_ref_clf = val_auc_ref_clf\n",
    "        save_path = CONFIG[\"output_dir\"] / \"classification_models_refined\" / f\"{CONFIG['clf_model_name']}_best.pth\"\n",
    "        torch.save(ref_clf_model.state_dict(), save_path)\n",
    "        print(f\"  Saved best refined classifier model to {save_path} (Val AUC: {best_val_auc_ref_clf:.4f})\")\n",
    "        epochs_no_improve_ref_clf = 0\n",
    "    else:\n",
    "        epochs_no_improve_ref_clf += 1\n",
    "\n",
    "    if epochs_no_improve_ref_clf >= CONFIG[\"clf_early_stopping_patience\"]:\n",
    "        print(f\"Early stopping for refined classifier at epoch {epoch+1}.\")\n",
    "        break\n",
    "print(\"Refined Classifier training finished.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Evaluation, Visualization, Reporting (Adapt as before)\n",
    "\n",
    "# %%\n",
    "# --- Final Evaluation (using ref_clf_model) ---\n",
    "# ... Load best ref_clf_model ...\n",
    "# ... Prepare test RefinedClassifierDataset (using dummy_pos_val_ref etc. for now) ...\n",
    "# ... Run validate_classifier_epoch_refined on test data ...\n",
    "# ... Calculate final metrics ...\n",
    "\n",
    "print(\"\\n--- Evaluation, Visualization, Reporting (Placeholders) ---\")\n",
    "# This part would reuse logic from previous notebook sections, adapting model names and paths.\n",
    "# Key tasks:\n",
    "# 1. Load the best `ref_clf_model`.\n",
    "# 2. Create a test `RefinedClassifierDataset` and `DataLoader`.\n",
    "# 3. Run `validate_classifier_epoch_refined` to get test loss and AUC.\n",
    "# 4. Perform full metric calculation (Precision, Recall, F1, ROC, Sens@Spec).\n",
    "# 5. Generate visualizations (3D overlay, GradCAM for `ref_clf_model`).\n",
    "# 6. Write the final report detailing `AdvancedHybridNetRefined` performance.\n",
    "\n",
    "# Example Test Evaluation Snippet (using dummy val data as test for now)\n",
    "print(\"\\nEvaluating Refined Model on 'Test' Set (using dummy validation data)...\")\n",
    "best_model_path_ref = CONFIG[\"output_dir\"] / \"classification_models_refined\" / f\"{CONFIG['clf_model_name']}_best.pth\"\n",
    "if best_model_path_ref.exists():\n",
    "    eval_model_ref = AdvancedHybridNetRefined(CONFIG).to(DEVICE)\n",
    "    eval_model_ref.load_state_dict(torch.load(best_model_path_ref, map_location=DEVICE))\n",
    "    print(f\"Loaded best refined model from {best_model_path_ref}\")\n",
    "    \n",
    "    # Using val_neg_samples_ref and dummy_pos_val_ref as a proxy for test data\n",
    "    ref_clf_test_dataset = RefinedClassifierDataset(dummy_pos_val_ref, val_neg_samples_ref, CONFIG['clf_epochs'], CONFIG, is_train=False)\n",
    "    if len(ref_clf_test_dataset) > 0:\n",
    "        ref_clf_test_loader = DataLoader(ref_clf_test_dataset, batch_size=CONFIG[\"clf_batch_size\"])\n",
    "        test_loss_ref, test_auc_ref = validate_classifier_epoch_refined(eval_model_ref, ref_clf_test_loader, ref_clf_loss_fn, DEVICE)\n",
    "        print(f\"Refined Model 'Test' Loss: {test_loss_ref:.4f}, 'Test' AUC: {test_auc_ref:.4f}\")\n",
    "    else:\n",
    "        print(\"Refined Model 'Test' set is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    print(\"Best refined model not found for evaluation.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
