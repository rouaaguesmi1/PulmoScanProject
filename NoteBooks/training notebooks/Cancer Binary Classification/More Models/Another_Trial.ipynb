{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8a88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Loading Data and Selecting EVEN Number of Scans Per Class (up to 50 each) ---\n",
      "Available Cancerous: 27, Non-Cancerous: 71\n",
      "Selecting 27 from each class.\n",
      "Total scans selected: 54\n",
      "\n",
      "Preprocessing for 54 scans (if not already done)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 54/54 [03:08<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing finished/checked in 188.68 seconds.\n",
      "Final patient count for training/validation: 54\n",
      "Swin3D Model Instantiated. Number of parameters: 2437849\n",
      "\n",
      "Swin3D Model output shape: torch.Size([2, 1])\n",
      "Calculated positive weight for BCEWithLogitsLoss (Swin3D): 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rouaa\\AppData\\Local\\Temp\\ipykernel_23708\\3822446021.py:658: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Swin3D Model (Balanced Data) for 50 epochs...\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary: Duration: 5.11s\n",
      "  Train Loss: 0.8260, Train Acc: 0.4186\n",
      "  Val Loss: 0.6914, Val Acc: 0.4545\n",
      "  Best model saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary: Duration: 3.07s\n",
      "  Train Loss: 0.7017, Train Acc: 0.3953\n",
      "  Val Loss: 0.6913, Val Acc: 0.4545\n",
      "  Best model saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Summary: Duration: 3.19s\n",
      "  Train Loss: 0.6874, Train Acc: 0.4419\n",
      "  Val Loss: 0.6841, Val Acc: 0.4545\n",
      "  Best model saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Summary: Duration: 2.94s\n",
      "  Train Loss: 0.7270, Train Acc: 0.5116\n",
      "  Val Loss: 0.6803, Val Acc: 0.4545\n",
      "  Best model saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Summary: Duration: 3.09s\n",
      "  Train Loss: 0.7026, Train Acc: 0.5116\n",
      "  Val Loss: 0.6797, Val Acc: 0.5455\n",
      "  Best model saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Summary: Duration: 3.17s\n",
      "  Train Loss: 0.7067, Train Acc: 0.6047\n",
      "  Val Loss: 0.6792, Val Acc: 0.5455\n",
      "  Best model saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Summary: Duration: 3.05s\n",
      "  Train Loss: 0.6818, Train Acc: 0.4884\n",
      "  Val Loss: 0.6809, Val Acc: 0.5455\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Summary: Duration: 2.96s\n",
      "  Train Loss: 0.6926, Train Acc: 0.4651\n",
      "  Val Loss: 0.6845, Val Acc: 0.3636\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Summary: Duration: 2.94s\n",
      "  Train Loss: 0.6766, Train Acc: 0.4884\n",
      "  Val Loss: 0.6796, Val Acc: 0.5455\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Summary: Duration: 2.96s\n",
      "  Train Loss: 0.6769, Train Acc: 0.5116\n",
      "  Val Loss: 0.6989, Val Acc: 0.4545\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Summary: Duration: 2.90s\n",
      "  Train Loss: 0.6690, Train Acc: 0.6279\n",
      "  Val Loss: 0.6947, Val Acc: 0.4545\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Summary: Duration: 2.97s\n",
      "  Train Loss: 0.6472, Train Acc: 0.7442\n",
      "  Val Loss: 0.7088, Val Acc: 0.1818\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Summary: Duration: 2.92s\n",
      "  Train Loss: 0.6339, Train Acc: 0.6279\n",
      "  Val Loss: 0.7324, Val Acc: 0.1818\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Summary: Duration: 2.94s\n",
      "  Train Loss: 0.4974, Train Acc: 0.7907\n",
      "  Val Loss: 1.0938, Val Acc: 0.4545\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Summary: Duration: 3.03s\n",
      "  Train Loss: 0.2839, Train Acc: 0.9070\n",
      "  Val Loss: 1.6706, Val Acc: 0.1818\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Summary: Duration: 3.01s\n",
      "  Train Loss: 0.2110, Train Acc: 0.8605\n",
      "  Val Loss: 1.5032, Val Acc: 0.2727\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Summary: Duration: 2.92s\n",
      "  Train Loss: 0.1434, Train Acc: 0.9302\n",
      "  Val Loss: 1.7229, Val Acc: 0.3636\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Summary: Duration: 2.97s\n",
      "  Train Loss: 0.1983, Train Acc: 0.9535\n",
      "  Val Loss: 2.1522, Val Acc: 0.0909\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Summary: Duration: 3.59s\n",
      "  Train Loss: 0.1254, Train Acc: 0.9302\n",
      "  Val Loss: 1.8719, Val Acc: 0.2727\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Summary: Duration: 4.57s\n",
      "  Train Loss: 0.0274, Train Acc: 1.0000\n",
      "  Val Loss: 2.0059, Val Acc: 0.3636\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Summary: Duration: 4.43s\n",
      "  Train Loss: 0.0367, Train Acc: 0.9767\n",
      "  Val Loss: 2.0584, Val Acc: 0.2727\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Summary: Duration: 3.64s\n",
      "  Train Loss: 0.0205, Train Acc: 1.0000\n",
      "  Val Loss: 2.0950, Val Acc: 0.2727\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Summary: Duration: 2.97s\n",
      "  Train Loss: 0.0528, Train Acc: 0.9767\n",
      "  Val Loss: 2.1016, Val Acc: 0.3636\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Summary: Duration: 2.87s\n",
      "  Train Loss: 0.0115, Train Acc: 1.0000\n",
      "  Val Loss: 1.9019, Val Acc: 0.3636\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Summary: Duration: 2.80s\n",
      "  Train Loss: 0.0343, Train Acc: 0.9767\n",
      "  Val Loss: 1.9967, Val Acc: 0.1818\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Summary: Duration: 2.82s\n",
      "  Train Loss: 0.0218, Train Acc: 1.0000\n",
      "  Val Loss: 2.1529, Val Acc: 0.2727\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Summary: Duration: 2.76s\n",
      "  Train Loss: 0.0162, Train Acc: 1.0000\n",
      "  Val Loss: 2.2026, Val Acc: 0.1818\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Summary: Duration: 2.84s\n",
      "  Train Loss: 0.0216, Train Acc: 1.0000\n",
      "  Val Loss: 2.1876, Val Acc: 0.3636\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Summary: Duration: 2.80s\n",
      "  Train Loss: 0.0267, Train Acc: 0.9767\n",
      "  Val Loss: 1.9990, Val Acc: 0.5455\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Summary: Duration: 2.78s\n",
      "  Train Loss: 0.0246, Train Acc: 1.0000\n",
      "  Val Loss: 1.9189, Val Acc: 0.4545\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Summary: Duration: 2.74s\n",
      "  Train Loss: 0.0063, Train Acc: 1.0000\n",
      "  Val Loss: 1.8903, Val Acc: 0.4545\n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Summary: Duration: 2.80s\n",
      "  Train Loss: 0.0121, Train Acc: 1.0000\n",
      "  Val Loss: 2.0291, Val Acc: 0.4545\n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Summary: Duration: 2.76s\n",
      "  Train Loss: 0.0062, Train Acc: 1.0000\n",
      "  Val Loss: 2.0723, Val Acc: 0.4545\n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Summary: Duration: 2.81s\n",
      "  Train Loss: 0.0163, Train Acc: 1.0000\n",
      "  Val Loss: 2.1892, Val Acc: 0.5455\n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Summary: Duration: 2.72s\n",
      "  Train Loss: 0.0051, Train Acc: 1.0000\n",
      "  Val Loss: 2.3220, Val Acc: 0.4545\n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Summary: Duration: 2.78s\n",
      "  Train Loss: 0.0079, Train Acc: 1.0000\n",
      "  Val Loss: 2.2067, Val Acc: 0.4545\n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Summary: Duration: 2.82s\n",
      "  Train Loss: 0.0046, Train Acc: 1.0000\n",
      "  Val Loss: 2.2117, Val Acc: 0.4545\n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Summary: Duration: 2.75s\n",
      "  Train Loss: 0.0037, Train Acc: 1.0000\n",
      "  Val Loss: 2.2218, Val Acc: 0.4545\n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Summary: Duration: 2.76s\n",
      "  Train Loss: 0.0050, Train Acc: 1.0000\n",
      "  Val Loss: 2.2510, Val Acc: 0.4545\n",
      "\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Summary: Duration: 2.75s\n",
      "  Train Loss: 0.0048, Train Acc: 1.0000\n",
      "  Val Loss: 2.2675, Val Acc: 0.4545\n",
      "\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Summary: Duration: 2.79s\n",
      "  Train Loss: 0.0052, Train Acc: 1.0000\n",
      "  Val Loss: 2.2475, Val Acc: 0.4545\n",
      "\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Summary: Duration: 2.78s\n",
      "  Train Loss: 0.0041, Train Acc: 1.0000\n",
      "  Val Loss: 2.2481, Val Acc: 0.4545\n",
      "\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Summary: Duration: 2.79s\n",
      "  Train Loss: 0.0127, Train Acc: 1.0000\n",
      "  Val Loss: 2.3977, Val Acc: 0.4545\n",
      "\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Summary: Duration: 2.75s\n",
      "  Train Loss: 0.0251, Train Acc: 0.9767\n",
      "  Val Loss: 2.3607, Val Acc: 0.2727\n",
      "\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Summary: Duration: 2.73s\n",
      "  Train Loss: 0.0040, Train Acc: 1.0000\n",
      "  Val Loss: 2.3738, Val Acc: 0.3636\n",
      "\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Summary: Duration: 2.81s\n",
      "  Train Loss: 0.0042, Train Acc: 1.0000\n",
      "  Val Loss: 2.3702, Val Acc: 0.2727\n",
      "\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Summary: Duration: 2.72s\n",
      "  Train Loss: 0.0338, Train Acc: 0.9767\n",
      "  Val Loss: 2.3677, Val Acc: 0.3636\n",
      "\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Summary: Duration: 2.79s\n",
      "  Train Loss: 0.0095, Train Acc: 1.0000\n",
      "  Val Loss: 2.0857, Val Acc: 0.3636\n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Summary: Duration: 2.74s\n",
      "  Train Loss: 0.0049, Train Acc: 1.0000\n",
      "  Val Loss: 2.2089, Val Acc: 0.3636\n",
      "\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Summary: Duration: 2.78s\n",
      "  Train Loss: 0.0036, Train Acc: 1.0000\n",
      "  Val Loss: 2.2947, Val Acc: 0.4545\n",
      "\n",
      "Swin3D Model (Balanced Data) Training Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rouaa\\AppData\\Local\\Temp\\ipykernel_23708\\3822446021.py:741: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training curves saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\training_curves_swin3d_corrected_balanced.png\n",
      "\n",
      "Evaluating Swin3D Model (Balanced Data) on Validation Set...\n",
      "Loaded best Swin3D model from C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\swin3d_model_corrected_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Loss (Swin3D Corrected, Balanced): 0.6792\n",
      "\n",
      "--- Final Validation Metrics (Swin3D Corrected, Balanced) ---\n",
      "Accuracy:  0.5455\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1-Score:  0.0000\n",
      "AUC-ROC:   0.1667\n",
      "\n",
      "Classification Report (Swin3D Corrected, Balanced):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Non-Cancer (0)       0.55      1.00      0.71         6\n",
      "    Cancer (1)       0.00      0.00      0.00         5\n",
      "\n",
      "      accuracy                           0.55        11\n",
      "     macro avg       0.27      0.50      0.35        11\n",
      "  weighted avg       0.30      0.55      0.39        11\n",
      "\n",
      "\n",
      "Confusion Matrix (Swin3D Corrected, Balanced):\n",
      "Confusion matrix plot saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\confusion_matrix_swin3d_corrected_balanced.png\n",
      "ROC curve plot saved to C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\\roc_curve_swin3d_corrected_balanced.png\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import scipy.ndimage\n",
    "from skimage.measure import label as skimage_label, regionprops\n",
    "from skimage.morphology import disk, binary_closing\n",
    "from skimage.segmentation import clear_border\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # For pad\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve, auc, f1_score,\n",
    "                             precision_score, recall_score, accuracy_score, ConfusionMatrixDisplay)\n",
    "# from einops import rearrange, repeat # Not strictly needed for this version\n",
    "\n",
    "# Configuration\n",
    "# --- MODIFY THESE PATHS ---\n",
    "DSB_PATH = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Stages\"\n",
    "DSB_LABELS_CSV = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\stage1_labels.csv\"\n",
    "# --- MODIFIED: New output path for Swin3D model ---\n",
    "PREPROCESSED_DSB_PATH = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Preprocessed_Data_DSB_Swin3D_Corrected\" # Added _Corrected\n",
    "# ---\n",
    "\n",
    "# Preprocessing & Model Params (mostly unchanged)\n",
    "TARGET_SPACING = [1.5, 1.5, 1.5]\n",
    "FINAL_SCAN_SIZE = (96, 128, 128) # (Depth, Height, Width)\n",
    "CLIP_BOUND_HU = [-1000.0, 400.0]\n",
    "PIXEL_MEAN = 0.25\n",
    "\n",
    "# Training Params\n",
    "NUM_CLASSES = 1\n",
    "BATCH_SIZE = 2 # Swin Transformers can be memory intensive\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "SCAN_LIMIT_PER_CLASS = 50 # Max scans per class if available\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PREPROCESSED_DSB_PATH, exist_ok=True)\n",
    "\n",
    "# Random seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Data Loading and Selection (BALANCED, Identical to previous script) ---\n",
    "print(f\"--- Loading Data and Selecting EVEN Number of Scans Per Class (up to {SCAN_LIMIT_PER_CLASS} each) ---\")\n",
    "if not os.path.isdir(DSB_PATH): raise SystemExit(f\"ERROR: DSB Scans path not found: {DSB_PATH}\")\n",
    "if not os.path.isfile(DSB_LABELS_CSV): raise SystemExit(f\"ERROR: DSB Labels CSV not found: {DSB_LABELS_CSV}\")\n",
    "dsb_labels_df = pd.read_csv(DSB_LABELS_CSV)\n",
    "dsb_labels_df = dsb_labels_df.rename(columns={'id': 'patient_id'})\n",
    "patient_labels_all = dsb_labels_df.set_index('patient_id')['cancer'].to_dict()\n",
    "scan_folders = [f for f in os.listdir(DSB_PATH) if os.path.isdir(os.path.join(DSB_PATH, f))]\n",
    "found_scan_ids = set(scan_folders)\n",
    "labeled_patient_ids_all = set(dsb_labels_df['patient_id'])\n",
    "common_ids_all = labeled_patient_ids_all.intersection(found_scan_ids)\n",
    "common_ids_cancer_available = [pid for pid in common_ids_all if patient_labels_all.get(pid) == 1]\n",
    "common_ids_non_cancer_available = [pid for pid in common_ids_all if patient_labels_all.get(pid) == 0]\n",
    "random.shuffle(common_ids_cancer_available)\n",
    "random.shuffle(common_ids_non_cancer_available)\n",
    "num_to_select_per_class = min(len(common_ids_cancer_available), len(common_ids_non_cancer_available), SCAN_LIMIT_PER_CLASS)\n",
    "print(f\"Available Cancerous: {len(common_ids_cancer_available)}, Non-Cancerous: {len(common_ids_non_cancer_available)}\")\n",
    "print(f\"Selecting {num_to_select_per_class} from each class.\")\n",
    "selected_cancer_ids = common_ids_cancer_available[:num_to_select_per_class]\n",
    "selected_non_cancer_ids = common_ids_non_cancer_available[:num_to_select_per_class]\n",
    "scans_to_process = selected_cancer_ids + selected_non_cancer_ids\n",
    "random.shuffle(scans_to_process)\n",
    "print(f\"Total scans selected: {len(scans_to_process)}\")\n",
    "patient_labels = {pid: patient_labels_all[pid] for pid in scans_to_process}\n",
    "\n",
    "# --- Preprocessing Functions (Identical) ---\n",
    "def load_scan_series(dicom_folder_path):\n",
    "    try:\n",
    "        series_ids = sitk.ImageSeriesReader.GetGDCMSeriesIDs(dicom_folder_path)\n",
    "        if not series_ids: return None, None, None\n",
    "        series_file_names = sitk.ImageSeriesReader.GetGDCMSeriesFileNames(dicom_folder_path, series_ids[0])\n",
    "        series_reader = sitk.ImageSeriesReader(); series_reader.SetFileNames(series_file_names)\n",
    "        itkimage = series_reader.Execute()\n",
    "        image_array = sitk.GetArrayFromImage(itkimage); origin = np.array(list(reversed(itkimage.GetOrigin()))); spacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "        return image_array, origin, spacing\n",
    "    except Exception as e: print(f\"Error reading DICOM {os.path.basename(dicom_folder_path)}: {e}\"); return None, None, None\n",
    "\n",
    "def resample(image, original_spacing, new_spacing=TARGET_SPACING):\n",
    "    try:\n",
    "        resize_factor = np.array(original_spacing) / np.array(new_spacing)\n",
    "        new_real_shape = image.shape * resize_factor; new_shape = np.round(new_real_shape)\n",
    "        real_resize_factor = new_shape / image.shape; actual_new_spacing = original_spacing / real_resize_factor\n",
    "        resampled_image = scipy.ndimage.zoom(image, real_resize_factor, mode='nearest', order=1)\n",
    "        return resampled_image, actual_new_spacing\n",
    "    except Exception as e: print(f\"Error resamping: {e}\"); return None, None\n",
    "\n",
    "def get_segmented_lungs(im_slice, hu_threshold=-320):\n",
    "    if im_slice.ndim != 2: return im_slice\n",
    "    binary = im_slice < hu_threshold; cleared = clear_border(binary)\n",
    "    label_image = skimage_label(cleared); areas = [r.area for r in regionprops(label_image)]; areas.sort()\n",
    "    area_threshold = areas[-2] if len(areas) >= 2 else (areas[-1] if len(areas) == 1 else 0)\n",
    "    if area_threshold > 0:\n",
    "        for region in regionprops(label_image):\n",
    "            if region.area < area_threshold:\n",
    "                for coordinates in region.coords: label_image[coordinates[0], coordinates[1]] = 0\n",
    "    binary = label_image > 0; selem = disk(2); binary = binary_closing(binary, selem)\n",
    "    selem_dilate = disk(5); final_mask = ndi.binary_dilation(binary, structure=selem_dilate)\n",
    "    background_val = CLIP_BOUND_HU[0] - 1; segmented_slice = im_slice.copy()\n",
    "    segmented_slice[final_mask == 0] = background_val\n",
    "    return segmented_slice\n",
    "\n",
    "def normalize_hu(image, clip_bounds=CLIP_BOUND_HU):\n",
    "    min_bound, max_bound = clip_bounds; image = np.clip(image, min_bound, max_bound)\n",
    "    image = (image - min_bound) / (max_bound - min_bound)\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def zero_center(image, pixel_mean=PIXEL_MEAN):\n",
    "    image = image - pixel_mean\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def resize_scan_to_target(image, target_shape=FINAL_SCAN_SIZE):\n",
    "    if image.shape == target_shape: return image\n",
    "    resize_factor = np.array(target_shape) / np.array(image.shape)\n",
    "    try:\n",
    "        resized_image = scipy.ndimage.zoom(image, resize_factor, order=1, mode='nearest')\n",
    "        if resized_image.shape != target_shape:\n",
    "            current_shape = resized_image.shape; diff = np.array(target_shape) - np.array(current_shape)\n",
    "            pad = np.maximum(diff, 0); crop = np.maximum(-diff, 0)\n",
    "            pad_width = tuple((p // 2, p - p // 2) for p in pad)\n",
    "            resized_image = np.pad(resized_image, pad_width, mode='edge')\n",
    "            crop_slice = tuple(slice(c // 2, s - (c - c // 2)) for c, s in zip(crop, resized_image.shape))\n",
    "            resized_image = resized_image[crop_slice]\n",
    "        if resized_image.shape != target_shape: print(f\"ERROR: Resize failed. Shape {resized_image.shape} vs Target {target_shape}\"); return None\n",
    "        return resized_image.astype(np.float32)\n",
    "    except Exception as e: print(f\"Error resizing to target: {e}\"); return None\n",
    "\n",
    "def preprocess_scan_dsb(patient_id, input_base_path, output_base_path, force_preprocess=False):\n",
    "    scan_folder_path = os.path.join(input_base_path, patient_id)\n",
    "    output_filename = os.path.join(output_base_path, f\"{patient_id}.npz\")\n",
    "    if os.path.exists(output_filename) and not force_preprocess: return True\n",
    "    image, origin, spacing = load_scan_series(scan_folder_path);\n",
    "    if image is None: return False\n",
    "    resampled_image, new_spacing = resample(image, spacing, TARGET_SPACING)\n",
    "    if resampled_image is None: del image; return False;\n",
    "    del image; segmented_lungs = np.zeros_like(resampled_image, dtype=np.float32)\n",
    "    for i in range(resampled_image.shape[0]): segmented_lungs[i] = get_segmented_lungs(resampled_image[i])\n",
    "    del resampled_image; normalized_image = normalize_hu(segmented_lungs, clip_bounds=CLIP_BOUND_HU); del segmented_lungs;\n",
    "    centered_image = zero_center(normalized_image, pixel_mean=PIXEL_MEAN); del normalized_image;\n",
    "    final_image = resize_scan_to_target(centered_image, target_shape=FINAL_SCAN_SIZE); del centered_image;\n",
    "    if final_image is None: return False\n",
    "    try: np.savez_compressed(output_filename, image=final_image.astype(np.float32)); return True\n",
    "    except Exception as e: print(f\"Error saving {patient_id}: {e}\"); return False\n",
    "\n",
    "#--- Preprocessing Execution ---\n",
    "successful_processed_ids = []\n",
    "print(f\"\\nPreprocessing for {len(scans_to_process)} scans (if not already done)...\")\n",
    "start_time = time.time()\n",
    "for patient_id in tqdm(scans_to_process, desc=f\"Preprocessing\"):\n",
    "    if preprocess_scan_dsb(patient_id, DSB_PATH, PREPROCESSED_DSB_PATH, force_preprocess=False):\n",
    "        successful_processed_ids.append(patient_id)\n",
    "end_time = time.time()\n",
    "print(f\"\\nPreprocessing finished/checked in {end_time - start_time:.2f} seconds.\")\n",
    "final_patient_list = successful_processed_ids\n",
    "if not final_patient_list: raise SystemExit(\"No scans processed. Cannot continue.\")\n",
    "patient_labels = {pid: patient_labels[pid] for pid in final_patient_list}\n",
    "print(f\"Final patient count for training/validation: {len(final_patient_list)}\")\n",
    "\n",
    "# --- Dataset and DataLoader (Identical) ---\n",
    "class PatientLevelDataset(Dataset):\n",
    "    def __init__(self, patient_ids, labels_dict, preprocessed_path):\n",
    "        self.patient_ids = patient_ids; self.labels_dict = labels_dict; self.preprocessed_path = preprocessed_path\n",
    "    def __len__(self): return len(self.patient_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]; label = self.labels_dict[patient_id]\n",
    "        scan_path = os.path.join(self.preprocessed_path, f\"{patient_id}.npz\")\n",
    "        try:\n",
    "            with np.load(scan_path) as npz_data: image = npz_data['image']\n",
    "            image_tensor = torch.from_numpy(image).float().unsqueeze(0)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "            return image_tensor, label_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading {patient_id}: {e}\"); dummy = torch.zeros((1, *FINAL_SCAN_SIZE), dtype=torch.float32)\n",
    "            return dummy, torch.tensor(-1, dtype=torch.float32)\n",
    "\n",
    "train_ids, val_ids = train_test_split(final_patient_list, test_size=0.2, random_state=SEED,\n",
    "                                      stratify=[patient_labels[pid] for pid in final_patient_list])\n",
    "train_dataset = PatientLevelDataset(train_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "val_dataset = PatientLevelDataset(val_ids, patient_labels, PREPROCESSED_DSB_PATH)\n",
    "NUM_WORKERS = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# --- 3D Swin Transformer Model (Corrected) ---\n",
    "\n",
    "def window_partition_3d(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, D, H, W, C)\n",
    "        window_size (tuple[int]): window size (Depth, Height, Width)\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size_d, window_size_h, window_size_w, C)\n",
    "    \"\"\"\n",
    "    B, D, H, W, C = x.shape\n",
    "    wd, wh, ww = window_size\n",
    "    # Ensure D, H, W are divisible by window_size dimensions for this simplified partition\n",
    "    assert D % wd == 0 and H % wh == 0 and W % ww == 0, \\\n",
    "        f\"Input shape ({D},{H},{W}) not divisible by window_size ({wd},{wh},{ww})\"\n",
    "    x = x.view(B, D // wd, wd, H // wh, wh, W // ww, ww, C)\n",
    "    windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, wd, wh, ww, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse_3d(windows, window_size, D, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size_d, window_size_h, window_size_w, C)\n",
    "        window_size (tuple[int]): Window size (Depth, Height, Width)\n",
    "        D, H, W: Original image dimensions (before windowing)\n",
    "    Returns:\n",
    "        x: (B, D, H, W, C)\n",
    "    \"\"\"\n",
    "    wd, wh, ww = window_size\n",
    "    # Calculate B based on the total number of elements and known dimensions\n",
    "    num_windows_per_sample = (D // wd) * (H // wh) * (W // ww)\n",
    "    if num_windows_per_sample == 0 : # Should not happen if assertions in partition hold\n",
    "        B = 0\n",
    "    else:\n",
    "        B = int(windows.shape[0] / num_windows_per_sample)\n",
    "\n",
    "    x = windows.view(B, D // wd, H // wh, W // ww, wd, wh, ww, -1)\n",
    "    x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(B, D, H, W, -1)\n",
    "    return x\n",
    "\n",
    "class WindowAttention3D(nn.Module):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wd, Wh, Ww (tuple)\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1), num_heads))\n",
    "\n",
    "        coords_d = torch.arange(self.window_size[0])\n",
    "        coords_h = torch.arange(self.window_size[1])\n",
    "        coords_w = torch.arange(self.window_size[2])\n",
    "        coords = torch.stack(torch.meshgrid([coords_d, coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "        relative_coords[:, :, 1] *= (2 * self.window_size[2] - 1)\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index, persistent=False)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            N, N, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock3D(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=(4,4,4), shift_size=(0,0,0),\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution # (Depth, Height, Width)\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size if isinstance(window_size, tuple) else (window_size,) * 3\n",
    "        self.shift_size = shift_size if isinstance(shift_size, tuple) else (shift_size,) * 3\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # Ensure input_resolution is divisible by window_size for this block\n",
    "        for i in range(3):\n",
    "            assert input_resolution[i] % self.window_size[i] == 0, \\\n",
    "                f\"Input dim {input_resolution[i]} not divisible by window_size {self.window_size[i]} at dim {i}\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention3D(\n",
    "            dim, window_size=self.window_size, num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = nn.Identity() if drop_path == 0. else DropPath(drop_path)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim), act_layer(), nn.Dropout(drop), # Added dropout in MLP\n",
    "            nn.Linear(mlp_hidden_dim, dim), nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "        if any(s > 0 for s in self.shift_size):\n",
    "            D, H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, D, H, W, 1)) # Create on CPU first\n",
    "            \n",
    "            # Slicing for mask creation, ensuring robustness\n",
    "            def get_slices(dim_size, win_size, sh_size):\n",
    "                if sh_size == 0 : # no shift, one segment\n",
    "                    return [slice(0, dim_size)]\n",
    "                # If dim_size is small, it implies fewer than 3 segments\n",
    "                if dim_size <= win_size: # Only one window segment if dim is small\n",
    "                     return [slice(0, dim_size)] # Could also be more complex if sh_size>0\n",
    "                else: # Standard Swin logic if dim > win_size\n",
    "                    return (slice(0, -win_size),\n",
    "                            slice(-win_size, -sh_size),\n",
    "                            slice(-sh_size, None))\n",
    "\n",
    "            slices_d = get_slices(D, self.window_size[0], self.shift_size[0])\n",
    "            slices_h = get_slices(H, self.window_size[1], self.shift_size[1])\n",
    "            slices_w = get_slices(W, self.window_size[2], self.shift_size[2])\n",
    "            \n",
    "            cnt = 0\n",
    "            for d_s in slices_d:\n",
    "                for h_s in slices_h:\n",
    "                    for w_s in slices_w:\n",
    "                        img_mask[:, d_s, h_s, w_s, :] = cnt\n",
    "                        cnt += 1\n",
    "            \n",
    "            mask_windows = window_partition_3d(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2])\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        self.register_buffer(\"attn_mask\", attn_mask, persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        D_orig, H_orig, W_orig = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == D_orig * H_orig * W_orig, f\"Input feature has wrong size {L} vs {D_orig*H_orig*W_orig}\"\n",
    "\n",
    "        shortcut = x\n",
    "        x_norm = self.norm1(x)\n",
    "        x_reshaped = x_norm.view(B, D_orig, H_orig, W_orig, C)\n",
    "\n",
    "        if any(s > 0 for s in self.shift_size):\n",
    "            shifted_x = torch.roll(x_reshaped, shifts=(-self.shift_size[0], -self.shift_size[1], -self.shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            shifted_x = x_reshaped\n",
    "\n",
    "        x_windows = window_partition_3d(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1] * self.window_size[2], C)\n",
    "        \n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows_reshaped = attn_windows.view(-1, self.window_size[0], self.window_size[1], self.window_size[2], C)\n",
    "        shifted_x_merged = window_reverse_3d(attn_windows_reshaped, self.window_size, D_orig, H_orig, W_orig)\n",
    "\n",
    "        if any(s > 0 for s in self.shift_size):\n",
    "            x_reversed_shift = torch.roll(shifted_x_merged, shifts=(self.shift_size[0], self.shift_size[1], self.shift_size[2]), dims=(1, 2, 3))\n",
    "        else:\n",
    "            x_reversed_shift = shifted_x_merged\n",
    "        \n",
    "        x_output_attn = x_reversed_shift.view(B, D_orig * H_orig * W_orig, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x_output_attn)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchMerging3D(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        # Merge 2x2x2 patches, channel dim becomes 8*dim, then projected to 2*dim\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        D, H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == D * H * W, \"input feature has wrong size\"\n",
    "        assert D % 2 == 0 and H % 2 == 0 and W % 2 == 0, \\\n",
    "            f\"Input dimensions ({D}x{H}x{W}) are not all even for PatchMerging.\"\n",
    "\n",
    "        x = x.view(B, D, H, W, C)\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = x.view(B, -1, 8 * C)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "class BasicLayer3D(nn.Module):\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock3D(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=tuple(0 if (i % 2 == 0) else w // 2 for w in window_size),\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x)\n",
    "            return x, x_down # Return pre-downsample for skip connection if needed by a larger model\n",
    "        return x, x # If no downsample, return x twice to match signature\n",
    "\n",
    "\n",
    "class PatchEmbed3D(nn.Module):\n",
    "    def __init__(self, img_size=(96,128,128), patch_size=(4,4,4), in_chans=1, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = [img_size[0] // patch_size[0],\n",
    "                                   img_size[1] // patch_size[1],\n",
    "                                   img_size[2] // patch_size[2]]\n",
    "        self.num_patches = self.patches_resolution[0] * self.patches_resolution[1] * self.patches_resolution[2]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = nn.Identity() # Use nn.Identity if norm_layer is None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        assert D == self.img_size[0] and H == self.img_size[1] and W == self.img_size[2], \\\n",
    "            f\"Input image size ({D}x{H}x{W}) doesn't match model ({self.img_size[0]}x{self.img_size[1]}x{self.img_size[2]}).\"\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# DropPath implementation (if not available in your torch version or for self-containment)\n",
    "def DropPath(drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "    if drop_prob == 0. or not True: # Assuming True means model.train()\n",
    "        return nn.Identity()\n",
    "    return DropPathLayer(drop_prob, scale_by_keep)\n",
    "\n",
    "class DropPathLayer(nn.Module):\n",
    "    def __init__(self, drop_prob=0., scale_by_keep=True):\n",
    "        super(DropPathLayer, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "        if keep_prob > 0.0 and self.scale_by_keep:\n",
    "            random_tensor.div_(keep_prob)\n",
    "        return x * random_tensor\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "\n",
    "\n",
    "class SwinTransformer3D(nn.Module):\n",
    "    def __init__(self, img_size=FINAL_SCAN_SIZE, patch_size=(4,4,4), in_chans=1, num_classes=NUM_CLASSES,\n",
    "                 embed_dim=48, depths=[2, 2, 6], num_heads=[3, 6, 12],\n",
    "                 window_size=(4,4,4), mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "\n",
    "        self.patch_embed = PatchEmbed3D(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.patches_resolution = self.patch_embed.patches_resolution # Store for use in layers\n",
    "\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            nn.init.trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "        else:\n",
    "            self.absolute_pos_embed = None\n",
    "\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_resolution = self.patches_resolution\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer3D(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=current_resolution,\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size, # Use the passed window_size\n",
    "                               mlp_ratio=mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging3D if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layer)\n",
    "            if i_layer < self.num_layers - 1: # Update resolution for the next layer\n",
    "                current_resolution = tuple(res // 2 for res in current_resolution)\n",
    "\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.absolute_pos_embed is not None:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            _, x = layer(x) # BasicLayer3D now returns pre-downsample (ignored here) and post-downsample\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the Swin3D Model\n",
    "# The key is that window_size must be compatible with feature map dimensions at ALL stages.\n",
    "# If img=(96,128,128), patch=(4,8,8) => patches_res=(24,16,16)\n",
    "# Stage 0 input: (24,16,16)\n",
    "# Stage 1 input: (12,8,8) (after 1 PatchMerging)\n",
    "# Stage 2 input: (6,4,4) (after 2 PatchMerging)\n",
    "# A window_size like (3,4,4) would work:\n",
    "# - Stage 0: (24,16,16) is div by (3,4,4)\n",
    "# - Stage 1: (12,8,8) is div by (3,4,4)\n",
    "# - Stage 2: (6,4,4) means 6 is div by 3, 4 by 4.\n",
    "# So, window_size=(3,4,4) seems appropriate.\n",
    "\n",
    "swin_model = SwinTransformer3D(\n",
    "    img_size=FINAL_SCAN_SIZE,\n",
    "    patch_size=(4,8,8),\n",
    "    embed_dim=64,\n",
    "    depths=[2, 2, 2],  # Reduced depth further for faster iteration/less memory\n",
    "    num_heads=[4, 8, 16],\n",
    "    window_size=(3,4,4), # Chosen to be compatible with (6,4,4)\n",
    "    num_classes=NUM_CLASSES,\n",
    "    drop_path_rate=0.1, # Increased slightly\n",
    "    ape=False\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Swin3D Model Instantiated. Number of parameters: {sum(p.numel() for p in swin_model.parameters() if p.requires_grad)}\")\n",
    "try:\n",
    "    dummy_input = torch.randn(BATCH_SIZE, 1, *FINAL_SCAN_SIZE).to(DEVICE)\n",
    "    output = swin_model(dummy_input)\n",
    "    print(f\"\\nSwin3D Model output shape: {output.shape}\") # Expected: (B, num_classes)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during Swin3D model test: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Loss and Optimizer ---\n",
    "train_labels_list = [patient_labels[pid] for pid in train_ids]\n",
    "count_0 = train_labels_list.count(0); count_1 = train_labels_list.count(1)\n",
    "# pos_weight should be close to 1.0 if train_ids are balanced\n",
    "pos_weight_val = (count_0 / count_1) if count_1 > 0 and count_0 > 0 and count_0 != count_1 else 1.0\n",
    "pos_weight_tensor = torch.tensor([pos_weight_val], device=DEVICE)\n",
    "print(f\"Calculated positive weight for BCEWithLogitsLoss (Swin3D): {pos_weight_val:.4f}\")\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "# Swin often uses AdamW with higher weight decay\n",
    "optimizer = optim.AdamW(swin_model.parameters(), lr=LEARNING_RATE, weight_decay=0.05) # Swin default wd\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "#--- Training and Validation Functions (Identical) ---\n",
    "def train_one_epoch_patient(model, dataloader, criterion, optimizer, device, scaler):\n",
    "    model.train(); running_loss = 0.0; total_samples = 0; correct_predictions = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False, ncols=100)\n",
    "    for inputs, labels in progress_bar:\n",
    "        valid_indices = labels != -1\n",
    "        inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].unsqueeze(1).to(device)\n",
    "        if inputs.nelement() == 0: continue\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "            outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "        if torch.isnan(loss) or torch.isinf(loss): print(f\"Invalid loss detected: {loss.item()}! Skipping batch.\");torch.cuda.empty_cache(); continue\n",
    "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        running_loss += loss.item() * inputs.size(0); total_samples += inputs.size(0)\n",
    "        preds = torch.sigmoid(outputs) > 0.5; correct_predictions += (preds == labels.bool()).sum().item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    if total_samples == 0: return 0.0, 0.0\n",
    "    return running_loss / total_samples, correct_predictions / total_samples\n",
    "\n",
    "def validate_patient(model, dataloader, criterion, device):\n",
    "    model.eval(); running_loss = 0.0; total_samples = 0; all_preds_proba = []; all_labels = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\", leave=False, ncols=100)\n",
    "        for inputs, labels in progress_bar:\n",
    "            valid_indices = labels != -1\n",
    "            inputs = inputs[valid_indices].to(device); labels = labels[valid_indices].unsqueeze(1).to(device)\n",
    "            if inputs.nelement() == 0: continue\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16, enabled=torch.cuda.is_available()):\n",
    "                outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "            if torch.isnan(loss) or torch.isinf(loss): print(f\"Invalid val_loss detected: {loss.item()}! Skipping batch.\"); continue\n",
    "            running_loss += loss.item() * inputs.size(0); total_samples += inputs.size(0)\n",
    "            all_preds_proba.extend(torch.sigmoid(outputs).cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
    "    if total_samples == 0: return 0.0, np.array([]), np.array([])\n",
    "    return running_loss / total_samples, np.array(all_labels).flatten(), np.array(all_preds_proba).flatten()\n",
    "\n",
    "#--- Training Loop ---\n",
    "print(f\"\\nStarting Training Swin3D Model (Balanced Data) for {EPOCHS} epochs...\")\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses, train_accs, val_accs_list = [], [], [], []\n",
    "MODEL_SAVE_PATH = os.path.join(PREPROCESSED_DSB_PATH, \"swin3d_model_corrected_best.pth\") # Added _corrected\n",
    "\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch_patient(swin_model, train_loader, criterion, optimizer, DEVICE, scaler)\n",
    "    val_loss, val_labels_epoch, val_preds_proba_epoch = validate_patient(swin_model, val_loader, criterion, DEVICE)\n",
    "    train_losses.append(train_loss); val_losses.append(val_loss); train_accs.append(train_acc)\n",
    "    end_epoch_time = time.time(); epoch_duration = end_epoch_time - start_epoch_time\n",
    "    val_acc_epoch = 0.0\n",
    "    if len(val_labels_epoch) > 0 and val_labels_epoch.size > 0 and val_preds_proba_epoch.size > 0:\n",
    "        val_acc_epoch = accuracy_score(val_labels_epoch, (val_preds_proba_epoch > 0.5).astype(int))\n",
    "    val_accs_list.append(val_acc_epoch)\n",
    "    print(f\"Epoch {epoch+1} Summary: Duration: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc_epoch:.4f}\")\n",
    "    if val_loss < best_val_loss and len(val_labels_epoch) > 0 and not (torch.isinf(torch.tensor(val_loss)) or torch.isnan(torch.tensor(val_loss))): # Ensure val_loss is valid\n",
    "        best_val_loss = val_loss\n",
    "        try: torch.save(swin_model.state_dict(), MODEL_SAVE_PATH); print(f\"  Best model saved to {MODEL_SAVE_PATH}\")\n",
    "        except Exception as e: print(f\"Error saving model: {e}\")\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nSwin3D Model (Balanced Data) Training Finished.\")\n",
    "\n",
    "# --- Plot Training History & Evaluation ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1); plt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss'); plt.plot(range(1, EPOCHS + 1), val_losses, label='Val Loss'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss Curve (Swin3D Corrected, Balanced)'); plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2); plt.plot(range(1, EPOCHS + 1), train_accs, label='Train Acc'); plt.plot(range(1, EPOCHS + 1), val_accs_list, label='Val Acc'); plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy Curve (Swin3D Corrected, Balanced)'); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout(); plot_save_path = os.path.join(PREPROCESSED_DSB_PATH, \"training_curves_swin3d_corrected_balanced.png\")\n",
    "plt.savefig(plot_save_path); print(f\"Training curves saved to {plot_save_path}\"); plt.close()\n",
    "\n",
    "print(\"\\nEvaluating Swin3D Model (Balanced Data) on Validation Set...\")\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    try: # Re-instantiate with the same parameters as trained\n",
    "        eval_model = SwinTransformer3D(\n",
    "            img_size=FINAL_SCAN_SIZE, patch_size=(4,8,8), embed_dim=64,\n",
    "            depths=[2,2,2], num_heads=[4,8,16], window_size=(3,4,4),\n",
    "            num_classes=NUM_CLASSES, drop_path_rate=0.1, ape=False\n",
    "        ).to(DEVICE)\n",
    "        eval_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "        print(f\"Loaded best Swin3D model from {MODEL_SAVE_PATH}\")\n",
    "    except Exception as e: print(f\"Could not load best model: {e}. Using last epoch model.\"); eval_model = swin_model\n",
    "else: print(\"Best model file not found. Using last epoch model.\"); eval_model = swin_model\n",
    "\n",
    "val_loss_final, final_val_labels, final_val_preds_proba = validate_patient(eval_model, val_loader, criterion, DEVICE)\n",
    "if len(final_val_labels) == 0: print(\"No valid validation predictions.\")\n",
    "else:\n",
    "    print(f\"\\nFinal Validation Loss (Swin3D Corrected, Balanced): {val_loss_final:.4f}\")\n",
    "    final_val_preds_binary = (final_val_preds_proba > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(final_val_labels, final_val_preds_binary)\n",
    "    precision = precision_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    recall = recall_score(final_val_labels, final_val_preds_binary, zero_division=0)\n",
    "    f1 = f1_score(final_val_labels, final_val_preds_binary, zero_division=0); auc_roc = float('nan')\n",
    "    if len(np.unique(final_val_labels)) > 1:\n",
    "        try: auc_roc = roc_auc_score(final_val_labels, final_val_preds_proba)\n",
    "        except ValueError as e: print(f\"AUC-ROC Error: {e}.\")\n",
    "    else: print(\"AUC-ROC not calculated: only one class in y_true.\")\n",
    "    print(\"\\n--- Final Validation Metrics (Swin3D Corrected, Balanced) ---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\\nAUC-ROC:   {auc_roc:.4f}\")\n",
    "    target_names = ['Non-Cancer (0)', 'Cancer (1)']\n",
    "    print(\"\\nClassification Report (Swin3D Corrected, Balanced):\")\n",
    "    if len(np.unique(final_val_labels)) > 1: print(classification_report(final_val_labels, final_val_preds_binary, target_names=target_names, zero_division=0))\n",
    "    else: print(\"Classification report not generated: only one class in y_true.\")\n",
    "    print(\"\\nConfusion Matrix (Swin3D Corrected, Balanced):\")\n",
    "    cm = confusion_matrix(final_val_labels, final_val_preds_binary, labels=[0,1]) # Ensure labels for CM\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=target_names); disp.plot(cmap=plt.cm.Blues); plt.title(\"Confusion Matrix (Swin3D Corrected, Balanced)\")\n",
    "    cm_plot_save_path = os.path.join(PREPROCESSED_DSB_PATH, \"confusion_matrix_swin3d_corrected_balanced.png\")\n",
    "    plt.savefig(cm_plot_save_path); print(f\"Confusion matrix plot saved to {cm_plot_save_path}\"); plt.close()\n",
    "    if not np.isnan(auc_roc) and not (torch.isinf(torch.tensor(auc_roc))): # check for inf as well\n",
    "        fpr, tpr, _ = roc_curve(final_val_labels, final_val_preds_proba)\n",
    "        plt.figure(figsize=(8,6)); plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_roc:.2f})'); plt.plot([0,1],[0,1],color='navy',lw=2,linestyle='--'); plt.xlim([0.0,1.0]); plt.ylim([0.0,1.05]); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve (Swin3D Corrected, Balanced)'); plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "        roc_plot_save_path = os.path.join(PREPROCESSED_DSB_PATH, \"roc_curve_swin3d_corrected_balanced.png\")\n",
    "        plt.savefig(roc_plot_save_path); print(f\"ROC curve plot saved to {roc_plot_save_path}\"); plt.close()\n",
    "    else: print(\"ROC curve not plotted (AUC is NaN or Inf).\")\n",
    "print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
