{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32133b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Configuration (SET FORCE_REPROCESS = True FOR ONE RUN)\n",
    "class Config:\n",
    "    \"\"\"Configuration class for all hyperparameters and paths.\"\"\"\n",
    "    DSB_PATH = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Stages\"\n",
    "    DSB_LABELS_CSV = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\stage1_labels.csv\"\n",
    "    PREPROCESSED_DSB_PATH = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\PreProCessing\"\n",
    "    MODEL_OUTPUT_DIR = r\"C:\\Users\\rouaa\\Documents\\Final_Pneumatect\\Models\"\n",
    "\n",
    "    TARGET_SPACING = [1.5, 1.5, 1.5]\n",
    "    FINAL_SCAN_SIZE = (32, 32, 32) # Keep reduced resolution\n",
    "    CLIP_BOUND_HU = [-1000.0, 400.0]\n",
    "    PIXEL_MEAN = 0.25\n",
    "\n",
    "    SCAN_LIMIT_PER_CLASS = 50\n",
    "    SEED = 42\n",
    "    # --- SET TO TRUE FOR ONE RUN ---\n",
    "    FORCE_REPROCESS = True\n",
    "    # --- SET TO TRUE FOR ONE RUN ---\n",
    "\n",
    "    BATCH_SIZE = 1 # Keep 1 for CPU\n",
    "    NUM_CLASSES = 1\n",
    "    LEARNING_RATE = 1e-4\n",
    "    EPOCHS = 10\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "    UNETPP_INITIAL_FILTERS = 8\n",
    "    UNETPP_DEPTH = 2\n",
    "    UNETPP_TRANSFORMER_EMBED_DIM = 64\n",
    "    UNETPP_TRANSFORMER_LAYERS = 1\n",
    "    UNETPP_TRANSFORMER_HEADS = 4\n",
    "    UNETPP_FINAL_FC_UNITS = 32\n",
    "    STOCHASTIC_DEPTH_RATE = 0.1\n",
    "    CLASSIFICATION_LOSS_WEIGHT = 1.0\n",
    "    SEGMENTATION_LOSS_WEIGHT = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "torch.manual_seed(Config.SEED)\n",
    "# No GPU-specific seeds needed\n",
    "\n",
    "# --- FORCE CPU USAGE ---\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\") # Will now print \"Using device: cpu\"\n",
    "# --- FORCE CPU USAGE ---\n",
    "\n",
    "os.makedirs(Config.PREPROCESSED_DSB_PATH, exist_ok=True)\n",
    "os.makedirs(Config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# %% Placeholder Data Functions (Ensure they use Config.FINAL_SCAN_SIZE)\n",
    "def preprocess_scans(scan_ids):\n",
    "    \"\"\"Placeholder for preprocessing logic.\"\"\"\n",
    "    print(\"Placeholder: Preprocessing scans...\")\n",
    "    successful_ids = []\n",
    "    os.makedirs(Config.PREPROCESSED_DSB_PATH, exist_ok=True)\n",
    "    target_size = Config.FINAL_SCAN_SIZE # Use size from Config\n",
    "    for patient_id in tqdm(scan_ids, desc=\"Preprocessing Simulation\"):\n",
    "        out_path = os.path.join(Config.PREPROCESSED_DSB_PATH, f\"{patient_id}.npz\")\n",
    "        if not os.path.exists(out_path) or Config.FORCE_REPROCESS:\n",
    "            if Config.FORCE_REPROCESS:\n",
    "                 print(f\"Reprocessing {patient_id} to size {target_size}...\")\n",
    "            else:\n",
    "                 # Only print if file truly doesn't exist when not forcing\n",
    "                 if not os.path.exists(out_path):\n",
    "                     print(f\"Simulating creation for {patient_id} (file not found)...\")\n",
    "\n",
    "            # Generate dummy data only if needed\n",
    "            if not os.path.exists(out_path) or Config.FORCE_REPROCESS:\n",
    "                dummy_image = np.random.rand(*target_size) * (Config.CLIP_BOUND_HU[1] - Config.CLIP_BOUND_HU[0]) + Config.CLIP_BOUND_HU[0]\n",
    "                dummy_image = (dummy_image - Config.CLIP_BOUND_HU[0]) / (Config.CLIP_BOUND_HU[1] - Config.CLIP_BOUND_HU[0]) # Normalize 0-1\n",
    "                dummy_image -= Config.PIXEL_MEAN # Zero center around PIXEL_MEAN\n",
    "                np.savez_compressed(out_path, image=dummy_image.astype(np.float32))\n",
    "\n",
    "        # Assume success if file exists after simulated processing\n",
    "        if os.path.exists(out_path):\n",
    "           successful_ids.append(patient_id)\n",
    "        else:\n",
    "            print(f\"Warning: Failed to preprocess or find {patient_id}\")\n",
    "\n",
    "    if Config.FORCE_REPROCESS:\n",
    "        print(\"\\nIMPORTANT: FORCE_REPROCESS was True. Ensure it is False for next runs.\\n\")\n",
    "\n",
    "    print(f\"Successfully preprocessed/found {len(successful_ids)} scans.\")\n",
    "    return successful_ids\n",
    "\n",
    "def load_and_select_data():\n",
    "    \"\"\"Placeholder for data loading and selection logic.\"\"\"\n",
    "    print(\"Placeholder: Loading and selecting data...\")\n",
    "    try:\n",
    "        df_labels = pd.read_csv(Config.DSB_LABELS_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Labels CSV not found at {Config.DSB_LABELS_CSV}\")\n",
    "        return [], {}\n",
    "    all_patient_ids = df_labels['id'].tolist()\n",
    "    patient_labels = df_labels.set_index('id')['cancer'].to_dict()\n",
    "\n",
    "    ids_class_0 = [pid for pid in all_patient_ids if patient_labels.get(pid) == 0]\n",
    "    ids_class_1 = [pid for pid in all_patient_ids if patient_labels.get(pid) == 1]\n",
    "    random.shuffle(ids_class_0)\n",
    "    random.shuffle(ids_class_1)\n",
    "    selected_ids = ids_class_0[:Config.SCAN_LIMIT_PER_CLASS] + ids_class_1[:Config.SCAN_LIMIT_PER_CLASS]\n",
    "    scans_to_process = selected_ids\n",
    "\n",
    "    print(f\"Selected {len(scans_to_process)} scans ({len(ids_class_0[:Config.SCAN_LIMIT_PER_CLASS])} class 0, {len(ids_class_1[:Config.SCAN_LIMIT_PER_CLASS])} class 1)\")\n",
    "    return scans_to_process, patient_labels\n",
    "\n",
    "def create_dataloaders(successful_ids, patient_labels):\n",
    "    \"\"\"Placeholder for creating dataloaders.\"\"\"\n",
    "    print(\"Placeholder: Creating dataloaders...\")\n",
    "    if not successful_ids:\n",
    "        raise ValueError(\"No successfully processed patient IDs found.\")\n",
    "\n",
    "    valid_patient_labels = {pid: label for pid, label in patient_labels.items() if pid in successful_ids}\n",
    "    stratify_labels = [valid_patient_labels.get(pid, -1) for pid in successful_ids]\n",
    "\n",
    "    filtered_ids = [pid for pid, label in zip(successful_ids, stratify_labels) if label != -1]\n",
    "    filtered_stratify_labels = [label for label in stratify_labels if label != -1]\n",
    "\n",
    "    if not filtered_ids:\n",
    "         raise ValueError(\"No patient IDs with valid labels found for splitting.\")\n",
    "\n",
    "    try:\n",
    "        train_ids, val_ids = train_test_split(\n",
    "            filtered_ids, test_size=0.2, random_state=Config.SEED,\n",
    "            stratify=filtered_stratify_labels\n",
    "        )\n",
    "    except ValueError as e:\n",
    "         print(f\"Stratified split failed: {e}. Falling back to non-stratified split.\")\n",
    "         train_ids, val_ids = train_test_split(\n",
    "             filtered_ids, test_size=0.2, random_state=Config.SEED)\n",
    "\n",
    "\n",
    "    train_dataset = PatientLevelDataset(train_ids, patient_labels, Config.PREPROCESSED_DSB_PATH, mask_path=Config.PREPROCESSED_DSB_PATH)\n",
    "    val_dataset = PatientLevelDataset(val_ids, patient_labels, Config.PREPROCESSED_DSB_PATH, mask_path=Config.PREPROCESSED_DSB_PATH)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=False # Set pin_memory to False for CPU\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=False # Set pin_memory to False for CPU\n",
    "    )\n",
    "    print(f\"Created DataLoaders: Train batches={len(train_loader)}, Val batches={len(val_loader)}\")\n",
    "    return train_loader, val_loader, train_ids, val_ids\n",
    "\n",
    "# %% Model Definition (remains the same)\n",
    "class SEBlock3D(nn.Module):\n",
    "    \"\"\"3D Squeeze-and-Excitation Block.\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock3D, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool3d(1)\n",
    "        rd = max(1, channels // reduction)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv3d(channels, rd, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(rd, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.squeeze(x)\n",
    "        y = self.excitation(y)\n",
    "        return x * y\n",
    "\n",
    "class ConvBlock3D(nn.Module):\n",
    "    \"\"\"Double Conv -> BN -> ReLU with SE.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, use_se=True, stochastic_depth_rate=0.0):\n",
    "        super(ConvBlock3D, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.stochastic_depth_rate = stochastic_depth_rate\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.se = SEBlock3D(out_channels, reduction=max(1, out_channels//16)) if use_se else nn.Identity()\n",
    "        self.dropout = nn.Dropout(stochastic_depth_rate) if stochastic_depth_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.se(x)\n",
    "        if self.training and self.stochastic_depth_rate > 0:\n",
    "             x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EfficientTransformerLayer3D(nn.Module):\n",
    "    \"\"\"Efficient Transformer Layer with MultiHead Attention.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim_factor=4, dropout=0.1, stochastic_depth_rate=0.0):\n",
    "        super(EfficientTransformerLayer3D, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "             raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        ff_dim = embed_dim * ff_dim_factor\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.stochastic_depth = nn.Dropout(stochastic_depth_rate) if stochastic_depth_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        identity1 = src\n",
    "        src_norm = self.norm1(src)\n",
    "        attn_output, attn_weights = self.attn(src_norm, src_norm, src_norm, attn_mask=src_mask,\n",
    "                                            key_padding_mask=src_key_padding_mask,\n",
    "                                            need_weights=True) # Keep True for visualization\n",
    "        src = identity1 + self.dropout1(attn_output)\n",
    "\n",
    "        identity2 = src\n",
    "        src_norm = self.norm2(src)\n",
    "        ffn_output = self.ffn(src_norm)\n",
    "        src = identity2 + self.dropout2(ffn_output)\n",
    "\n",
    "        if self.training and isinstance(self.stochastic_depth, nn.Dropout):\n",
    "             src = self.stochastic_depth(src)\n",
    "\n",
    "        return src, attn_weights\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block for Multi-Scale Integration.\"\"\"\n",
    "    def __init__(self, in_channels, embed_dim, num_layers, num_heads, spatial_dims, dropout=0.1, stochastic_depth_rate=0.0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.patch_projection = nn.Conv3d(in_channels, embed_dim, kernel_size=1) if in_channels != embed_dim else nn.Identity()\n",
    "        if not spatial_dims or any(s <= 0 for s in spatial_dims):\n",
    "             raise ValueError(f\"Invalid spatial_dims for TransformerBlock: {spatial_dims}\")\n",
    "        num_patches = np.prod(spatial_dims)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim) * 0.02)\n",
    "        self.dropout_pos = nn.Dropout(dropout)\n",
    "\n",
    "        sd_rates = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            EfficientTransformerLayer3D(embed_dim, num_heads, dropout=dropout,\n",
    "                                     stochastic_depth_rate=sd_rates[i])\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.norm_out = nn.LayerNorm(embed_dim)\n",
    "        self.out_projection = nn.Conv3d(embed_dim, in_channels, kernel_size=1) if in_channels != embed_dim else nn.Identity()\n",
    "        self.spatial_dims = spatial_dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, d, h, w = x.shape\n",
    "        if (d, h, w) != self.spatial_dims:\n",
    "             print(f\"Warning: Input spatial dimensions {d,h,w} differ from expected {self.spatial_dims} in TransformerBlock.\")\n",
    "             # Consider adaptive pooling if this becomes an issue:\n",
    "             # x = F.adaptive_avg_pool3d(x, self.spatial_dims)\n",
    "\n",
    "        x_proj = self.patch_projection(x)\n",
    "        embed_dim = x_proj.shape[1]\n",
    "        x_flat = x_proj.flatten(2).transpose(1, 2) # Shape: (b, num_patches, embed_dim)\n",
    "\n",
    "        if x_flat.shape[1] != self.pos_embed.shape[1]:\n",
    "             print(f\"Warning: Patch count {x_flat.shape[1]} mismatch with pos embed {self.pos_embed.shape[1]}. Skipping pos embed.\")\n",
    "             x_processed = x_flat\n",
    "        else:\n",
    "             x_processed = x_flat + self.pos_embed\n",
    "\n",
    "        x_processed = self.dropout_pos(x_processed)\n",
    "\n",
    "        attn_weights = []\n",
    "        for layer in self.transformer_layers:\n",
    "            x_processed, attn = layer(x_processed)\n",
    "            attn_weights.append(attn)\n",
    "\n",
    "        x_processed = self.norm_out(x_processed)\n",
    "        x_reshaped = x_processed.transpose(1, 2).view(b, embed_dim, *self.spatial_dims)\n",
    "        x_out = self.out_projection(x_reshaped)\n",
    "        return x_out, attn_weights\n",
    "\n",
    "\n",
    "class EfficientUNetPlusPlus_SE_Transformer(nn.Module):\n",
    "    \"\"\"Efficient U-Net++ with SE and Multi-Scale Transformers (Transformer skipped at level 0).\"\"\"\n",
    "    def __init__(self, input_scan_size=Config.FINAL_SCAN_SIZE, # Use Config default\n",
    "                 in_channels=1, num_classes=1,\n",
    "                 initial_filters=Config.UNETPP_INITIAL_FILTERS, depth=Config.UNETPP_DEPTH,\n",
    "                 use_se=True, transformer_embed_dim=Config.UNETPP_TRANSFORMER_EMBED_DIM,\n",
    "                 transformer_layers=Config.UNETPP_TRANSFORMER_LAYERS,\n",
    "                 transformer_heads=Config.UNETPP_TRANSFORMER_HEADS, transformer_dropout=0.1,\n",
    "                 final_fc_units=Config.UNETPP_FINAL_FC_UNITS,\n",
    "                 stochastic_depth_rate=Config.STOCHASTIC_DEPTH_RATE):\n",
    "        super(EfficientUNetPlusPlus_SE_Transformer, self).__init__()\n",
    "        self.depth = depth\n",
    "        if not isinstance(input_scan_size, tuple) or len(input_scan_size) != 3:\n",
    "             raise ValueError(\"input_scan_size must be a tuple of 3 integers (D, H, W)\")\n",
    "        self.input_scan_size = input_scan_size\n",
    "        nf = initial_filters\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.transformer_blocks = nn.ModuleList() # Will hold Transformers or Identity\n",
    "        self.pools = nn.ModuleList()\n",
    "        encoder_output_channels = []\n",
    "        current_channels = in_channels\n",
    "\n",
    "        for i in range(depth + 1):\n",
    "            out_ch = nf * (2**i)\n",
    "            embed_dim_for_layer = transformer_embed_dim\n",
    "            if i > 0 and embed_dim_for_layer % transformer_heads != 0:\n",
    "                 adjusted_embed_dim = (embed_dim_for_layer // transformer_heads) * transformer_heads\n",
    "                 if adjusted_embed_dim == 0: adjusted_embed_dim = transformer_heads\n",
    "                 print(f\"Warning: Level {i} embed_dim {embed_dim_for_layer} adjusted to {adjusted_embed_dim} for heads {transformer_heads}.\")\n",
    "                 embed_dim_for_layer = adjusted_embed_dim\n",
    "\n",
    "            self.encoder_blocks.append(ConvBlock3D(\n",
    "                current_channels, out_ch, use_se=use_se,\n",
    "                stochastic_depth_rate=stochastic_depth_rate\n",
    "            ))\n",
    "\n",
    "            spatial_dims = tuple(s // (2**i) for s in input_scan_size)\n",
    "            if any(s <= 0 for s in spatial_dims):\n",
    "                 raise ValueError(f\"Calculated spatial dimensions {spatial_dims} at depth {i} are invalid.\")\n",
    "\n",
    "            # Apply Transformer only for levels i > 0\n",
    "            if i > 0:\n",
    "                 self.transformer_blocks.append(TransformerBlock(\n",
    "                     in_channels=out_ch,\n",
    "                     embed_dim=embed_dim_for_layer,\n",
    "                     num_layers=transformer_layers,\n",
    "                     num_heads=transformer_heads,\n",
    "                     spatial_dims=spatial_dims,\n",
    "                     dropout=transformer_dropout,\n",
    "                     stochastic_depth_rate=stochastic_depth_rate\n",
    "                 ))\n",
    "            else:\n",
    "                 self.transformer_blocks.append(nn.Identity()) # Skip transformer at full resolution\n",
    "\n",
    "            encoder_output_channels.append(out_ch)\n",
    "            if i < depth:\n",
    "                self.pools.append(nn.MaxPool3d(2, 2))\n",
    "            current_channels = out_ch\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_conv_modulelist = nn.ModuleList()\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "             ch_from_below = encoder_output_channels[i+1]\n",
    "             ch_to_current_level_filters = encoder_output_channels[i]\n",
    "             self.upsamplers.append(\n",
    "                 nn.ConvTranspose3d(ch_from_below, ch_to_current_level_filters, kernel_size=2, stride=2)\n",
    "             )\n",
    "\n",
    "        for i in range(depth):\n",
    "            level_i_decoder_blocks = nn.ModuleList()\n",
    "            for j in range(1, depth - i + 1):\n",
    "                in_ch_Xij = encoder_output_channels[i] * j + encoder_output_channels[i]\n",
    "                out_ch_Xij = encoder_output_channels[i]\n",
    "                level_i_decoder_blocks.append(ConvBlock3D(\n",
    "                    in_ch_Xij, out_ch_Xij, use_se=use_se,\n",
    "                    stochastic_depth_rate=stochastic_depth_rate\n",
    "                ))\n",
    "            self.decoder_conv_modulelist.append(level_i_decoder_blocks)\n",
    "\n",
    "        # Classification Head\n",
    "        final_decoder_output_channels = encoder_output_channels[0]\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(final_decoder_output_channels, final_fc_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(final_fc_units, num_classes)\n",
    "        )\n",
    "\n",
    "        # Segmentation Head (outputs logits)\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv3d(final_decoder_output_channels, 1, kernel_size=1),\n",
    "            nn.Identity() # Output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[2:] != self.input_scan_size:\n",
    "             print(f\"Warning: Input scan size {x.shape[2:]} does not match model expected size {self.input_scan_size}. Interpolating.\")\n",
    "             x = F.interpolate(x, size=self.input_scan_size, mode='trilinear', align_corners=False)\n",
    "\n",
    "        X_features = [[None] * (self.depth + 1) for _ in range(self.depth + 1)]\n",
    "        attention_maps = [[] for _ in range(self.depth + 1)] # Store list of lists\n",
    "\n",
    "        # Encoder path\n",
    "        current = x\n",
    "        for i in range(self.depth + 1):\n",
    "            conv_out = self.encoder_blocks[i](current)\n",
    "            transformer_module = self.transformer_blocks[i]\n",
    "\n",
    "            # Apply transformer or identity\n",
    "            if isinstance(transformer_module, TransformerBlock):\n",
    "                trans_out, attn_list = transformer_module(conv_out)\n",
    "                X_features[i][0] = trans_out\n",
    "                if attn_list: # attn_list is a list of weights per layer\n",
    "                    attention_maps[i].extend(attn_list)\n",
    "            else: # nn.Identity\n",
    "                X_features[i][0] = transformer_module(conv_out)\n",
    "                # attention_maps[i] remains empty list []\n",
    "\n",
    "            if i < self.depth:\n",
    "                current = self.pools[i](X_features[i][0])\n",
    "\n",
    "        # Decoder path\n",
    "        for j in range(1, self.depth + 1):\n",
    "             for i in range(self.depth - j + 1):\n",
    "                 inputs_same_level = [X_features[i][k] for k in range(j)]\n",
    "                 upsampled_input = self.upsamplers[i](X_features[i+1][j-1])\n",
    "                 target_spatial = X_features[i][0].shape[2:]\n",
    "                 if upsampled_input.shape[2:] != target_spatial:\n",
    "                     upsampled_input = F.interpolate(\n",
    "                         upsampled_input, size=target_spatial,\n",
    "                         mode='trilinear', align_corners=False\n",
    "                     )\n",
    "                 combined = torch.cat(inputs_same_level + [upsampled_input], dim=1)\n",
    "                 X_features[i][j] = self.decoder_conv_modulelist[i][j-1](combined)\n",
    "\n",
    "        # Final Outputs\n",
    "        final_decoder_output = X_features[0][self.depth]\n",
    "        classification_logits = self.classification_head(final_decoder_output)\n",
    "        segmentation_logits = self.segmentation_head(final_decoder_output) # Get logits\n",
    "\n",
    "        # Return classification logits, segmentation LOGITS, and attention maps\n",
    "        return classification_logits, segmentation_logits, attention_maps\n",
    "\n",
    "    def get_attention_maps(self, x):\n",
    "        \"\"\"Extract attention maps (first layer only per block) for visualization.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            if x.shape[2:] != self.input_scan_size:\n",
    "                 x = F.interpolate(x, size=self.input_scan_size, mode='trilinear', align_corners=False)\n",
    "            # We only need the third return value (attention maps)\n",
    "            _, _, attention_maps_nested = self.forward(x)\n",
    "\n",
    "        # Filter out empty lists (from level 0) and get the first layer's map from others\n",
    "        flat_attention_maps = []\n",
    "        for level_maps in attention_maps_nested: # Iterate through levels\n",
    "             if level_maps: # Check if list for this level is not empty (i.e., transformer was applied)\n",
    "                 # level_maps is list of attention tensors, one per transformer layer\n",
    "                 # Take the first layer's attention map for simplicity\n",
    "                 first_layer_attn = level_maps[0] # Shape (batch, heads, seq, seq)\n",
    "                 flat_attention_maps.append(first_layer_attn)\n",
    "        return flat_attention_maps\n",
    "\n",
    "\n",
    "# %% Dataset\n",
    "class PatientLevelDataset(Dataset):\n",
    "    \"\"\"Dataset with classification labels and segmentation masks.\"\"\"\n",
    "    def __init__(self, patient_ids, labels_dict, preprocessed_path, mask_path=None):\n",
    "        self.patient_ids = patient_ids\n",
    "        self.labels_dict = labels_dict\n",
    "        self.preprocessed_path = preprocessed_path\n",
    "        self.mask_path = mask_path if mask_path else preprocessed_path\n",
    "        self.target_size = Config.FINAL_SCAN_SIZE # Use size from Config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        label = self.labels_dict.get(patient_id, -1)\n",
    "        scan_path = os.path.join(self.preprocessed_path, f\"{patient_id}.npz\")\n",
    "\n",
    "        error_image = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "        error_label = torch.tensor(-1, dtype=torch.float32)\n",
    "        error_mask = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "\n",
    "        if label == -1:\n",
    "            return error_image, error_label, error_mask\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(scan_path):\n",
    "                 # print(f\"Error: Preprocessed file not found: {scan_path}\") # Reduce noise\n",
    "                 return error_image, error_label, error_mask\n",
    "\n",
    "            with np.load(scan_path) as npz_data:\n",
    "                if 'image' not in npz_data:\n",
    "                     print(f\"Error: 'image' key not found in {scan_path}\")\n",
    "                     return error_image, error_label, error_mask\n",
    "                image = npz_data['image']\n",
    "\n",
    "            if image.shape != self.target_size:\n",
    "                print(f\"Shape mismatch for {patient_id}: Expected {self.target_size}, got {image.shape}. Attempting resize.\")\n",
    "                try:\n",
    "                     zoom_factors = tuple(t / s for t, s in zip(self.target_size, image.shape))\n",
    "                     image_resized = ndimage.zoom(image, zoom_factors, order=1) # order=1 for image\n",
    "                     if image_resized.shape != self.target_size:\n",
    "                          print(f\"Error: Resizing failed for {patient_id}. Got shape {image_resized.shape}\")\n",
    "                          return error_image, error_label, error_mask\n",
    "                     image = image_resized\n",
    "                except Exception as resize_e:\n",
    "                    print(f\"Error during image resizing for {patient_id}: {resize_e}\")\n",
    "                    return error_image, error_label, error_mask\n",
    "\n",
    "            image_tensor = torch.from_numpy(image).float().unsqueeze(0)\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "            # Load segmentation mask\n",
    "            mask_tensor = torch.zeros((1, *self.target_size), dtype=torch.float32)\n",
    "            mask_file = os.path.join(self.mask_path, f\"{patient_id}_mask.npz\")\n",
    "            if os.path.exists(mask_file):\n",
    "                try:\n",
    "                    with np.load(mask_file) as mask_data:\n",
    "                        if 'mask' in mask_data:\n",
    "                             mask = mask_data['mask']\n",
    "                             if mask.shape == self.target_size:\n",
    "                                 mask_tensor = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "                             else:\n",
    "                                 # print(f\"Warning: Mask shape mismatch for {patient_id}. Expected {self.target_size}, got {mask.shape}. Attempting resize.\")\n",
    "                                 try:\n",
    "                                      zoom_factors = tuple(t / s for t, s in zip(self.target_size, mask.shape))\n",
    "                                      mask_resized = ndimage.zoom(mask, zoom_factors, order=0) # order=0 for mask\n",
    "                                      if mask_resized.shape == self.target_size:\n",
    "                                           mask_tensor = torch.from_numpy(mask_resized).float().unsqueeze(0)\n",
    "                                      # else: # Reduce noise\n",
    "                                      #      print(f\"Warning: Mask resizing failed for {patient_id}. Using zero mask.\")\n",
    "                                 except Exception as resize_mask_e:\n",
    "                                     print(f\"Error during mask resizing for {patient_id}: {resize_mask_e}. Using zero mask.\")\n",
    "                except Exception as e_mask:\n",
    "                    print(f\"Error loading mask for {patient_id}: {e_mask}. Using zero mask.\")\n",
    "\n",
    "            mask_tensor = mask_tensor.float()\n",
    "            return image_tensor, label_tensor, mask_tensor\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             # print(f\"Error: File not found for {patient_id} at {scan_path}\") # Reduce noise\n",
    "             return error_image, error_label, error_mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {patient_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Print detailed error\n",
    "            return error_image, error_label, error_mask\n",
    "\n",
    "\n",
    "# %% Training and Validation Functions (CPU VERSION - NO AMP/SCALER)\n",
    "def train_one_epoch(model, dataloader, criterion_cls, criterion_seg, optimizer, device): # Removed scaler\n",
    "    model.train()\n",
    "    running_loss_cls = 0.0\n",
    "    running_loss_seg = 0.0\n",
    "    total_loss_weighted = 0.0\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for inputs, labels, masks in tqdm(dataloader, desc=\"Training (CPU)\", leave=False): # Indicate CPU\n",
    "        valid_indices = labels != -1\n",
    "        if not torch.any(valid_indices):\n",
    "            continue\n",
    "\n",
    "        # Ensure batch dimension exists even if batch size is 1\n",
    "        inputs = inputs[valid_indices].to(device)\n",
    "        labels = labels[valid_indices].unsqueeze(1).to(device) # Ensure (N, 1) shape\n",
    "        masks = masks[valid_indices].to(device)\n",
    "\n",
    "        current_batch_size = inputs.size(0)\n",
    "        if current_batch_size == 0: continue\n",
    "        total_samples += current_batch_size\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # --- NO torch.amp.autocast ---\n",
    "        # Model returns logits for both outputs\n",
    "        cls_logits, seg_logits, _ = model(inputs)\n",
    "\n",
    "        # Classification loss (expects logits)\n",
    "        loss_cls = criterion_cls(cls_logits, labels)\n",
    "\n",
    "        # Segmentation loss (expects logits)\n",
    "        loss_seg = criterion_seg(seg_logits, masks)\n",
    "\n",
    "        # Combined weighted loss\n",
    "        loss = Config.CLASSIFICATION_LOSS_WEIGHT * loss_cls + Config.SEGMENTATION_LOSS_WEIGHT * loss_seg\n",
    "        # --- NO torch.amp.autocast ---\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Warning: NaN/Inf loss detected (Cls: {loss_cls.item():.4f}, Seg: {loss_seg.item():.4f}). Skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        # --- Standard backward pass (NO SCALER) ---\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # --- Standard backward pass (NO SCALER) ---\n",
    "\n",
    "        running_loss_cls += loss_cls.item() * current_batch_size\n",
    "        running_loss_seg += loss_seg.item() * current_batch_size\n",
    "        total_loss_weighted += loss.item() * current_batch_size\n",
    "\n",
    "        preds = torch.sigmoid(cls_logits) > 0.5\n",
    "        correct_predictions += (preds == labels.bool()).sum().item()\n",
    "\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No valid samples processed in training epoch.\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    avg_loss_cls = running_loss_cls / total_samples\n",
    "    avg_loss_seg = running_loss_seg / total_samples\n",
    "    avg_loss_total = total_loss_weighted / total_samples\n",
    "    avg_acc = correct_predictions / total_samples\n",
    "\n",
    "    return avg_loss_cls, avg_loss_seg, avg_loss_total, avg_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion_cls, criterion_seg, device): # Removed scaler\n",
    "    model.eval()\n",
    "    running_loss_cls = 0.0\n",
    "    running_loss_seg = 0.0\n",
    "    total_loss_weighted = 0.0\n",
    "    total_samples = 0\n",
    "    all_preds_proba = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, masks in tqdm(dataloader, desc=\"Validating (CPU)\", leave=False): # Indicate CPU\n",
    "            valid_indices = labels != -1\n",
    "            if not torch.any(valid_indices):\n",
    "                continue\n",
    "\n",
    "            inputs = inputs[valid_indices].to(device)\n",
    "            labels = labels[valid_indices].to(device) # Keep 1D for metrics later\n",
    "            masks = masks[valid_indices].to(device)\n",
    "\n",
    "            current_batch_size = inputs.size(0)\n",
    "            if current_batch_size == 0: continue\n",
    "            total_samples += current_batch_size\n",
    "\n",
    "            # --- NO torch.amp.autocast ---\n",
    "            cls_logits, seg_logits, _ = model(inputs)\n",
    "\n",
    "            # Classification loss expects logits and (N, 1) labels\n",
    "            loss_cls = criterion_cls(cls_logits, labels.unsqueeze(1))\n",
    "\n",
    "            # Segmentation loss expects logits\n",
    "            loss_seg = criterion_seg(seg_logits, masks)\n",
    "\n",
    "            loss = Config.CLASSIFICATION_LOSS_WEIGHT * loss_cls + Config.SEGMENTATION_LOSS_WEIGHT * loss_seg\n",
    "            # --- NO torch.amp.autocast ---\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: NaN/Inf validation loss detected (Cls: {loss_cls.item():.4f}, Seg: {loss_seg.item():.4f}). Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            running_loss_cls += loss_cls.item() * current_batch_size\n",
    "            running_loss_seg += loss_seg.item() * current_batch_size\n",
    "            total_loss_weighted += loss.item() * current_batch_size\n",
    "\n",
    "            all_preds_proba.extend(torch.sigmoid(cls_logits).cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No valid samples processed in validation.\")\n",
    "        return 0.0, 0.0, 0.0, np.array([]), np.array([])\n",
    "\n",
    "    avg_loss_cls = running_loss_cls / total_samples\n",
    "    avg_loss_seg = running_loss_seg / total_samples\n",
    "    avg_loss_total = total_loss_weighted / total_samples\n",
    "\n",
    "    return avg_loss_cls, avg_loss_seg, avg_loss_total, np.array(all_labels), np.array(all_preds_proba)\n",
    "\n",
    "# %% Attention Visualization (remains the same, uses device=\"cpu\")\n",
    "def visualize_attention_maps(model, input_tensor, save_path, device):\n",
    "    \"\"\"Visualize Transformer attention maps (first layer only per block).\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Model and tensor are on CPU, unsqueeze adds batch dim\n",
    "        input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "        attention_maps_list = model.get_attention_maps(input_tensor)\n",
    "\n",
    "    if not attention_maps_list:\n",
    "         print(\"No attention maps returned by the model.\")\n",
    "         return\n",
    "\n",
    "    print(f\"Visualizing {len(attention_maps_list)} attention map(s)...\")\n",
    "    for map_idx, attn_map_layer in enumerate(attention_maps_list):\n",
    "        # Level 0 was skipped, so first map (map_idx 0) is from level 1\n",
    "        encoder_level = map_idx + 1\n",
    "\n",
    "        if attn_map_layer is None or attn_map_layer.numel() == 0:\n",
    "            print(f\"Skipping empty attention map for Encoder Level {encoder_level}\")\n",
    "            continue\n",
    "\n",
    "        # Data is already on CPU\n",
    "        attn_map_numpy = attn_map_layer.squeeze(0).numpy() # Shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Average across heads: (num_heads, seq_len, seq_len) -> (seq_len, seq_len)\n",
    "        mean_attn = np.mean(attn_map_numpy, axis=0)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        im = plt.imshow(mean_attn, cmap='viridis', aspect='auto')\n",
    "        plt.colorbar(im)\n",
    "        plt.title(f'Mean Attention Map - Encoder Level {encoder_level} (CPU)')\n",
    "        plt.xlabel('Key Positions (Flattened)')\n",
    "        plt.ylabel('Query Positions (Flattened)')\n",
    "        plt.savefig(os.path.join(save_path, f'attention_encoder_level_{encoder_level}_cpu.png'))\n",
    "        plt.close()\n",
    "    print(f\"Attention maps saved to {save_path}\")\n",
    "\n",
    "\n",
    "# %% Main Execution (CPU VERSION)\n",
    "def main():\n",
    "    # Load data IDs and labels\n",
    "    scans_to_process, patient_labels = load_and_select_data()\n",
    "    if not scans_to_process:\n",
    "        print(\"Error: No scans selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess scans (ensure they exist with the correct 32x32x32 size)\n",
    "    successful_ids = preprocess_scans(scans_to_process)\n",
    "    if not successful_ids:\n",
    "        print(\"Error: No scans were successfully preprocessed or found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Create dataloaders (will use pin_memory=False)\n",
    "    try:\n",
    "        train_loader, val_loader, train_ids, val_ids = create_dataloaders(\n",
    "            successful_ids, patient_labels\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating dataloaders: {e}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Initialize model and move to CPU\n",
    "    model = EfficientUNetPlusPlus_SE_Transformer().to(DEVICE) # DEVICE is \"cpu\"\n",
    "\n",
    "    # Loss and optimizer (Using BCEWithLogitsLoss for both)\n",
    "    labels_list = [patient_labels.get(pid) for pid in successful_ids if patient_labels.get(pid) is not None]\n",
    "    pos_weight_val = 1.0\n",
    "    if labels_list:\n",
    "        num_neg = sum(1 for l in labels_list if l == 0)\n",
    "        num_pos = sum(1 for l in labels_list if l == 1)\n",
    "        if num_pos > 0:\n",
    "            pos_weight_val = num_neg / num_pos\n",
    "    print(f\"Calculated pos_weight for BCEWithLogitsLoss: {pos_weight_val:.2f}\")\n",
    "\n",
    "    # Loss functions are defined on CPU by default\n",
    "    criterion_cls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_val])) # No device needed\n",
    "    criterion_seg = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.EPOCHS)\n",
    "    # --- NO SCALER NEEDED FOR CPU ---\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses_cls, train_losses_seg, train_losses_total = [], [], []\n",
    "    val_losses_cls, val_losses_seg, val_losses_total = [], [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    # Modify save path name to indicate CPU run\n",
    "    model_save_path = os.path.join(Config.MODEL_OUTPUT_DIR, \"efficient_unetpp_se_transformer_cpu.pth\")\n",
    "\n",
    "    print(f\"\\nStarting training on CPU for {Config.EPOCHS} epochs with Batch Size {Config.BATCH_SIZE}...\")\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{Config.EPOCHS}\")\n",
    "        start_time = time.time()\n",
    "        # --- Pass device=\"cpu\", no scaler ---\n",
    "        train_loss_cls, train_loss_seg, train_loss_total, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion_cls, criterion_seg, optimizer, DEVICE\n",
    "        )\n",
    "        val_loss_cls, val_loss_seg, val_loss_total, val_labels, val_preds_proba = validate(\n",
    "            model, val_loader, criterion_cls, criterion_seg, DEVICE\n",
    "        )\n",
    "        epoch_time = time.time() - start_time\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses_cls.append(train_loss_cls)\n",
    "        train_losses_seg.append(train_loss_seg)\n",
    "        train_losses_total.append(train_loss_total)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses_cls.append(val_loss_cls)\n",
    "        val_losses_seg.append(val_loss_seg)\n",
    "        val_losses_total.append(val_loss_total)\n",
    "\n",
    "        val_acc = 0.0\n",
    "        if len(val_labels) > 0 and len(val_preds_proba) == len(val_labels):\n",
    "             try:\n",
    "                 val_preds_binary = (val_preds_proba > 0.5).astype(int)\n",
    "                 val_acc = accuracy_score(val_labels, val_preds_binary)\n",
    "             except Exception as e:\n",
    "                 print(f\"Could not calculate validation accuracy: {e}\")\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Time: {epoch_time:.2f}s\") # Expect this to be much longer on CPU\n",
    "        print(f\"Train Loss (Cls/Seg/Total): {train_loss_cls:.4f} / {train_loss_seg:.4f} / {train_loss_total:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss   (Cls/Seg/Total): {val_loss_cls:.4f} / {val_loss_seg:.4f} / {val_loss_total:.4f},   Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Current LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model based on total validation loss\n",
    "        if not np.isnan(val_loss_total) and not np.isinf(val_loss_total) and val_loss_total < best_val_loss and len(val_labels) > 0:\n",
    "            best_val_loss = val_loss_total\n",
    "            try:\n",
    "                # Ensure model is saved in CPU format if needed elsewhere\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Saved best model (Val Loss: {best_val_loss:.4f}) to {model_save_path}\")\n",
    "            except Exception as save_e:\n",
    "                print(f\"Error saving model: {save_e}\")\n",
    "        elif np.isnan(val_loss_total) or np.isinf(val_loss_total):\n",
    "             print(f\"Skipping model save due to invalid validation loss: {val_loss_total}\")\n",
    "\n",
    "    # Plot training history\n",
    "    print(\"\\nPlotting training history...\")\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(1, Config.EPOCHS + 1), train_losses_cls, label='Train Cls Loss')\n",
    "    plt.plot(range(1, Config.EPOCHS + 1), val_losses_cls, label='Val Cls Loss')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Classification Loss (CPU)')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(1, Config.EPOCHS + 1), train_losses_seg, label='Train Seg Loss')\n",
    "    plt.plot(range(1, Config.EPOCHS + 1), val_losses_seg, label='Val Seg Loss')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Segmentation Loss (CPU)')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(1, Config.EPOCHS + 1), train_accs, label='Train Acc')\n",
    "    plt.plot(range(1, Config.EPOCHS + 1), val_accs, label='Val Acc')\n",
    "    plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy (CPU)')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plot_save_path = os.path.join(Config.MODEL_OUTPUT_DIR, \"training_curves_cpu.png\")\n",
    "    plt.savefig(plot_save_path)\n",
    "    print(f\"Training curves saved to {plot_save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"\\n--- Final Evaluation on Validation Set using Best Model (CPU) ---\")\n",
    "    if os.path.exists(model_save_path):\n",
    "        try:\n",
    "            # Load model weights onto CPU\n",
    "            model.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))\n",
    "            print(f\"Loaded best CPU model from {model_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model: {e}. Evaluation might use last epoch's weights.\")\n",
    "\n",
    "        val_loss_cls, val_loss_seg, val_loss_total, val_labels, val_preds_proba = validate(\n",
    "            model, val_loader, criterion_cls, criterion_seg, DEVICE # DEVICE is 'cpu'\n",
    "        )\n",
    "\n",
    "        if len(val_labels) > 0 and len(val_preds_proba) == len(val_labels):\n",
    "            val_preds_binary = (val_preds_proba > 0.5).astype(int)\n",
    "            auc_roc = float('nan')\n",
    "            if len(np.unique(val_labels)) > 1:\n",
    "                 try: auc_roc = roc_auc_score(val_labels, val_preds_proba)\n",
    "                 except ValueError as e: print(f\"Could not calculate AUC: {e}\")\n",
    "\n",
    "            print(f\"\\nValidation Loss (Cls/Seg/Total): {val_loss_cls:.4f} / {val_loss_seg:.4f} / {val_loss_total:.4f}\")\n",
    "            print(f\"Accuracy:  {accuracy_score(val_labels, val_preds_binary):.4f}\")\n",
    "            print(f\"Precision: {precision_score(val_labels, val_preds_binary, zero_division=0):.4f}\")\n",
    "            print(f\"Recall:    {recall_score(val_labels, val_preds_binary, zero_division=0):.4f}\")\n",
    "            print(f\"F1 Score:  {f1_score(val_labels, val_preds_binary, zero_division=0):.4f}\")\n",
    "            print(f\"AUC ROC:   {auc_roc:.4f}\")\n",
    "\n",
    "            print(\"\\nClassification Report:\")\n",
    "            target_names = ['Class 0 (Non-Cancer)', 'Class 1 (Cancer)']\n",
    "            try: print(classification_report(val_labels, val_preds_binary, target_names=target_names, zero_division=0))\n",
    "            except Exception as e: print(f\"Could not generate classification report: {e}\")\n",
    "\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            try:\n",
    "                cm = confusion_matrix(val_labels, val_preds_binary)\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "                disp.plot(cmap=plt.cm.Blues)\n",
    "                cm_save_path = os.path.join(Config.MODEL_OUTPUT_DIR, \"confusion_matrix_cpu.png\")\n",
    "                plt.savefig(cm_save_path)\n",
    "                print(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "                plt.close()\n",
    "            except Exception as e: print(f\"Could not plot confusion matrix: {e}\")\n",
    "        else:\n",
    "            print(\"Validation set empty or prediction/label length mismatch. No final metrics.\")\n",
    "    else:\n",
    "        print(f\"Best model file not found at {model_save_path}. Skipping final evaluation.\")\n",
    "\n",
    "    # Attention Map Visualization (will run on CPU)\n",
    "    print(\"\\n--- Visualizing Attention Maps (CPU) ---\")\n",
    "    if val_loader and len(val_loader.dataset) > 0:\n",
    "        try:\n",
    "            input_sample, vis_label, _ = val_loader.dataset[0]\n",
    "            if vis_label != -1:\n",
    "                 attention_save_path = os.path.join(Config.MODEL_OUTPUT_DIR, \"attention_maps_cpu\")\n",
    "                 visualize_attention_maps(model, input_sample, attention_save_path, DEVICE) # DEVICE is 'cpu'\n",
    "            else:\n",
    "                print(\"First validation sample is invalid, skipping attention visualization.\")\n",
    "        except IndexError:\n",
    "             print(\"Validation dataset is empty, cannot visualize attention.\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error during attention visualization: {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Validation loader/dataset not available or empty, skipping attention visualization.\")\n",
    "\n",
    "    print(\"\\n--- Execution Finished (CPU) ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- A critical error occurred during execution ---\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
